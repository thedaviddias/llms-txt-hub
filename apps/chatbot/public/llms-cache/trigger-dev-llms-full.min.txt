# API keys
Source: https://trigger.dev/docs/apikeys
How to authenticate with Trigger.dev so you can trigger tasks.
### Authentication and your secret keys
When you [trigger a task](/triggering) from your backend code, you need to set the `TRIGGER_SECRET_KEY` environment variable.
Each environment has its own secret key. You can find the value on the API keys page in the Trigger.dev dashboard:
![How to find your secret key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/api-keys.png)
### Automatically Configuring the SDK
To automatically configure the SDK with your secret key, you can set the `TRIGGER_SECRET_KEY` environment variable. The SDK will automatically use this value when calling API methods (like `trigger`).
```bash .env
TRIGGER_SECRET_KEY="tr_dev_â€¦"
```
You can do the same if you are self-hosting and need to change the default URL by using `TRIGGER_API_URL`.
```bash .env
TRIGGER_API_URL="https://trigger.example.com"
```
The default URL is `https://api.trigger.dev`.
### Manually Configuring the SDK
If you prefer to manually configure the SDK, you can call the `configure` method:
```ts
import { configure } from "@trigger.dev/sdk/v3";
import { myTask } from "./trigger/myTasks";
configure({
secretKey: "tr_dev_1234", // WARNING: Never actually hardcode your secret key like this
baseURL: "https://mytrigger.example.com", // Optional
});
async function triggerTask() {
await myTask.trigger({ userId: "1234" }); // This will use the secret key and base URL you configured
}
```
# Bulk actions
Source: https://trigger.dev/docs/bulk-actions
Perform actions like replay and cancel on multiple runs at once.
Bulk actions allow you to perform operations like replaying or canceling on multiple runs at once. This is especially useful when you need to retry a batch of failed runs with a new version of your code, or when you need to cancel multiple in-progress runs.
## Bulk replaying
You can replay multiple runs at once by selecting them from the table on the Runs page using the checkbox on the left hand side of the row. Then click the "Replay runs" button from the bulk action bar that appears at the bottom of the screen.
This is especially useful if you have lots of failed runs and want to run them all again. To do this, first filter the runs by the status you want, then select all the runs you want to replay and click the "Replay runs" button from the bulk action bar at the bottom of the page.
<video src="https://content.trigger.dev/bulk-replaying-runs.mp4" preload="auto" controls={true} loop muted autoPlay={true} width="100%" height="100%" />
## Bulk canceling
Similar to replaying multiple runs, you can cancel multiple runs at once. This is particularly useful when you have a batch of runs that you want to stop, perhaps because they were triggered with incorrect parameters or are no longer needed.
To cancel multiple runs:
1. Filter the runs table to show the runs you want to cancel (e.g., all runs with status "QUEUED" or "EXECUTING")
2. Use the checkboxes on the left side of the runs table to select the runs you want to cancel
3. Click the "Cancel runs" button in the bulk action bar that appears at the bottom of the screen
After confirming, all selected runs that can be canceled (those in appropriate states like QUEUED or EXECUTING) will be canceled. The status of these runs will change to "CANCELED" and they will not be resumed.
<Note>
You can only cancel runs that are in states that allow cancellation (like QUEUED or EXECUTING).
Runs that are already completed, failed, or in other final states cannot be canceled.
</Note>
# Changelog
Source: https://trigger.dev/docs/changelog
Our [changelog](https://trigger.dev/changelog) is the best way to stay up to date with the latest changes to Trigger.
# CLI deploy command
Source: https://trigger.dev/docs/cli-deploy-commands
Use the deploy command to deploy your tasks to Trigger.dev.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
<Warning>
This will fail in CI if any version mismatches are detected. Ensure everything runs locally first
using the [dev](/cli-dev-commands) command and don't bypass the version checks!
</Warning>
It performs a few steps to deploy:
1. Optionally updates packages when running locally.
2. Compiles and bundles the code.
3. Deploys the code to the Trigger.dev instance.
4. Registers the tasks as a new version in the environment (prod by default).
You can also setup [GitHub Actions](/github-actions) to deploy your tasks automatically.
## Arguments
```
npx trigger.dev@latest deploy [path]
```
<ParamField body="Project path" type="[path]">
The path to the project. Defaults to the current directory.
</ParamField>
## Options
<ParamField body="Config file" type="--config | -c">
The name of the config file found at the project path. Defaults to `trigger.config.ts`
</ParamField>
<ParamField body="Project ref" type="--project-ref | -p">
The project ref. Required if there is no config file.
</ParamField>
<ParamField body="Env file" type="--env-file">
Load environment variables from a file. This will only hydrate the `process.env` of the CLI
process, not the tasks.
</ParamField>
<ParamField body="Skip update check" type="--skip-update-check">
Skip checking for `@trigger.dev` package updates.
</ParamField>
<ParamField body="Environment" type="--env | -e">
Defaults to `prod` but you can specify `staging`.
</ParamField>
<ParamField body="Dry run" type="--dry-run">
Create a deployable build but don't deploy it. Prints out the build path so you can inspect it.
</ParamField>
<ParamField body="Skip promotion" type="--skip-promotion">
Skips automatically promoting the newly deployed version to the "current" deploy.
</ParamField>
<ParamField body="Skip syncing env vars" type="--skip-sync-env-vars">
Turn off syncing environment variables with the Trigger.dev instance.
</ParamField>
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
### Self-hosting
These options are typically used when [self-hosting](/open-source-self-hosting) or for local development.
<ParamField body="Self-hosted (builds locally)" type="--self-hosted">
Builds and loads the image using your local docker. Use the `--registry` option to specify the
registry to push the image to when using `--self-hosted`, or just use `--push` to push to the
default registry.
</ParamField>
<ParamField body="Skip deploying the image" type="--skip-deploy | -D">
Load the built image into your local docker.
</ParamField>
<ParamField body="Load image" type="--load-image">
Loads the image into your local docker after building it.
</ParamField>
<ParamField body="Registry" type="--registry">
Specify the registry to push the image to when using `--self-hosted`. Will automatically enable
`--push`.
</ParamField>
<ParamField body="Push image" type="--push">
When using the `--self-hosted` flag, push the image to the registry.
</ParamField>
<ParamField body="Namespace" type="--namespace">
The namespace to use when pushing the image to the registry. For example, if pushing to Docker
Hub, the namespace is your Docker Hub username.
</ParamField>
<ParamField body="Network" type="--network">
The networking mode for RUN instructions when using `--self-hosted`.
</ParamField>
## Examples
### Push to Docker Hub (self-hosted)
An example of deploying to Docker Hub when using a self-hosted setup:
```bash
npx trigger.dev@latest deploy \
--self-hosted \
--load-image \
--registry docker.io \
--namespace mydockerhubusername
```
# CLI dev command
Source: https://trigger.dev/docs/cli-dev
The `trigger.dev dev` command is used to run your tasks locally.
This runs a server on your machine that can execute Trigger.dev tasks:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
It will first perform an update check to prevent version mismatches, failed deploys, and other errors. You will always be prompted first.
You will see in the terminal that the server is running and listening for tasks. When you run a task, you will see it in the terminal along with a link to view it in the dashboard.
It is worth noting that each task runs in a separate Node process. This means that if you have a long-running task, it will not block other tasks from running.
## Options
<ParamField body="Config file" type="--config | -c">
The name of the config file found at the project path. Defaults to `trigger.config.ts`
</ParamField>
<ParamField body="Project ref" type="--project-ref | -p">
The project ref. Required if there is no config file.
</ParamField>
<ParamField body="Env file" type="--env-file">
Load environment variables from a file. This will only hydrate the `process.env` of the CLI
process, not the tasks.
</ParamField>
<ParamField body="Skip update check" type="--skip-update-check">
Skip checking for `@trigger.dev` package updates.
</ParamField>
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
## Concurrently running the terminal
Install the concurrently package as a dev dependency:
```ts
concurrently --raw --kill-others npm:dev:remix npm:dev:trigger
```
Then add something like this in your package.json scripts:
```json
"scripts": {
"dev": "concurrently --raw --kill-others npm:dev:*",
"dev:trigger": "npx trigger.dev@latest dev",
// Add your framework-specific dev script here, for example:
// "dev:next": "next dev",
// "dev:remix": "remix dev",
//...
}
```
# CLI dev command
Source: https://trigger.dev/docs/cli-dev-commands
The `trigger.dev dev` command is used to run your tasks locally.
This runs a server on your machine that can execute Trigger.dev tasks:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
It will first perform an update check to prevent version mismatches, failed deploys, and other errors. You will always be prompted first.
You will see in the terminal that the server is running and listening for tasks. When you run a task, you will see it in the terminal along with a link to view it in the dashboard.
It is worth noting that each task runs in a separate Node process. This means that if you have a long-running task, it will not block other tasks from running.
## Options
<ParamField body="Config file" type="--config | -c">
The name of the config file found at the project path. Defaults to `trigger.config.ts`
</ParamField>
<ParamField body="Project ref" type="--project-ref | -p">
The project ref. Required if there is no config file.
</ParamField>
<ParamField body="Env file" type="--env-file">
Load environment variables from a file. This will only hydrate the `process.env` of the CLI
process, not the tasks.
</ParamField>
<ParamField body="Skip update check" type="--skip-update-check">
Skip checking for `@trigger.dev` package updates.
</ParamField>
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
## Concurrently running the terminal
Install the concurrently package as a dev dependency:
```ts
concurrently --raw --kill-others npm:dev:remix npm:dev:trigger
```
Then add something like this in your package.json scripts:
```json
"scripts": {
"dev": "concurrently --raw --kill-others npm:dev:*",
"dev:trigger": "npx trigger.dev@latest dev",
// Add your framework-specific dev script here, for example:
// "dev:next": "next dev",
// "dev:remix": "remix dev",
//...
}
```
# CLI init command
Source: https://trigger.dev/docs/cli-init-commands
Use these options when running the CLI `init` command.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
## Options
<ParamField body="Javascript" type="--javascript">
By default, the init command assumes you are using TypeScript. Use this flag to initialize a
project that uses JavaScript.
</ParamField>
<ParamField body="Project ref" type="--project-ref | -p">
The project ref to use when initializing the project.
</ParamField>
<ParamField body="Package tag" type="--tag | -t">
The version of the `@trigger.dev/sdk` package to install. Defaults to `latest`.
</ParamField>
<ParamField body="Skip package install" type="--skip-package-install">
Skip installing the `@trigger.dev/sdk` package.
</ParamField>
<ParamField body="Override config" type="--override-config">
Override the existing config file if it exists.
</ParamField>
<ParamField body="Package arguments" type="--pkg-args">
Additional arguments to pass to the package manager. Accepts CSV for multiple args.
</ParamField>
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# Introduction
Source: https://trigger.dev/docs/cli-introduction
The Trigger.dev CLI has a number of options and commands to help you develop locally, self host, and deploy your tasks.
## Options
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
## Commands
| Command                                      | Description                                                        |
| :------------------------------------------- | :----------------------------------------------------------------- |
| [login](/cli-login-commands)                 | Login with Trigger.dev so you can perform authenticated actions.   |
| [init](/cli-init-commands)                   | Initialize your existing project for development with Trigger.dev. |
| [dev](/cli-dev-commands)                     | Run your Trigger.dev tasks locally.                                |
| [deploy](/cli-deploy-commands)               | Deploy your Trigger.dev v3 project to the cloud.                   |
| [whoami](/cli-whoami-commands)               | Display the current logged in user and project details.            |
| [logout](/cli-logout-commands)               | Logout of Trigger.dev.                                             |
| [list-profiles](/cli-list-profiles-commands) | List all of your CLI profiles.                                     |
| [update](/cli-update-commands)               | Updates all `@trigger.dev/*` packages to match the CLI version.    |
# CLI list-profiles command
Source: https://trigger.dev/docs/cli-list-profiles-commands
Use these options when using the `list-profiles` CLI command.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest list-profiles
```
```bash pnpm
pnpm dlx trigger.dev@latest list-profiles
```
```bash yarn
yarn dlx trigger.dev@latest list-profiles
```
</CodeGroup>
## Options
### Common options
These options are available on most commands.
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# CLI login command
Source: https://trigger.dev/docs/cli-login-commands
Use these options when logging in to Trigger.dev using the CLI.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest login
```
```bash pnpm
pnpm dlx trigger.dev@latest login
```
```bash yarn
yarn dlx trigger.dev@latest login
```
</CodeGroup>
## Options
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# CLI logout command
Source: https://trigger.dev/docs/cli-logout-commands
Use these options when using the `logout` CLI command.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest logout
```
```bash pnpm
pnpm dlx trigger.dev@latest logout
```
```bash yarn
yarn dlx trigger.dev@latest logout
```
</CodeGroup>
## Options
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# CLI promote command
Source: https://trigger.dev/docs/cli-promote-commands
Use the promote command to promote a previously deployed version to the current version.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest promote [version]
```
```bash pnpm
pnpm dlx trigger.dev@latest promote [version]
```
```bash yarn
yarn dlx trigger.dev@latest promote [version]
```
</CodeGroup>
## Arguments
```
npx trigger.dev@latest promote [version]
```
<ParamField body="Deployment version" type="[version]">
The version to promote. This is the version that was previously deployed.
</ParamField>
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# CLI update command
Source: https://trigger.dev/docs/cli-update-commands
Use these options when using the `update` CLI command.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest update
```
```bash pnpm
pnpm dlx trigger.dev@latest update
```
```bash yarn
yarn dlx trigger.dev@latest update
```
</CodeGroup>
## Options
### Common options
These options are available on most commands.
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# CLI whoami command
Source: https://trigger.dev/docs/cli-whoami-commands
Use these options to display the current logged in user and project details.
Run the command like this:
<CodeGroup>
```bash npm
npx trigger.dev@latest whoami
```
```bash pnpm
pnpm dlx trigger.dev@latest whoami
```
```bash yarn
yarn dlx trigger.dev@latest whoami
```
</CodeGroup>
## Options
### Common options
These options are available on most commands.
<ParamField body="Login profile" type="--profile">
The login profile to use. Defaults to "default".
</ParamField>
<ParamField body="API URL" type="--api-url | -a">
Override the default API URL. If not specified, it uses `https://api.trigger.dev`. This can also be set via the `TRIGGER_API_URL` environment variable.
</ParamField>
<ParamField body="Log level" type="--log-level | -l">
The CLI log level to use. Options are `debug`, `info`, `log`, `warn`, `error`, and `none`. This does not affect the log level of your trigger.dev tasks. Defaults to `log`.
</ParamField>
<ParamField body="Skip telemetry" type="--skip-telemetry">
Opt-out of sending telemetry data. This can also be done via the `TRIGGER_TELEMETRY_DISABLED` environment variable. Just set it to anything other than an empty string.
</ParamField>
<ParamField body="Help" type="--help | -h">
Shows the help information for the command.
</ParamField>
<ParamField body="Version" type="--version | -v">
Displays the version number of the CLI.
</ParamField>
# Discord Community
Source: https://trigger.dev/docs/community
Please [join our community on Discord](https://trigger.dev/discord) to ask questions, share your projects, and get help from other developers.
# The trigger.config.ts file
Source: https://trigger.dev/docs/config/config-file
This file is used to configure your project and how it's built.
The `trigger.config.ts` file is used to configure your Trigger.dev project. It is a TypeScript file at the root of your project that exports a default configuration object. Here's an example:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
// Your project ref (you can see it on the Project settings page in the dashboard)
project: "<project ref>",
//The paths for your trigger folders
dirs: ["./trigger"],
retries: {
//If you want to retry a task in dev mode (when using the CLI)
enabledInDev: false,
//the default retry settings. Used if you don't specify on a task.
default: {
maxAttempts: 3,
minTimeoutInMs: 1000,
maxTimeoutInMs: 10000,
factor: 2,
randomize: true,
},
},
});
```
The config file handles a lot of things, like:
* Specifying where your trigger tasks are located using the `dirs` option.
* Setting the default retry settings.
* Configuring OpenTelemetry instrumentations.
* Customizing the build process.
* Adding global task lifecycle functions.
<Note>
The config file is bundled with your project, so code imported in the config file is also bundled,
which can have an effect on build times and cold start duration. One important qualification is
anything defined in the `build` config is automatically stripped out of the config file, and
imports used inside build config with be tree-shaken out.
</Note>
## Dirs
You can specify the directories where your tasks are located using the `dirs` option:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
dirs: ["./trigger"],
});
```
If you omit the `dirs` option, we will automatically detect directories that are named `trigger` in your project, but we recommend specifying the directories explicitly. The `dirs` option is an array of strings, so you can specify multiple directories if you have tasks in multiple locations.
We will search for TypeScript and JavaScript files in the specified directories and include them in the build process. We automatically exclude files that have `.test` or `.spec` in the name, but you can customize this by specifying glob patterns in the `ignorePatterns` option:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
dirs: ["./trigger"],
ignorePatterns: ["**/*.my-test.ts"],
});
```
## Lifecycle functions
You can add lifecycle functions to get notified when any task starts, succeeds, or fails using `onStart`, `onSuccess` and `onFailure`:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
onSuccess: async (payload, output, { ctx }) => {
console.log("Task succeeded", ctx.task.id);
},
onFailure: async (payload, error, { ctx }) => {
console.log("Task failed", ctx.task.id);
},
onStart: async (payload, { ctx }) => {
console.log("Task started", ctx.task.id);
},
init: async (payload, { ctx }) => {
console.log("I run before any task is run");
},
});
```
Read more about task lifecycle functions in the [tasks overview](/tasks/overview).
## Instrumentations
We use OpenTelemetry (OTEL) for our run logs. This means you get a lot of information about your tasks with no effort. But you probably want to add more information to your logs. For example, here's all the Prisma calls automatically logged:
![The run log](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/auto-instrumentation.png)
Here we add Prisma and OpenAI instrumentations to your `trigger.config.ts` file.
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { PrismaInstrumentation } from "@prisma/instrumentation";
import { OpenAIInstrumentation } from "@traceloop/instrumentation-openai";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
telemetry: {
instrumentations: [new PrismaInstrumentation(), new OpenAIInstrumentation()],
},
});
```
There is a [huge library of instrumentations](https://opentelemetry.io/ecosystem/registry/?language=js) you can easily add to your project like this.
Some ones we recommend:
| Package                               | Description                                                                                                              |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `@opentelemetry/instrumentation-http` | Logs all HTTP calls                                                                                                      |
| `@prisma/instrumentation`             | Logs all Prisma calls, you need to [enable tracing](https://github.com/prisma/prisma/tree/main/packages/instrumentation) |
| `@traceloop/instrumentation-openai`   | Logs all OpenAI calls                                                                                                    |
<Note>
`@opentelemetry/instrumentation-fs` which logs all file system calls is currently not supported.
</Note>
### Exporters
You can also configure custom exporters to send your telemetry data to other services. For example, you can send your logs to [Axiom](https://axiom.co/docs/guides/opentelemetry-nodejs#exporter-instrumentation-ts):
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-proto';
// Initialize OTLP trace exporter with the endpoint URL and headers
const axiomExporter = new OTLPTraceExporter({
url: 'https://api.axiom.co/v1/traces',
headers: {
'Authorization': `Bearer ${process.env.AXIOM_API_TOKEN}`,
'X-Axiom-Dataset': process.env.AXIOM_DATASET
},
});
export default defineConfig({
project: "<project ref>",
// Your other config settings...
telemetry: {
instrumentations: [
// Your instrumentations here
],
exporters: [axiomExporter],
},
});
```
Make sure to set the `AXIOM_API_TOKEN` and `AXIOM_DATASET` environment variables in your project.
## Runtime
We currently only officially support the `node` runtime, but you can try our experimental `bun` runtime by setting the `runtime` option in your config file:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
runtime: "bun",
});
```
See our [Bun guide](/guides/frameworks/bun) for more information.
## Default machine
You can specify the default machine for all tasks in your project:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
defaultMachine: "large-1x",
});
```
See our [machines documentation](/machines) for more information.
## Log level
You can set the log level for your project:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
logLevel: "debug",
});
```
The `logLevel` only determines which logs are sent to the Trigger.dev instance when using the `logger` API. All `console` based logs are always sent.
## Max duration
You can set the default `maxDuration` for all tasks in your project:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
maxDuration: 60, // 60 seconds
});
```
See our [maxDuration guide](/runs/max-duration) for more information.
## Build configuration
You can customize the build process using the `build` option:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
// Don't bundle these packages
external: ["header-generator"],
},
});
```
<Note>
The `trigger.config.ts` file is included in the bundle, but with the `build` configuration
stripped out. These means any imports only used inside the `build` configuration are also removed
from the final bundle.
</Note>
### External
All code is bundled by default, but you can exclude some packages from the bundle using the `external` option:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
external: ["header-generator"],
},
});
```
When a package is excluded from the bundle, it will be added to a dynamically generated package.json file in the build directory. The version of the package will be the same as the version found in your `node_modules` directory.
Each entry in the external should be a package name, not necessarily the import path. For example, if you want to exclude the `ai` package, but you are importing `ai/rsc`, you should just include `ai` in the `external` array:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
external: ["ai"],
},
});
```
<Note>
Any packages that install or build a native binary should be added to external, as native binaries
cannot be bundled. For example, `re2`, `sharp`, and `sqlite3` should be added to external.
</Note>
### JSX
You can customize the `jsx` options that are passed to `esbuild` using the `jsx` option:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
jsx: {
// Use the Fragment component instead of React.Fragment
fragment: "Fragment",
// Use the h function instead of React.createElement
factory: "h",
// Turn off automatic runtime
automatic: false,
},
},
});
```
By default we enabled [esbuild's automatic JSX runtime](https://esbuild.github.io/content-types/#auto-import-for-jsx) which means you don't need to import `React` in your JSX files. You can disable this by setting `automatic` to `false`.
See the [esbuild JSX documentation](https://esbuild.github.io/content-types/#jsx) for more information.
### Conditions
You can add custom [import conditions](https://esbuild.github.io/api/#conditions) to your build using the `conditions` option:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
conditions: ["react-server"],
},
});
```
These conditions effect how imports are resolved during the build process. For example, the `react-server` condition will resolve `ai/rsc` to the server version of the `ai/rsc` export.
Custom conditions will also be passed to the `node` runtime when running your tasks.
### Extensions
Build extension allow you to hook into the build system and customize the build process or the resulting bundle and container image (in the case of deploying). You can use pre-built extensions by installing the `@trigger.dev/build` package into your `devDependencies`, or you can create your own.
#### additionalFiles
See the [additionalFiles documentation](/config/extensions/additionalFiles) for more information.
#### `additionalPackages`
See the [additionalPackages documentation](/config/extensions/additionalPackages) for more information.
#### `emitDecoratorMetadata`
See the [emitDecoratorMetadata documentation](/config/extensions/emitDecoratorMetadata) for more information.
#### Prisma
See the [prismaExtension documentation](/config/extensions/prismaExtension) for more information.
#### syncEnvVars
See the [syncEnvVars documentation](/config/extensions/syncEnvVars) for more information.
#### puppeteer
See the [puppeteer documentation](/config/extensions/puppeteer) for more information.
#### ffmpeg
See the [ffmpeg documentation](/config/extensions/ffmpeg) for more information.
#### esbuild plugins
See the [esbuild plugins documentation](/config/extensions/esbuildPlugin) for more information.
#### aptGet
See the [aptGet documentation](/config/extensions/aptGet) for more information.
# Additional Files
Source: https://trigger.dev/docs/config/extensions/additionalFiles
Use the additionalFiles build extension to copy additional files to the build directory
Import the `additionalFiles` build extension and use it in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { additionalFiles } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
additionalFiles({ files: ["wrangler/wrangler.toml", "./assets/**", "./fonts/**"] }),
],
},
});
```
This will copy the files specified in the `files` array to the build directory. The `files` array can contain globs. The output paths will match the path of the file, relative to the root of the project.
This extension effects both the `dev` and the `deploy` commands, and the resulting paths will be the same for both.
<Note>The root of the project is the directory that contains the trigger.config.ts file</Note>
# Additional Packages
Source: https://trigger.dev/docs/config/extensions/additionalPackages
Use the additionalPackages build extension to include additional packages in the build
Import the `additionalPackages` build extension and use it in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { additionalPackages } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [additionalPackages({ packages: ["wrangler"] })],
},
});
```
This allows you to include additional packages in the build that are not automatically included via imports. This is useful if you want to install a package that includes a CLI tool that you want to invoke in your tasks via `exec`. We will try to automatically resolve the version of the package but you can specify the version by using the `@` symbol:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [additionalPackages({ packages: ["wrangler@1.19.0"] })],
},
});
```
This extension does not do anything in `dev` mode, but it will install the packages in the build directory when you run `deploy`. The packages will be installed in the `node_modules` directory in the build directory.
# apt-get
Source: https://trigger.dev/docs/config/extensions/aptGet
Use the aptGet build extension to install system packages into the deployed image
You can install system packages into the deployed image using the `aptGet` extension:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { aptGet } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [aptGet({ packages: ["ffmpeg"] })],
},
});
```
If you want to install a specific version of a package, you can specify the version like this:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [aptGet({ packages: ["ffmpeg=6.0-4"] })],
},
});
```
# Audio Waveform
Source: https://trigger.dev/docs/config/extensions/audioWaveform
Use the audioWaveform build extension to add support for Audio Waveform in your project
Previously, we installed [Audio Waveform](https://github.com/bbc/audiowaveform) in the build image. That's been moved to a build extension:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { audioWaveform } from "@trigger.dev/build/extensions/audioWaveform";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [audioWaveform()], // uses verson 1.1.0 of audiowaveform by default
},
});
```
# Custom build extensions
Source: https://trigger.dev/docs/config/extensions/custom
Customize how your project is built and deployed to Trigger.dev with your own custom build extensions
Build extensions allow you to hook into the build system and customize the build process or the resulting bundle and container image (in the case of deploying). See our [build extension overview](/config/extensions/overview) for more information on how to install and use our built-in extensions. Build extensions can do the following:
* Add additional files to the build
* Add dependencies to the list of externals
* Add esbuild plugins
* Add additional npm dependencies
* Add additional system packages to the image build container
* Add commands to run in the image build container
* Add environment variables to the image build container
* Sync environment variables to your Trigger.dev project
## Creating a build extension
Build extensions are added to your `trigger.config.ts` file, with a required `name` and optional build hook functions. Here's a simple example of a build extension that just logs a message when the build starts:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
onBuildStart: async (context) => {
console.log("Build starting!");
},
},
],
},
});
```
You can also extract that out into a function instead of defining it inline, in which case you will need to import the `BuildExtension` type from the `@trigger.dev/build` package:
<Note>
You'll need to add the `@trigger.dev/build` package to your `devDependencies` before the below
code will work. Make sure it's version matches that of the installed `@trigger.dev/sdk` package.
</Note>
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { BuildExtension } from "@trigger.dev/build";
export default defineConfig({
project: "my-project",
build: {
extensions: [myExtension()],
},
});
function myExtension(): BuildExtension {
return {
name: "my-extension",
onBuildStart: async (context) => {
console.log("Build starting!");
},
};
}
```
## Build hooks
### externalsForTarget
This allows the extension to add additional dependencies to the list of externals for the build. This is useful for dependencies that are not included in the bundle, but are expected to be available at runtime.
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
externalsForTarget: async (target) => {
return ["my-dependency"];
},
},
],
},
});
```
### onBuildStart
This hook runs before the build starts. It receives the `BuildContext` object as an argument.
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
onBuildStart: async (context) => {
console.log("Build starting!");
},
},
],
},
});
```
If you want to add an esbuild plugin, you must do so in the `onBuildStart` hook. Here's an example of adding a custom esbuild plugin:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
onBuildStart: async (context) => {
context.registerPlugin({
name: "my-plugin",
setup(build) {
build.onLoad({ filter: /.*/, namespace: "file" }, async (args) => {
return {
contents: "console.log('Hello, world!')",
loader: "js",
};
});
},
});
},
},
],
},
});
```
You can use the `BuildContext.target` property to determine if the build is for `dev` or `deploy`:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
onBuildStart: async (context) => {
if (context.target === "dev") {
console.log("Building for dev");
} else {
console.log("Building for deploy");
}
},
},
],
},
});
```
### onBuildComplete
This hook runs after the build completes. It receives the `BuildContext` object and a `BuildManifest` object as arguments. This is where you can add in one or more `BuildLayer`'s to the context.
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
onBuildComplete: async (context, manifest) => {
context.addLayer({
id: "more-dependencies",
dependencies,
});
},
},
],
},
});
```
See the [addLayer](#addlayer) documentation for more information on how to use `addLayer`.
## BuildTarget
Can either be `dev` or `deploy`, matching the CLI command name that is being run.
```sh
npx trigger.dev@latest dev # BuildTarget is "dev"
npx trigger.dev@latest deploy # BuildTarget is "deploy"
```
## BuildContext
### addLayer()
<ParamField path="layer" type="BuildLayer">
The layer to add to the build context. See the [BuildLayer](#buildlayer) documentation for more
information.
</ParamField>
### registerPlugin()
<ParamField path="plugin" type="esbuild.Plugin" required>
The esbuild plugin to register.
</ParamField>
<ParamField path="options" type="object">
<Expandable title="properties">
<ResponseField name="target" type="BuildTarget">
An optional target to register the plugin for. If not provided, the plugin will be registered
for all targets.
</ResponseField>
<ResponseField name="placement" type="first | last">
An optional placement for the plugin. If not provided, the plugin will be registered in place.
This allows you to control the order of plugins.
</ResponseField>
</Expandable>
</ParamField>
### resolvePath()
Resolves a path relative to the project's working directory.
<ParamField path="path" type="string">
The path to resolve.
</ParamField>
```ts
const resolvedPath = context.resolvePath("my-other-dependency");
```
### properties
<ParamField path="target" type="BuildTarget">
The target of the build, either `dev` or `deploy`.
</ParamField>
<ParamField path="config" type="ResolvedConfig">
<Expandable title="properties">
<ResponseField name="runtime" type="string">
The runtime of the project (either node or bun)
</ResponseField>
<ResponseField name="project" type="string">
The project ref
</ResponseField>
<ResponseField name="dirs" type="string[]">
The trigger directories to search for tasks
</ResponseField>
<ResponseField name="build" type="object">
The build configuration object
</ResponseField>
<ResponseField name="workingDir" type="string">
The working directory of the project
</ResponseField>
<ResponseField name="workspaceDir" type="string">
The root workspace directory of the project
</ResponseField>
<ResponseField name="packageJsonPath" type="string">
The path to the package.json file
</ResponseField>
<ResponseField name="lockfilePath" type="string">
The path to the lockfile (package-lock.json, yarn.lock, or pnpm-lock.yaml)
</ResponseField>
<ResponseField name="configFile" type="string">
The path to the trigger.config.ts file
</ResponseField>
<ResponseField name="tsconfigPath" type="string">
The path to the tsconfig.json file
</ResponseField>
</Expandable>
</ParamField>
<ParamField path="logger" type="BuildLogger">
A logger object that can be used to log messages to the console.
</ParamField>
## BuildLayer
<ParamField path="id" type="string">
A unique identifier for the layer.
</ParamField>
<ParamField path="commands" type="string[]">
An array of commands to run in the image build container.
```ts
commands: ["echo 'Hello, world!'"];
```
These commands are run after packages have been installed and the code copied into the container in the "build" stage of the Dockerfile. This means you cannot install system packages in these commands because they won't be available in the final stage. To do that, please use the `pkgs` property of the `image` object.
</ParamField>
<ParamField path="image" type="object">
<Expandable title="properties">
<ResponseField name="pkgs" type="string[]">
An array of system packages to install in the image build container.
</ResponseField>
<ResponseField name="instructions" type="string[]">
An array of instructions to add to the Dockerfile.
</ResponseField>
</Expandable>
</ParamField>
<ParamField path="build" type="object">
<Expandable title="properties">
<ResponseField name="env" type="Record<string, string>">
Environment variables to add to the image build container, but only during the "build" stage
of the Dockerfile. This is where you'd put environment variables that are needed when running
any of the commands in the `commands` array.
</ResponseField>
</Expandable>
</ParamField>
<ParamField path="deploy" type="object">
<Expandable title="properties">
<ResponseField name="env" type="Record<string, string>">
Environment variables that should sync to the Trigger.dev project, which will then be avalable
in your tasks at runtime. Importantly, these are NOT added to the image build container, but
are instead added to the Trigger.dev project and stored securely.
</ResponseField>
</Expandable>
</ParamField>
<ParamField path="dependencies" type="Record<string, string>">
An object of dependencies to add to the build. The key is the package name and the value is the
version.
```ts
dependencies: {
"my-dependency": "^1.0.0",
};
```
</ParamField>
### examples
Add a command that will echo the value of an environment variable:
```ts
context.addLayer({
id: "my-layer",
commands: [`echo $MY_ENV_VAR`],
build: {
env: {
MY_ENV_VAR: "Hello, world!",
},
},
});
```
## Troubleshooting
When creating a build extension, you may run into issues with the build process. One thing that can help is turning on `debug` logging when running either `dev` or `deploy`:
```sh
npx trigger.dev@latest dev --log-level debug
npx trigger.dev@latest deploy --log-level debug
```
Another helpful tool is the `--dry-run` flag on the `deploy` command, which will bundle your project and generate the Containerfile (e.g. the Dockerfile) without actually deploying it. This can help you see what the final image will look like and debug any issues with the build process.
```sh
npx trigger.dev@latest deploy --dry-run
```
You should also take a look at our built in extensions for inspiration on how to create your own. You can find them in in [the source code here](https://github.com/triggerdotdev/trigger.dev/tree/main/packages/build/src/extensions).
# Emit Decorator Metadata
Source: https://trigger.dev/docs/config/extensions/emitDecoratorMetadata
Use the emitDecoratorMetadata build extension to enable support for the emitDecoratorMetadata TypeScript compiler option
If you need support for the `emitDecoratorMetadata` typescript compiler option, import the `emitDecoratorMetadata` build extension and use it in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { emitDecoratorMetadata } from "@trigger.dev/build/extensions/typescript";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [emitDecoratorMetadata()],
},
});
```
This is usually required if you are using certain ORMs, like TypeORM, that require this option to be enabled. It's not enabled by default because there is a performance cost to enabling it.
<Note>
emitDecoratorMetadata works by hooking into the esbuild bundle process and using the TypeScript
compiler API to compile files where we detect the use of decorators. This means you must have
`emitDecoratorMetadata` enabled in your `tsconfig.json` file, as well as `typescript` installed in
your `devDependencies`.
</Note>
# esbuild Plugin
Source: https://trigger.dev/docs/config/extensions/esbuildPlugin
Use the esbuildPlugin build extension to add existing or custom esbuild plugins to your build process
You can easily add existing or custom esbuild plugins to your build process using the `esbuildPlugin` extension:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { esbuildPlugin } from "@trigger.dev/build/extensions";
import { sentryEsbuildPlugin } from "@sentry/esbuild-plugin";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
esbuildPlugin(
sentryEsbuildPlugin({
org: process.env.SENTRY_ORG,
project: process.env.SENTRY_PROJECT,
authToken: process.env.SENTRY_AUTH_TOKEN,
}),
// optional - only runs during the deploy command, and adds the plugin to the end of the list of plugins
{ placement: "last", target: "deploy" }
),
],
},
});
```
# FFmpeg
Source: https://trigger.dev/docs/config/extensions/ffmpeg
Use the ffmpeg build extension to include FFmpeg in your project
You can add the `ffmpeg` build extension to your build process:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { ffmpeg } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [ffmpeg()],
},
});
```
By default, this will install the version of `ffmpeg` that is available in the Debian package manager. If you need a specific version, you can pass in the version as an argument:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { ffmpeg } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [ffmpeg({ version: "6.0-4" })],
},
});
```
This extension will also add the `FFMPEG_PATH` and `FFPROBE_PATH` to your environment variables, making it easy to use popular ffmpeg libraries like `fluent-ffmpeg`.
Note that `fluent-ffmpeg` needs to be added to [`external`](/config/config-file#external) in your `trigger.config.ts` file.
Follow [this example](/guides/examples/ffmpeg-video-processing) to get setup with Trigger.dev and FFmpeg in your project.
# Build extensions
Source: https://trigger.dev/docs/config/extensions/overview
Customize how your project is built and deployed to Trigger.dev with build extensions
Build extensions allow you to hook into the build system and customize the build process or the resulting bundle and container image (in the case of deploying).
You can use pre-built extensions by installing the `@trigger.dev/build` package into your `devDependencies`, or you can create your own.
Build extensions are added to your `trigger.config.ts` file under the `build.extensions` property:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "my-project",
build: {
extensions: [
{
name: "my-extension",
onBuildStart: async (context) => {
console.log("Build starting!");
},
},
],
},
});
```
If you are using a pre-built extension, you can import it from the `@trigger.dev/build` package:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { ffmpeg } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "my-project",
build: {
extensions: [ffmpeg()],
},
});
```
## Built-in extensions
Trigger.dev provides a set of built-in extensions that you can use to customize how your project is built and deployed. These extensions are available out of the box and can be configured in your `trigger.config.ts` file.
| Extension                                                             | Description                                                                    |
| :-------------------------------------------------------------------- | :----------------------------------------------------------------------------- |
| [prismaExtension](/config/extensions/prismaExtension)                 | Using prisma in your Trigger.dev tasks                                         |
| [pythonExtension](/config/extensions/pythonExtension)                 | Execute Python scripts in your project                                         |
| [puppeteer](/config/extensions/puppeteer)                             | Use Puppeteer in your Trigger.dev tasks                                        |
| [ffmpeg](/config/extensions/ffmpeg)                                   | Use FFmpeg in your Trigger.dev tasks                                           |
| [aptGet](/config/extensions/aptGet)                                   | Install system packages in your build image                                    |
| [additionalFiles](/config/extensions/additionalFiles)                 | Copy additional files to your build image                                      |
| [additionalPackages](/config/extensions/additionalPackages)           | Install additional npm packages in your build image                            |
| [syncEnvVars](/config/extensions/syncEnvVars)                         | Automatically sync environment variables from external services to Trigger.dev |
| [syncVercelEnvVars](/config/extensions/syncEnvVars#syncVercelEnvVars) | Automatically sync environment variables from Vercel to Trigger.dev            |
| [esbuildPlugin](/config/extensions/esbuildPlugin)                     | Add existing or custom esbuild extensions to customize your build process      |
| [emitDecoratorMetadata](/config/extensions/emitDecoratorMetadata)     | Enable `emitDecoratorMetadata` in your TypeScript build                        |
| [audioWaveform](/config/extensions/audioWaveform)                     | Add Audio Waveform to your build image                                         |
## Custom extensions
If one of the built-in extensions doesn't meet your needs, you can create your own custom extension. See our [guide on creating custom build extensions](/config/extensions/custom) for more information.
# Prisma
Source: https://trigger.dev/docs/config/extensions/prismaExtension
Use the prismaExtension build extension to use Prisma with Trigger.dev
If you are using Prisma, you should use the prisma build extension.
* Automatically handles copying Prisma files to the build directory
* Generates the Prisma client during the deploy process
* Optionally will migrate the database during the deploy process
* Support for TypedSQL and multiple schema files
* You can use `prismaSchemaFolder` to specify just the directory containing your schema file, instead of the full path
* You can add the extension twice if you have multiple separate schemas in the same project (example below)
You can use it for a simple Prisma setup like this:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { prismaExtension } from "@trigger.dev/build/extensions/prisma";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
prismaExtension({
version: "5.19.0", // optional, we'll automatically detect the version if not provided
schema: "prisma/schema.prisma",
}),
],
},
});
```
<Note>
This does not have any effect when running the `dev` command, only when running the `deploy`
command.
</Note>
### Migrations
If you want to also run migrations during the build process, you can pass in the `migrate` option:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { prismaExtension } from "@trigger.dev/build/extensions/prisma";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
prismaExtension({
schema: "prisma/schema.prisma",
migrate: true,
directUrlEnvVarName: "DATABASE_URL_UNPOOLED", // optional - the name of the environment variable that contains the direct database URL if you are using a direct database URL
}),
],
},
});
```
### clientGenerator
If you have multiple `generator` statements defined in your schema file, you can pass in the `clientGenerator` option to specify the `prisma-client-js` generator, which will prevent other generators from being generated. Some examples where you may need to do this include when using the `prisma-kysely` or `prisma-json-types-generator` generators.
<CodeGroup>
```prisma schema.prisma
datasource db {
provider  = "postgresql"
url       = env("DATABASE_URL")
directUrl = env("DATABASE_URL_UNPOOLED")
}
// We only want to generate the prisma-client-js generator
generator client {
provider        = "prisma-client-js"
}
generator kysely {
provider     = "prisma-kysely"
output       = "../../src/kysely"
enumFileName = "enums.ts"
fileName     = "types.ts"
}
generator json {
provider = "prisma-json-types-generator"
}
```
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { prismaExtension } from "@trigger.dev/build/extensions/prisma";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
prismaExtension({
schema: "prisma/schema.prisma",
clientGenerator: "client",
}),
],
},
});
```
</CodeGroup>
### TypedSQL
If you are using [TypedSQL](https://www.prisma.io/typedsql), you'll need to enable it via the `typedSql` option:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
prismaExtension({
schema: "prisma/schema.prisma",
typedSql: true,
}),
],
},
});
```
<Note>
The `prismaExtension` will inject the `DATABASE_URL` environment variable into the build process. Learn more about setting environment variables for deploying in our [Environment Variables](/deploy-environment-variables) guide.
These environment variables are only used during the build process and are not embedded in the final container image.
</Note>
### Multiple schemas
If you have multiple separate schemas in the same project you can add the extension multiple times:
```ts
prismaExtension({
schema: 'prisma/schema/main.prisma',
version: '6.2.0',
migrate: false,
}),
prismaExtension({
schema: 'prisma/schema/secondary.prisma',
version: '6.2.0',
migrate: false,
}),
```
# Puppeteer
Source: https://trigger.dev/docs/config/extensions/puppeteer
Use the puppeteer build extension to enable support for Puppeteer in your project
<ScrapingWarning />
To use Puppeteer in your project, add these build settings to your `trigger.config.ts` file:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { puppeteer } from "@trigger.dev/build/extensions/puppeteer";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [puppeteer()],
},
});
```
And add the following environment variable in your Trigger.dev dashboard on the Environment Variables page:
```bash
PUPPETEER_EXECUTABLE_PATH: "/usr/bin/google-chrome-stable",
```
Follow [this example](/guides/examples/puppeteer) to get setup with Trigger.dev and Puppeteer in your project.
# Python
Source: https://trigger.dev/docs/config/extensions/pythonExtension
Use the python build extension to add support for executing Python scripts in your project
If you need to execute Python scripts in your Trigger.dev project, you can use the `pythonExtension` build extension via the `@trigger.dev/python` package.
First, you'll need to install the `@trigger.dev/python` package:
```bash
npm add @trigger.dev/python
```
Then, you can use the `pythonExtension` build extension in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { pythonExtension } from "@trigger.dev/python/extension";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [pythonExtension()],
},
});
```
This will take care of adding python to the build image and setting up the necessary environment variables to execute Python scripts. You can then use our `python` utilities in the `@trigger.dev/python` package to execute Python scripts in your tasks. For example, running a Python script inline in a task:
```ts
import { task } from "@trigger.dev/sdk/v3";
import { python } from "@trigger.dev/python";
export const myScript = task({
id: "my-python-script",
run: async () => {
const result = await python.runInline(`print("Hello, world!")`);
return result.stdout;
},
});
```
## Adding python scripts
You can automatically add python scripts to your project using the `scripts` option in the `pythonExtension` function. This will copy the specified scripts to the build directory during the deploy process. For example:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { pythonExtension } from "@trigger.dev/python/extension";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
pythonExtension({
scripts: ["./python/**/*.py"],
}),
],
},
});
```
This will copy all Python files in the `python` directory to the build directory during the deploy process. You can then execute these scripts using the `python.runScript` function:
```ts
import { task } from "@trigger.dev/sdk/v3";
import { python } from "@trigger.dev/python";
export const myScript = task({
id: "my-python-script",
run: async () => {
const result = await python.runScript("./python/my_script.py", ["hello", "world"]);
return result.stdout;
},
});
```
<Note>
The pythonExtension will also take care of moving the scripts to the correct location during `dev`
mode, so you can use the same exact path in development as you do in production.
</Note>
## Using requirements files
If you have a `requirements.txt` file in your project, you can use the `requirementsFile` option in the `pythonExtension` function to install the required packages during the build process. For example:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { pythonExtension } from "@trigger.dev/python/extension";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
pythonExtension({
requirementsFile: "./requirements.txt",
}),
],
},
});
```
This will install the packages specified in the `requirements.txt` file during the build process. You can then use these packages in your Python scripts.
<Note>
The `requirementsFile` option is only available in production mode. In development mode, you can
install the required packages manually using the `pip` command.
</Note>
## Virtual environments
If you are using a virtual environment in your project, you can use the `devPythonBinaryPath` option in the `pythonExtension` function to specify the path to the Python binary in the virtual environment. For example:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { pythonExtension } from "@trigger.dev/python/extension";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
pythonExtension({
devPythonBinaryPath: ".venv/bin/python",
}),
],
},
});
```
This has no effect in production mode, but in development mode, it will use the specified Python binary to execute Python scripts.
## Streaming output
All of the `python` functions have a streaming version that allows you to stream the output of the Python script as it runs. For example:
```ts
import { task } from "@trigger.dev/sdk/v3";
import { python } from "@trigger.dev/python";
export const myStreamingScript = task({
id: "my-streaming-python-script",
run: async () => {
// You don't need to await the result
const result = python.stream.runScript("./python/my_script.py", ["hello", "world"]);
// result is an async iterable/readable stream
for await (const chunk of streamingResult) {
console.log(chunk);
}
},
});
```
## Environment variables
We automatically inject the environment variables in the `process.env` object when running Python scripts. You can access these environment variables in your Python scripts using the `os.environ` dictionary. For example:
```python
import os
print(os.environ["MY_ENV_VAR"])
```
You can also pass additional environment variables to the Python script using the `env` option in the `python.runScript` function. For example:
```ts
import { task } from "@trigger.dev/sdk/v3";
import { python } from "@trigger.dev/python";
export const myScript = task({
id: "my-python-script",
run: async () => {
const result = await python.runScript("./python/my_script.py", ["hello", "world"], {
env: {
MY_ENV_VAR: "my value",
},
});
return result.stdout;
},
});
```
# Sync env vars
Source: https://trigger.dev/docs/config/extensions/syncEnvVars
Use the syncEnvVars build extension to automatically sync environment variables to Trigger.dev
The `syncEnvVars` build extension will sync env vars from another service into Trigger.dev before the deployment starts. This is useful if you are using a secret store service like Infisical or AWS Secrets Manager to store your secrets.
`syncEnvVars` takes an async callback function, and any env vars returned from the callback will be synced to Trigger.dev.
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncEnvVars } from "@trigger.dev/build/extensions/core";
export default defineConfig({
build: {
extensions: [
syncEnvVars(async (ctx) => {
return [
{ name: "SECRET_KEY", value: "secret-value" },
{ name: "ANOTHER_SECRET", value: "another-secret-value" },
];
}),
],
},
});
```
The callback is passed a context object with the following properties:
* `environment`: The environment name that the task is being deployed to (e.g. `production`, `staging`, etc.)
* `projectRef`: The project ref of the Trigger.dev project
* `env`: The environment variables that are currently set in the Trigger.dev project
### Example: Sync env vars from Infisical
In this example we're using env vars from [Infisical](https://infisical.com).
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncEnvVars } from "@trigger.dev/build/extensions/core";
import { InfisicalSDK } from "@infisical/sdk";
export default defineConfig({
build: {
extensions: [
syncEnvVars(async (ctx) => {
const client = new InfisicalSDK();
await client.auth().universalAuth.login({
clientId: process.env.INFISICAL_CLIENT_ID!,
clientSecret: process.env.INFISICAL_CLIENT_SECRET!,
});
const { secrets } = await client.secrets().listSecrets({
environment: ctx.environment,
projectId: process.env.INFISICAL_PROJECT_ID!,
});
return secrets.map((secret) => ({
name: secret.secretKey,
value: secret.secretValue,
}));
}),
],
},
});
```
### syncVercelEnvVars
The `syncVercelEnvVars` build extension syncs environment variables from your Vercel project to Trigger.dev.
<Note>
You need to set the `VERCEL_ACCESS_TOKEN` and `VERCEL_PROJECT_ID` environment variables, or pass
in the token and project ID as arguments to the `syncVercelEnvVars` build extension. If you're
working with a team project, you'll also need to set `VERCEL_TEAM_ID`, which can be found in your
team settings. You can find / generate the `VERCEL_ACCESS_TOKEN` in your Vercel
[dashboard](https://vercel.com/account/settings/tokens). Make sure the scope of the token covers
the project with the environment variables you want to sync.
</Note>
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncVercelEnvVars } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
// This will automatically use the VERCEL_ACCESS_TOKEN and VERCEL_PROJECT_ID environment variables
extensions: [syncVercelEnvVars()],
},
});
```
Or you can pass in the token and project ID as arguments:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncVercelEnvVars } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
syncVercelEnvVars({
projectId: "your-vercel-project-id",
vercelAccessToken: "your-vercel-access-token",
vercelTeamId: "your-vercel-team-id", // optional
}),
],
},
});
```
# Context
Source: https://trigger.dev/docs/context
Get the context of a task run.
Context (`ctx`) is a way to get information about a run.
<Note>
The context object does not change whilst your code is executing. This means values like `ctx.run.durationMs` will be fixed at the moment the `run()` function is called.
</Note>
<RequestExample>
```typescript Context example
import { task } from "@trigger.dev/sdk/v3";
export const parentTask = task({
id: "parent-task",
run: async (payload: { message: string }, { ctx }) => {
if (ctx.environment.type === "DEVELOPMENT") {
return;
}
},
});
```
</RequestExample>
## Context properties
<ResponseField name="task" type="object">
<Expandable title="properties" defaultOpen={true}>
<ResponseField name="exportName" type="string">
The exported function name of the task e.g. `myTask` if you defined it like this: `export const myTask = task(...)`.
</ResponseField>
<ResponseField name="id" type="string">
The ID of the task.
</ResponseField>
<ResponseField name="filePath" type="string">
The file path of the task.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="attempt" type="object">
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the execution attempt.
</ResponseField>
<ResponseField name="number" type="number">
The attempt number.
</ResponseField>
<ResponseField name="startedAt" type="date">
The start time of the attempt.
</ResponseField>
<ResponseField name="backgroundWorkerId" type="string">
The ID of the background worker.
</ResponseField>
<ResponseField name="backgroundWorkerTaskId" type="string">
The ID of the background worker task.
</ResponseField>
<ResponseField name="status" type="string">
The current status of the attempt.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="run" type="object">
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the task run.
</ResponseField>
<ResponseField name="context" type="any" optional>
The context of the task run.
</ResponseField>
<ResponseField name="tags" type="array">
An array of [tags](/tags) associated with the task run.
</ResponseField>
<ResponseField name="isTest" type="boolean">
Whether this is a [test run](/run-tests).
</ResponseField>
<ResponseField name="createdAt" type="date">
The creation time of the task run.
</ResponseField>
<ResponseField name="startedAt" type="date">
The start time of the task run.
</ResponseField>
<ResponseField name="idempotencyKey" type="string" optional>
An optional [idempotency key](/idempotency) for the task run.
</ResponseField>
<ResponseField name="maxAttempts" type="number" optional>
The [maximum number of attempts](/triggering#maxattempts) allowed for this task run.
</ResponseField>
<ResponseField name="durationMs" type="number">
The duration of the task run in milliseconds when the `run()` function is called. For live values use the [usage SDK functions](/run-usage).
</ResponseField>
<ResponseField name="costInCents" type="number">
The cost of the task run in cents when the `run()` function is called. For live values use the [usage SDK functions](/run-usage).
</ResponseField>
<ResponseField name="baseCostInCents" type="number">
The base cost of the task run in cents when the `run()` function is called. For live values use the [usage SDK functions](/run-usage).
</ResponseField>
<ResponseField name="version" type="string" optional>
The [version](/versioning) of the task run.
</ResponseField>
<ResponseField name="maxDuration" type="number" optional>
The [maximum allowed duration](/runs/max-duration) for the task run.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="queue" type="object">
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the queue.
</ResponseField>
<ResponseField name="name" type="string">
The name of the queue.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="environment" type="object">
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the environment.
</ResponseField>
<ResponseField name="slug" type="string">
The slug of the environment.
</ResponseField>
<ResponseField name="type" type="string">
The type of the environment (PRODUCTION, STAGING, DEVELOPMENT, or PREVIEW).
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="organization" type="object">
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the organization.
</ResponseField>
<ResponseField name="slug" type="string">
The slug of the organization.
</ResponseField>
<ResponseField name="name" type="string">
The name of the organization.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="project" type="object">
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the project.
</ResponseField>
<ResponseField name="ref" type="string">
The reference of the project.
</ResponseField>
<ResponseField name="slug" type="string">
The slug of the project.
</ResponseField>
<ResponseField name="name" type="string">
The name of the project.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="batch" type="object" optional>
Optional information about the batch, if applicable.
<Expandable title="properties">
<ResponseField name="id" type="string">
The ID of the batch.
</ResponseField>
</Expandable>
</ResponseField>
<ResponseField name="machine" type="object" optional>
Optional information about the machine preset used for execution.
<Expandable title="properties">
<ResponseField name="name" type="string">
The name of the machine preset.
</ResponseField>
<ResponseField name="cpu" type="number">
The CPU allocation for the machine.
</ResponseField>
<ResponseField name="memory" type="number">
The memory allocation for the machine.
</ResponseField>
<ResponseField name="centsPerMs" type="number">
The cost in cents per millisecond for this machine preset.
</ResponseField>
</Expandable>
</ResponseField>
# Environment Variables
Source: https://trigger.dev/docs/deploy-environment-variables
Any environment variables used in your tasks need to be added so the deployed code will run successfully.
An environment variable in Node.js is accessed in your code using `process.env.MY_ENV_VAR`.
We deploy your tasks and scale them up and down when they are triggered. So any environment variables you use in your tasks need to accessible to us so your code will run successfully.
## In the dashboard
### Setting environment variables
<Steps>
<Step title="Go to the Environment Variables page">
In the sidebar select the "Environment Variables" page, then press the "New environment variable"
button. ![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-page.jpg)
</Step>
<Step title="Add your environment variables">
You can add values for your local dev environment, staging and prod. ![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-panel.jpg)
</Step>
</Steps>
<Note>
Specifying Dev values is optional. They will be overriden by values in your .env file when running
locally.
</Note>
### Editing environment variables
You can edit an environment variable's values. You cannot edit the key name, you must delete and create a new one.
<Steps>
<Step title="Press the action button on a variable">
![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-actions.png)
</Step>
<Step title="Press edit">
![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-edit-popover.png)
</Step>
</Steps>
### Deleting environment variables
<Warn>
Environment variables are fetched and injected before a runs begins. So if you delete one you can
cause runs to fail that are expecting variables to be set.
</Warn>
<Steps>
<Step title="Press the action button on a variable">
![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-actions.png)
</Step>
<Step title="Press delete">
This will immediately delete the variable. ![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-delete-popover.png)
</Step>
</Steps>
## In your code
You can use our SDK to get and manipulate environment variables. You can also easily sync environment variables from another service into Trigger.dev.
### Directly manipulating environment variables
We have a complete set of SDK functions (and REST API) you can use to directly manipulate environment variables.
| Function                                           | Description                                                 |
| -------------------------------------------------- | ----------------------------------------------------------- |
| [envvars.list()](/management/envvars/list)         | List all environment variables                              |
| [envvars.upload()](/management/envvars/import)     | Upload multiple env vars. You can override existing values. |
| [envvars.create()](/management/envvars/create)     | Create a new environment variable                           |
| [envvars.retrieve()](/management/envvars/retrieve) | Retrieve an environment variable                            |
| [envvars.update()](/management/envvars/update)     | Update a single environment variable                        |
| [envvars.del()](/management/envvars/delete)        | Delete a single environment variable                        |
### Sync env vars from another service
You could use the SDK functions above but it's much easier to use our `syncEnvVars` build extension in your `trigger.config` file.
<Note>
To use the `syncEnvVars` build extension, you should first install the `@trigger.dev/build`
package into your devDependencies.
</Note>
In this example we're using env vars from [Infisical](https://infisical.com).
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncEnvVars } from "@trigger.dev/build/extensions/core";
import { InfisicalSDK } from "@infisical/sdk";
export default defineConfig({
build: {
extensions: [
syncEnvVars(async (ctx) => {
const client = new InfisicalSDK();
await client.auth().universalAuth.login({
clientId: process.env.INFISICAL_CLIENT_ID!,
clientSecret: process.env.INFISICAL_CLIENT_SECRET!,
});
const { secrets } = await client.secrets().listSecrets({
environment: ctx.environment,
projectId: process.env.INFISICAL_PROJECT_ID!,
});
return secrets.map((secret) => ({
name: secret.secretKey,
value: secret.secretValue,
}));
}),
],
},
});
```
#### Syncing environment variables from Vercel
To sync environment variables from your Vercel projects to Trigger.dev, you can use our build extension. Check out our [syncing environment variables from Vercel guide](/guides/examples/vercel-sync-env-vars).
#### Deploy
When you run the [CLI deploy command](/cli-deploy) directly or using [GitHub Actions](/github-actions) it will sync the environment variables from [Infisical](https://infisical.com) to Trigger.dev. This means they'll appear on the Environment Variables page so you can confirm that it's worked.
This means that you need to redeploy your Trigger.dev tasks if you change the environment variables in [Infisical](https://infisical.com).
<Note>
The `process.env.INFISICAL_CLIENT_ID`, `process.env.INFISICAL_CLIENT_SECRET` and
`process.env.INFISICAL_PROJECT_ID` will need to be supplied to the `deploy` CLI command. You can
do this via the `--env-file .env` flag or by setting them as environment variables in your
terminal.
</Note>
#### Dev
`syncEnvVars` does not have any effect when running the `dev` command locally. If you want to inject environment variables from another service into your local environment you can do so via a `.env` file or just supplying them as environment variables in your terminal. Most services will have a CLI tool that allows you to run a command with environment variables set:
```sh
infisical run -- npx trigger.dev@latest dev
```
Any environment variables set in the CLI command will be available to your local Trigger.dev tasks.
### The syncEnvVars callback return type
You can return env vars as an object with string keys and values, or an array of names + values.
```ts
return {
MY_ENV_VAR: "my value",
MY_OTHER_ENV_VAR: "my other value",
};
```
or
```ts
return [
{
name: "MY_ENV_VAR",
value: "my value",
},
{
name: "MY_OTHER_ENV_VAR",
value: "my other value",
},
];
```
This should mean that for most secret services you won't need to convert the data into a different format.
### Using Google credential JSON files
Securely pass a Google credential JSON file to your Trigger.dev task using environment variables.
<Steps>
<Step title="Convert the Google credential file to base64">
In your terminal, run the following command and copy the resulting base64 string:
```
base64 -i path/to/your/service-account-file.json
```
</Step>
<Step title="Set up the environment variable in Trigger.dev">
Follow [these steps](/deploy-environment-variables) to set a new environment variable using the base64 string as the value.
```
GOOGLE_CREDENTIALS_BASE64="<your base64 string>"
```
</Step>
<Step title="Use the environment variable in your code">
Add the following code to your Trigger.dev task:
```ts
import { google } from "googleapis";
const credentials = JSON.parse(
Buffer.from(process.env.GOOGLE_CREDENTIALS_BASE64, "base64").toString("utf8")
);
const auth = new google.auth.GoogleAuth({
credentials,
scopes: ["https://www.googleapis.com/auth/cloud-platform"],
});
const client = await auth.getClient();
```
</Step>
<Step title="Use the client in your code">
You can now use the `client` object to make authenticated requests to Google APIs
</Step>
</Steps>
# Atomic deploys
Source: https://trigger.dev/docs/deployment/atomic-deployment
Use atomic deploys to coordinate changes to your tasks and your application.
Atomic deploys in Trigger.dev allow you to synchronize the deployment of your application with a specific version of your tasks. This ensures that your application always uses the correct version of its associated tasks, preventing inconsistencies or errors due to version mismatches.
## How it works
Atomic deploys achieve synchronization by deploying your tasks to Trigger.dev without promoting them to the default version. Instead, you explicitly specify the deployed task version in your applicationâ€™s environment. Hereâ€™s the process at a glance:
1. **Deploy Tasks to Trigger.dev**: Use the Trigger.dev CLI to deploy your tasks with the `--skip-promotion` flag. This creates a new task version without making it the default.
2. **Capture the Deployment Version**: The CLI outputs the version of the deployed tasks, which youâ€™ll use in the next step.
3. **Deploy Your Application**: Deploy your application (e.g., to Vercel), setting an environment variable like `TRIGGER_VERSION` to the captured task version.
## Vercel CLI & GitHub Actions
If you deploy to Vercel via their CLI, you can use this sample workflow that demonstrates performing atomic deploys with GitHub Actions, Trigger.dev, and Vercel:
```yml
name: Deploy to Trigger.dev (prod)
on:
push:
branches:
- main
concurrency:
group: ${{ github.workflow }}
cancel-in-progress: true
jobs:
deploy:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v4
- name: Use Node.js 20.x
uses: actions/setup-node@v4
with:
node-version: "20.x"
- name: Install dependencies
run: npm install
- name: Deploy Trigger.dev
id: deploy-trigger
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
run: |
npx trigger.dev@latest deploy --skip-promotion
- name: Deploy to Vercel
run: npx vercel --yes --prod -e TRIGGER_VERSION=$TRIGGER_VERSION --token $VERCEL_TOKEN
env:
VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
TRIGGER_VERSION: ${{ steps.deploy-trigger.outputs.deploymentVersion }}
- name: Promote Trigger.dev Version
if: false
run: npx trigger.dev@latest promote $TRIGGER_VERSION
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
TRIGGER_VERSION: ${{ steps.deploy-trigger.outputs.deploymentVersion }}
```
* Deploy to Trigger.dev
* The `npx trigger.dev deploy` command uses `--skip-promotion` to deploy the tasks without setting the version as the default.
* The stepâ€™s id: `deploy-trigger` allows us to capture the deployment version in the output (deploymentVersion).
* Deploy to Vercel:
* The `npx vercel` command deploys the application, setting the `TRIGGER_VERSION` environment variable to the task version from the previous step.
* The --prod flag ensures a production deployment, and -e passes the environment variable.
* The `@trigger.dev/sdk` automatically uses the `TRIGGER_VERSION` environment variable to trigger the correct version of the tasks.
For this workflow to work, you need to set up the following secrets in your GitHub repository:
* `TRIGGER_ACCESS_TOKEN`: Your Trigger.dev personal access token. View the instructions [here](/github-actions) to learn more.
* `VERCEL_TOKEN`: Your Vercel personal access token. You can find this in your Vercel account settings.
## Vercel GitHub integration
If you're are using Vercel, chances are you are using their GitHub integration and deploying your application directly from pushes to GitHub. This section covers how to achieve atomic deploys with Trigger.dev in this setup.
### Turn off automatic promotion
By default, Vercel automatically promotes new deployments to production. To prevent this, you need to disable the auto-promotion feature in your Vercel project settings:
1. Go to your Production environment settings in Vercel at `https://vercel.com/<team-slug>/<project-slug>/settings/environments/production`
2. Disable the "Auto-assign Custom Production Domains" setting:
![Vercel project settings showing the auto-promotion setting](https://mintlify.s3.us-west-1.amazonaws.com/trigger/deployment/auto-assign-production-domains.png)
3. Hit the "Save" button to apply the changes.
Now whenever you push to your main branch, Vercel will deploy your application to the production environment without promoting it, and you can control the promotion manually.
### Deploy with Trigger.dev
Now we want to deploy that same commit to Trigger.dev, and then promote the Vercel deployment when that completes. Here's a sample GitHub Actions workflow that does this:
```yml
name: Deploy to Trigger.dev (prod)
on:
push:
branches:
- main
concurrency:
group: ${{ github.workflow }}
cancel-in-progress: true
jobs:
deploy:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v4
- name: Use Node.js 20.x
uses: actions/setup-node@v4
with:
node-version: "20.x"
- name: Install dependencies
run: npm install
- name: Wait for vercel deployment (push)
id: wait-for-vercel
uses: ludalex/vercel-wait@v1
with:
project-id: ${{ secrets.VERCEL_PROJECT_ID }}
team-id: ${{ secrets.VERCEL_SCOPE_NAME }}
token: ${{ secrets.VERCEL_TOKEN }}
sha: ${{ github.sha }}
- name: ðŸš€ Deploy Trigger.dev
id: deploy-trigger
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
run: |
npx trigger.dev@latest deploy
- name: Promote Vercel deploy
run: npx vercel promote $VERCEL_DEPLOYMENT_ID --yes --token $VERCEL_TOKEN --scope $VERCEL_SCOPE_NAME
env:
VERCEL_DEPLOYMENT_ID: ${{ steps.wait-for-vercel.outputs.deployment-id }}
VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
VERCEL_SCOPE_NAME: ${{ secrets.VERCEL_SCOPE_NAME }}
```
This workflow does the following:
1. Waits for the Vercel deployment to complete using the `ludalex/vercel-wait` action.
2. Deploys the tasks to Trigger.dev using the `npx trigger.dev deploy` command. There's no need to use the `--skip-promotion` flag because we want to promote the deployment.
3. Promotes the Vercel deployment using the `npx vercel promote` command.
For this workflow to work, you need to set up the following secrets in your GitHub repository:
* `TRIGGER_ACCESS_TOKEN`: Your Trigger.dev personal access token. View the instructions [here](/github-actions) to learn more.
* `VERCEL_TOKEN`: Your Vercel personal access token. You can find this in your Vercel account settings.
* `VERCEL_PROJECT_ID`: Your Vercel project ID. You can find this in your Vercel project settings.
* `VERCEL_SCOPE_NAME`: Your Vercel team slug.
Checkout our [example repo](https://github.com/ericallam/vercel-atomic-deploys) to see this workflow in action.
<Note>
We are using the `ludalex/vercel-wait` action above as a fork of the [official
tj-actions/vercel-wait](https://github.com/tj-actions/vercel-wait) action because there is a bug
in the official action that exits early if the deployment isn't found in the first check and due
to the fact that it supports treating skipped (cancelled) Vercel deployments as valid (on by default).
I've opened a PR for this issue [here](https://github.com/tj-actions/vercel-wait/pull/106).
</Note>
# Deployment
Source: https://trigger.dev/docs/deployment/overview
Learn how to deploy your tasks to Trigger.dev.
Before you can run production workloads on Trigger.dev, you need to deploy your tasks. The only way to do this at the moment is through the [deploy CLI command](/cli-deploy):
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
## Deploying 101
Let's assume you have an existing trigger.dev project with a few tasks that you have been running locally but now want to deploy to the Trigger.dev cloud (or your self-hosted instance).
First, let's make sure you are logged in to the CLI (if you haven't already):
```bash
npx trigger.dev login
```
This will open a browser window where you can log in with your Trigger.dev account and link your CLI.
Now you can deploy your tasks:
```bash
npx trigger.dev deploy
```
This should print out a success message and let you know a new version has been deployed:
```bash
Trigger.dev (3.3.16)
------------------------------------------------------
â”Œ  Deploying project
â”‚
â—‡  Retrieved your account details for eric@trigger.dev
â”‚
â—‡  Successfully built code
â”‚
â—‡  Successfully deployed version 20250228.1
â”‚
â””  Version 20250228.1 deployed with 4 detected tasks
```
Now if you visit your Trigger.dev dashboard you should see the new version deployed:
![Trigger.dev dashboard showing the latest version deployed](https://mintlify.s3.us-west-1.amazonaws.com/trigger/deployment/my-first-deployment.png)
<Note>
Deploying consists of building your tasks and uploading them to the Trigger.dev cloud. This
process can take a few seconds to a few minutes depending on the size of your project.
</Note>
## Triggering deployed tasks
Once you have deployed your tasks, you can trigger tasks exactly the same way you did locally, but with the "PROD" API key:
![Trigger.dev dashboard showing the API key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/deployment/api-key.png)
Copy the API key from the dashboard and set the `TRIGGER_SECRET_KEY` environment variable, and then any tasks you trigger will run against the deployed version:
```txt .env
TRIGGER_SECRET_KEY="tr_prod_abc123"
```
Now you can trigger your tasks:
```ts
import { myTask } from "./trigger/tasks";
await myTask.trigger({ foo: "bar" });
```
See our [triggering tasks](/triggering) guide for more information.
## Versions
When you deploy your tasks, Trigger.dev creates a new version of all tasks in your project. A version is a snapshot of your tasks at a certain point in time. This ensures that tasks are not affected by changes to the code.
### Current version
When you deploy, the version number is automatically incremented, and the new version is set as the current version for that environment.
<Note>
A single environment (prod, staging, etc.) can only have a single "current" version at a time.
</Note>
The current version defines which version of the code new task runs will execute against. When a task run starts, it is locked to the current version. This ensures that the task run is not affected by changes to the code. Retries of the task run will also be locked to the original version.
<Note>
When you Replay a run in the dashboard we will create a new run, locked to the current version and
not necessarily the version of the original run.
</Note>
### Version locking
You can optionally specify the version when triggering a task using the `version` parameter. This is useful when you want to run a task against a specific version of the code:
```ts
await myTask.trigger({ foo: "bar" }, { version: "20250228.1" });
```
If you want to set a global version to run all tasks against, you can use the `TRIGGER_VERSION` environment variable:
```bash
TRIGGER_VERSION=20250228.1
```
### Child tasks and auto-version locking
Trigger and wait functions version lock child task runs to the parent task run version. This ensures the results from child runs match what the parent task is expecting. If you don't wait then version locking doesn't apply.
| Trigger function        | Parent task version | Child task version | isLocked |
| ----------------------- | ------------------- | ------------------ | -------- |
| `trigger()`             | `20240313.2`        | Current            | No       |
| `batchTrigger()`        | `20240313.2`        | Current            | No       |
| `triggerAndWait()`      | `20240313.2`        | `20240313.2`       | Yes      |
| `batchTriggerAndWait()` | `20240313.2`        | `20240313.2`       | Yes      |
### Skipping promotion
When you deploy, the new version is automatically promoted be the current version. If you want to skip this promotion, you can use the `--skip-promotion` flag:
```bash
npx trigger.dev deploy --skip-promotion
```
This will create a new deployment version but not promote it to the current version:
![Trigger.dev dashboard showing the latest version deployed but not promoted](https://mintlify.s3.us-west-1.amazonaws.com/trigger/deployment/skip-promotion.png)
This allows you to deploy and test a new version without affecting new task runs. When you want to promote the version, you can do so from the CLI:
```bash
npx trigger.dev promote 20250228.1
```
Or from the dashboard:
![Trigger.dev dashboard showing the promote button](https://mintlify.s3.us-west-1.amazonaws.com/trigger/deployment/promote-button.png)
To learn more about skipping promotion and how this enables atomic deployments, see our [Atomic deployment](/deployment/atomic-deployment) guide.
## Staging deploys
By default, the `deploy` command will deploy to the `prod` environment. If you want to deploy to a different environment, you can use the `--env` flag:
```bash
npx trigger.dev deploy --env staging
```
<Note>
If you are using the Trigger.dev Cloud, staging deploys are only available on the Hobby and Pro
plans.
</Note>
This will create an entirely new version of your tasks for the `staging` environment, with a new version number and an independent current version:
![Trigger.dev dashboard showing the staging environment](https://mintlify.s3.us-west-1.amazonaws.com/trigger/deployment/staging-deploy.png)
Now you can trigger tasks against the staging environment by setting the `TRIGGER_SECRET_KEY` environment variable to the staging API key:
```txt .env
TRIGGER_SECRET_KEY="tr_stg_abcd123"
```
Currently, we only support two environments: `prod` and `staging`. Multiple environments are on our roadmap which you can track [here](https://feedback.trigger.dev/p/more-environments).
## Environment variables
To add custom environment variables to your deployed tasks, you need to add them to your project in the Trigger.dev dashboard, or automatically sync them using our [syncEnvVars](/config/config-file#syncenvvars) or [syncVercelEnvVars](/config/config-file#syncvercelenvvars) build extensions.
For more information on environment variables, see our [environment variables](/deploy-environment-variables) guide.
## Troubleshooting
When things go wrong with your deployment, there are a few things you can do to diagnose the issue:
### Dry runs
You can do a "dry run" of the deployment to see what is built and uploaded without actually deploying:
```bash
npx trigger.dev deploy --dry-run
#  Dry run complete. View the built project at /<project path>/.trigger/tmp/<build dir>
```
The dry run will output the build directory where you can inspect the built tasks and dependencies. You can also compress this directory and send it to us if you need help debugging.
### Debug logs
You can run the deploy command with `--log-level debug` at the end. This will print out a lot of information about the deploy. If you can't figure out the problem from the information below please join [our Discord](https://trigger.dev/discord) and create a help forum post. Do NOT share the extended debug logs publicly as they might reveal private information about your project.
### Common issues
#### `Failed to build project image: Error building image`
There should be a link below the error message to the full build logs on your machine. Take a look at these to see what went wrong. Join [our Discord](https://trigger.dev/discord) and you share it privately with us if you can't figure out what's going wrong. Do NOT share these publicly as the verbose logs might reveal private information about your project.
#### `Deployment encountered an error`
Usually there will be some useful guidance below this message. If you can't figure out what's going wrong then join [our Discord](https://trigger.dev/discord) and create a Help forum post with a link to your deployment.
#### `No loader is configured for ".node" files`
This happens because `.node` files are native code and can't be bundled like other packages. To fix this, add your package to [`build.external`](/config/config-file#external) in the `trigger.config.ts` file like this:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
external: ["your-node-package"],
},
});
```
### `Cannot find matching keyid`
This error occurs when using Node.js v22 with corepack, as it's not yet compatible with the latest package manager signatures. To fix this, either:
1. Downgrade to Node.js v20 (LTS), or
2. Install corepack globally: `npm i -g corepack@latest`
The corepack bug and workaround are detailed in [this issue](https://github.com/npm/cli/issues/8075).
# Errors & Retrying
Source: https://trigger.dev/docs/errors-retrying
How to deal with errors and write reliable tasks.
When an uncaught error is thrown inside your task, that task attempt will fail.
You can configure retrying in two ways:
1. In your [trigger.config file](/config/config-file) you can set the default retrying behavior for all tasks.
2. On each task you can set the retrying behavior.
<Note>
By default when you create your project using the CLI init command we disabled retrying in the DEV
environment. You can enable it in your [trigger.config file](/config/config-file).
</Note>
## A simple example with OpenAI
This task will retry 10 times with exponential backoff.
* `openai.chat.completions.create()` can throw an error.
* The result can be empty and we want to try again. So we manually throw an error.
```ts /trigger/openai.ts
import { task } from "@trigger.dev/sdk/v3";
import OpenAI from "openai";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
export const openaiTask = task({
id: "openai-task",
//specifying retry options overrides the defaults defined in your trigger.config file
retry: {
maxAttempts: 10,
factor: 1.8,
minTimeoutInMs: 500,
maxTimeoutInMs: 30_000,
randomize: false,
},
run: async (payload: { prompt: string }) => {
//if this fails, it will throw an error and retry
const chatCompletion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
});
if (chatCompletion.choices[0]?.message.content === undefined) {
//sometimes OpenAI returns an empty response, let's retry by throwing an error
throw new Error("OpenAI call failed");
}
return chatCompletion.choices[0].message.content;
},
});
```
## Combining tasks
One way to gain reliability is to break your work into smaller tasks and [trigger](/triggering) them from each other. Each task can have its own retrying behavior:
```ts /trigger/multiple-tasks.ts
import { task } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
retry: {
maxAttempts: 10,
},
run: async (payload: string) => {
const result = await otherTask.triggerAndWait("some data");
//...do other stuff
},
});
export const otherTask = task({
id: "other-task",
retry: {
maxAttempts: 5,
},
run: async (payload: string) => {
return {
foo: "bar",
};
},
});
```
Another benefit of this approach is that you can view the logs and retry each task independently from the dashboard.
## Retrying smaller parts of a task
Another complimentary strategy is to perform retrying inside of your task.
We provide some useful functions that you can use to retry smaller parts of a task. Of course, you can also write your own logic or use other packages.
### retry.onThrow()
You can retry a block of code that can throw an error, with the same retry settings as a task.
```ts /trigger/retry-on-throw.ts
import { task, logger, retry } from "@trigger.dev/sdk/v3";
export const retryOnThrow = task({
id: "retry-on-throw",
run: async (payload: any) => {
//Will retry up to 3 times. If it fails 3 times it will throw.
const result = await retry.onThrow(
async ({ attempt }) => {
//throw on purpose the first 2 times, obviously this is a contrived example
if (attempt < 3) throw new Error("failed");
//...
return {
foo: "bar",
};
},
{ maxAttempts: 3, randomize: false }
);
//this will log out after 3 attempts of retry.onThrow
logger.info("Result", { result });
},
});
```
<Note>
If all of the attempts with `retry.onThrow` fail, an error will be thrown. You can catch this or
let it cause a retry of the entire task.
</Note>
### retry.fetch()
You can use `fetch`, `axios`, or any other library in your code.
But we do provide a convenient function to perform HTTP requests with conditional retrying based on the response:
```ts /trigger/retry-fetch.ts
import { task, logger, retry } from "@trigger.dev/sdk/v3";
export const taskWithFetchRetries = task({
id: "task-with-fetch-retries",
run: async ({ payload, ctx }) => {
//if the Response is a 429 (too many requests), it will retry using the data from the response. A lot of good APIs send these headers.
const headersResponse = await retry.fetch("http://my.host/test-headers", {
retry: {
byStatus: {
"429": {
strategy: "headers",
limitHeader: "x-ratelimit-limit",
remainingHeader: "x-ratelimit-remaining",
resetHeader: "x-ratelimit-reset",
resetFormat: "unix_timestamp_in_ms",
},
},
},
});
const json = await headersResponse.json();
logger.info("Fetched headers response", { json });
//if the Response is a 500-599 (issue with the server you're calling), it will retry up to 10 times with exponential backoff
const backoffResponse = await retry.fetch("http://my.host/test-backoff", {
retry: {
byStatus: {
"500-599": {
strategy: "backoff",
maxAttempts: 10,
factor: 2,
minTimeoutInMs: 1_000,
maxTimeoutInMs: 30_000,
randomize: false,
},
},
},
});
const json2 = await backoffResponse.json();
logger.info("Fetched backoff response", { json2 });
//You can additionally specify a timeout. In this case if the response takes longer than 1 second, it will retry up to 5 times with exponential backoff
const timeoutResponse = await retry.fetch("https://httpbin.org/delay/2", {
timeoutInMs: 1000,
retry: {
timeout: {
maxAttempts: 5,
factor: 1.8,
minTimeoutInMs: 500,
maxTimeoutInMs: 30_000,
randomize: false,
},
},
});
const json3 = await timeoutResponse.json();
logger.info("Fetched timeout response", { json3 });
return {
result: "success",
payload,
json,
json2,
json3,
};
},
});
```
<Note>
If all of the attempts with `retry.fetch` fail, an error will be thrown. You can catch this or let
it cause a retry of the entire task.
</Note>
## Advanced error handling and retrying
We provide a `handleError` callback on the task and in your `trigger.config` file. This gets called when an uncaught error is thrown in your task.
You can
* Inspect the error, log it, and return a different error if you'd like.
* Modify the retrying behavior based on the error, payload, context, etc.
If you don't return anything from the function it will use the settings on the task (or inherited from the config). So you only need to use this to override things.
### OpenAI error handling example
OpenAI calls can fail for a lot of reasons and the ideal retry behavior is different for each.
In this complicated example:
* We skip retrying if there's no Response status.
* We skip retrying if you've run out of credits.
* If there are no Response headers we let the normal retrying logic handle it (return undefined).
* If we've run out of requests or tokens we retry at the time specified in the headers.
<CodeGroup>
```ts tasks.ts
import { task } from "@trigger.dev/sdk/v3";
import { calculateISO8601DurationOpenAIVariantResetAt, openai } from "./openai.js";
export const openaiTask = task({
id: "openai-task",
retry: {
maxAttempts: 1,
},
run: async (payload: { prompt: string }) => {
const chatCompletion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
});
return chatCompletion.choices[0].message.content;
},
handleError: async (payload, error, { ctx, retryAt }) => {
if (error instanceof OpenAI.APIError) {
if (!error.status) {
return {
skipRetrying: true,
};
}
if (error.status === 429 && error.type === "insufficient_quota") {
return {
skipRetrying: true,
};
}
if (!error.headers) {
//returning undefined means the normal retrying logic will be used
return;
}
const remainingRequests = error.headers["x-ratelimit-remaining-requests"];
const requestResets = error.headers["x-ratelimit-reset-requests"];
if (typeof remainingRequests === "string" && Number(remainingRequests) === 0) {
return {
retryAt: calculateISO8601DurationOpenAIVariantResetAt(requestResets),
};
}
const remainingTokens = error.headers["x-ratelimit-remaining-tokens"];
const tokensResets = error.headers["x-ratelimit-reset-tokens"];
if (typeof remainingTokens === "string" && Number(remainingTokens) === 0) {
return {
retryAt: calculateISO8601DurationOpenAIVariantResetAt(tokensResets),
};
}
}
},
});
```
```ts openai.ts
import { OpenAI } from "openai";
export const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });
export function calculateISO8601DurationOpenAIVariantResetAt(
resets: string,
now: Date = new Date()
): Date | undefined {
// Check if the input is null or undefined
if (!resets) return undefined;
// Regular expression to match the duration string pattern
const pattern = /^(?:(\d+)d)?(?:(\d+)h)?(?:(\d+)m)?(?:(\d+(?:\.\d+)?)s)?(?:(\d+)ms)?$/;
const match = resets.match(pattern);
// If the string doesn't match the expected format, return undefined
if (!match) return undefined;
// Extract days, hours, minutes, seconds, and milliseconds from the string
const days = parseInt(match[1] ?? "0", 10) || 0;
const hours = parseInt(match[2] ?? "0", 10) || 0;
const minutes = parseInt(match[3] ?? "0", 10) || 0;
const seconds = parseFloat(match[4] ?? "0") || 0;
const milliseconds = parseInt(match[5] ?? "0", 10) || 0;
// Calculate the future date based on the current date plus the extracted time
const resetAt = new Date(now);
resetAt.setDate(resetAt.getDate() + days);
resetAt.setHours(resetAt.getHours() + hours);
resetAt.setMinutes(resetAt.getMinutes() + minutes);
resetAt.setSeconds(resetAt.getSeconds() + Math.floor(seconds));
resetAt.setMilliseconds(
resetAt.getMilliseconds() + (seconds - Math.floor(seconds)) * 1000 + milliseconds
);
return resetAt;
}
```
</CodeGroup>
## Preventing retries
### Using `AbortTaskRunError`
You can prevent retries by throwing an `AbortTaskRunError`. This will fail the task attempt and disable retrying.
```ts /trigger/myTasks.ts
import { task, AbortTaskRunError } from "@trigger.dev/sdk/v3";
export const openaiTask = task({
id: "openai-task",
run: async (payload: { prompt: string }) => {
//if this fails, it will throw an error and stop retrying
const chatCompletion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
});
if (chatCompletion.choices[0]?.message.content === undefined) {
// If OpenAI returns an empty response, abort retrying
throw new AbortTaskRunError("OpenAI call failed");
}
return chatCompletion.choices[0].message.content;
},
});
```
### Using try/catch
Sometimes you want to catch an error and don't want to retry the task. You can use try/catch as you normally would. In this example we fallback to using Replicate if OpenAI fails.
```ts /trigger/myTasks.ts
import { task } from "@trigger.dev/sdk/v3";
export const openaiTask = task({
id: "openai-task",
run: async (payload: { prompt: string }) => {
try {
//if this fails, it will throw an error and retry
const chatCompletion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
});
if (chatCompletion.choices[0]?.message.content === undefined) {
//sometimes OpenAI returns an empty response, let's retry by throwing an error
throw new Error("OpenAI call failed");
}
return chatCompletion.choices[0].message.content;
} catch (error) {
//use Replicate if OpenAI fails
const prediction = await replicate.run(
"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3",
{
input: {
prompt: payload.prompt,
max_new_tokens: 250,
},
}
);
if (prediction.output === undefined) {
//retry if Replicate fails
throw new Error("Replicate call failed");
}
return prediction.output;
}
},
});
```
# Overview & Authentication
Source: https://trigger.dev/docs/frontend/overview
Using the Trigger.dev SDK from your frontend application.
You can use our [React hooks](/frontend/react-hooks) in your frontend application to interact with the Trigger.dev API. This guide will show you how to generate Public Access Tokens that can be used to authenticate your requests.
## Authentication
To create a Public Access Token, you can use the `auth.createPublicToken` function in your **backend** code:
```tsx
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken(); // ðŸ‘ˆ this public access token has no permissions, so is pretty useless!
```
### Scopes
By default a Public Access Token has no permissions. You must specify the scopes you need when creating a Public Access Token:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
scopes: {
read: {
runs: true, // âŒ this token can read all runs, possibly useful for debugging/testing
},
},
});
```
This will allow the token to read all runs, which is probably not what you want. You can specify only certain runs by passing an array of run IDs:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
scopes: {
read: {
runs: ["run_1234", "run_5678"], // âœ… this token can read only these runs
},
},
});
```
You can scope the token to only read certain tasks:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
scopes: {
read: {
tasks: ["my-task-1", "my-task-2"], // ðŸ‘ˆ this token can read all runs of these tasks
},
},
});
```
Or tags:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
scopes: {
read: {
tags: ["my-tag-1", "my-tag-2"], // ðŸ‘ˆ this token can read all runs with these tags
},
},
});
```
Or a specific batch of runs:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
scopes: {
read: {
batch: "batch_1234", // ðŸ‘ˆ this token can read all runs in this batch
},
},
});
```
You can also combine scopes. For example, to read runs with specific tags and for specific tasks:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
scopes: {
read: {
tasks: ["my-task-1", "my-task-2"],
tags: ["my-tag-1", "my-tag-2"],
},
},
});
```
### Expiration
By default, Public Access Token's expire after 15 minutes. You can specify a different expiration time when creating a Public Access Token:
```ts
import { auth } from "@trigger.dev/sdk/v3";
const publicToken = await auth.createPublicToken({
expirationTime: "1hr",
});
```
* If `expirationTime` is a string, it will be treated as a time span
* If `expirationTime` is a number, it will be treated as a Unix timestamp
* If `expirationTime` is a `Date`, it will be treated as a date
The format used for a time span is the same as the [jose package](https://github.com/panva/jose), which is a number followed by a unit. Valid units are: "sec", "secs", "second", "seconds", "s", "minute", "minutes", "min", "mins", "m", "hour", "hours", "hr", "hrs", "h", "day", "days", "d", "week", "weeks", "w", "year", "years", "yr", "yrs", and "y". It is not possible to specify months. 365.25 days is used as an alias for a year. If the string is suffixed with "ago", or prefixed with a "-", the resulting time span gets subtracted from the current unix timestamp. A "from now" suffix can also be used for readability when adding to the current unix timestamp.
## Auto-generated tokens
When triggering a task from your backend, the `handle` received from the `trigger` function now includes a `publicAccessToken` field. This token can be used to authenticate requests in your frontend application:
```ts
import { tasks } from "@trigger.dev/sdk/v3";
const handle = await tasks.trigger("my-task", { some: "data" });
console.log(handle.publicAccessToken);
```
By default, tokens returned from the `trigger` function expire after 15 minutes and have a read scope for that specific run. You can customize the expiration of the auto-generated tokens by passing a `publicTokenOptions` object to the `trigger` function:
```ts
import { tasks } from "@trigger.dev/sdk/v3";
const handle = await tasks.trigger(
"my-task",
{ some: "data" },
{
tags: ["my-tag"],
},
{
publicAccessToken: {
expirationTime: "1hr",
},
}
);
```
You will also get back a Public Access Token when using the `batchTrigger` function:
```ts
import { tasks } from "@trigger.dev/sdk/v3";
const handle = await tasks.batchTrigger("my-task", [
{ payload: { some: "data" } },
{ payload: { some: "data" } },
{ payload: { some: "data" } },
]);
console.log(handle.publicAccessToken);
```
## Usage
To learn how to use these Public Access Tokens, see our [React hooks](/frontend/react-hooks) guide.
# Overview
Source: https://trigger.dev/docs/frontend/react-hooks/overview
Using the Trigger.dev v3 API from your React application.
Our react hooks package provides a set of hooks that make it easy to interact with the Trigger.dev API from your React application, using our [frontend API](/frontend/overview). You can use these hooks to fetch runs, and subscribe to real-time updates, and trigger tasks from your frontend application.
## Installation
Install the `@trigger.dev/react-hooks` package in your project:
<CodeGroup>
```bash npm
npm add @trigger.dev/react-hooks
```
```bash pnpm
pnpm add @trigger.dev/react-hooks
```
```bash yarn
yarn install @trigger.dev/react-hooks
```
</CodeGroup>
## Authentication
All hooks accept an optional last argument `options` that accepts an `accessToken` param, which should be a valid Public Access Token. Learn more about [generating tokens in the frontend guide](/frontend/overview).
```tsx
import { useRealtimeRun } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, error } = useRealtimeRun(runId, {
accessToken: publicAccessToken, // This is required
baseURL: "https://your-trigger-dev-instance.com", // optional, only needed if you are self-hosting Trigger.dev
});
// ...
}
```
Alternatively, you can use our `TriggerAuthContext` provider
```tsx
import { TriggerAuthContext } from "@trigger.dev/react-hooks";
export function SetupTrigger({ publicAccessToken }: { publicAccessToken: string }) {
return (
<TriggerAuthContext.Provider value={{ accessToken: publicAccessToken }}>
<MyComponent />
</TriggerAuthContext.Provider>
);
}
```
Now children components can use the hooks to interact with the Trigger.dev API. If you are self-hosting Trigger.dev, you can provide the `baseURL` to the `TriggerAuthContext` provider.
```tsx
import { TriggerAuthContext } from "@trigger.dev/react-hooks";
export function SetupTrigger({ publicAccessToken }: { publicAccessToken: string }) {
return (
<TriggerAuthContext.Provider
value={{
accessToken: publicAccessToken,
baseURL: "https://your-trigger-dev-instance.com",
}}
>
<MyComponent />
</TriggerAuthContext.Provider>
);
}
```
### Next.js and client components
If you are using Next.js with the App Router, you have to make sure the component that uses the `TriggerAuthContext` is a client component. So for example, the following code will not work:
```tsx app/page.tsx
import { TriggerAuthContext } from "@trigger.dev/react-hooks";
export default function Page() {
return (
<TriggerAuthContext.Provider value={{ accessToken: "your-access-token" }}>
<MyComponent />
</TriggerAuthContext.Provider>
);
}
```
That's because `Page` is a server component and the `TriggerAuthContext.Provider` uses client-only react code. To fix this, wrap the `TriggerAuthContext.Provider` in a client component:
```ts components/TriggerProvider.tsx
"use client";
import { TriggerAuthContext } from "@trigger.dev/react-hooks";
export function TriggerProvider({
accessToken,
children,
}: {
accessToken: string;
children: React.ReactNode;
}) {
return (
<TriggerAuthContext.Provider
value={{
accessToken,
}}
>
{children}
</TriggerAuthContext.Provider>
);
}
```
### Passing the token to the frontend
Techniques for passing the token to the frontend vary depending on your setup. Here are a few ways to do it for different setups:
#### Next.js App Router
If you are using Next.js with the App Router and you are triggering a task from a server action, you can use cookies to store and pass the token to the frontend.
```tsx actions/trigger.ts
"use server";
import { tasks } from "@trigger.dev/sdk/v3";
import type { exampleTask } from "@/trigger/example";
import { redirect } from "next/navigation";
import { cookies } from "next/headers";
export async function startRun() {
const handle = await tasks.trigger<typeof exampleTask>("example", { foo: "bar" });
// Set the auto-generated publicAccessToken in a cookie
cookies().set("publicAccessToken", handle.publicAccessToken); // âœ… this token only has access to read this run
redirect(`/runs/${handle.id}`);
}
```
Then in the `/runs/[id].tsx` page, you can read the token from the cookie and pass it to the `TriggerProvider`.
```tsx pages/runs/[id].tsx
import { TriggerProvider } from "@/components/TriggerProvider";
export default function RunPage({ params }: { params: { id: string } }) {
const publicAccessToken = cookies().get("publicAccessToken");
return (
<TriggerProvider accessToken={publicAccessToken}>
<RunDetails id={params.id} />
</TriggerProvider>
);
}
```
Instead of a cookie, you could also use a query parameter to pass the token to the frontend:
```tsx actions/trigger.ts
import { tasks } from "@trigger.dev/sdk/v3";
import type { exampleTask } from "@/trigger/example";
import { redirect } from "next/navigation";
import { cookies } from "next/headers";
export async function startRun() {
const handle = await tasks.trigger<typeof exampleTask>("example", { foo: "bar" });
redirect(`/runs/${handle.id}?publicAccessToken=${handle.publicAccessToken}`);
}
```
And then in the `/runs/[id].tsx` page:
```tsx pages/runs/[id].tsx
import { TriggerProvider } from "@/components/TriggerProvider";
export default function RunPage({
params,
searchParams,
}: {
params: { id: string };
searchParams: { publicAccessToken: string };
}) {
return (
<TriggerProvider accessToken={searchParams.publicAccessToken}>
<RunDetails id={params.id} />
</TriggerProvider>
);
}
```
Another alternative would be to use a server-side rendered page to fetch the token and pass it to the frontend:
<CodeGroup>
```tsx pages/runs/[id].tsx
import { TriggerProvider } from "@/components/TriggerProvider";
import { generatePublicAccessToken } from "@/trigger/auth";
export default async function RunPage({ params }: { params: { id: string } }) {
// This will be executed on the server only
const publicAccessToken = await generatePublicAccessToken(params.id);
return (
<TriggerProvider accessToken={publicAccessToken}>
<RunDetails id={params.id} />
</TriggerProvider>
);
}
```
```tsx trigger/auth.ts
import { auth } from "@trigger.dev/sdk/v3";
export async function generatePublicAccessToken(runId: string) {
return auth.createPublicToken({
scopes: {
read: {
runs: [runId],
},
},
expirationTime: "1h",
});
}
```
</CodeGroup>
## SWR vs Realtime hooks
We offer two "styles" of hooks: SWR and Realtime. The SWR hooks use the [swr](https://swr.vercel.app/) library to fetch data once and cache it. The Realtime hooks use [Trigger.dev realtime](/realtime) to subscribe to updates in real-time.
<Note>
It can be a little confusing which one to use because [swr](https://swr.vercel.app/) can also be
configured to poll for updates. But because of rate-limits and the way the Trigger.dev API works,
we recommend using the Realtime hooks for most use-cases.
</Note>
## SWR Hooks
### useRun
The `useRun` hook allows you to fetch a run by its ID.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRun } from "@trigger.dev/react-hooks";
export function MyComponent({ runId }: { runId: string }) {
const { run, error, isLoading } = useRun(runId);
if (isLoading) return <div>Loading...</div>;
if (error) return <div>Error: {error.message}</div>;
return <div>Run: {run.id}</div>;
}
```
The `run` object returned is the same as the [run object](/management/runs/retrieve) returned by the Trigger.dev API. To correctly type the run's payload and output, you can provide the type of your task to the `useRun` hook:
```tsx
import { useRun } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
export function MyComponent({ runId }: { runId: string }) {
const { run, error, isLoading } = useRun<typeof myTask>(runId, {
refreshInterval: 0, // Disable polling
});
if (isLoading) return <div>Loading...</div>;
if (error) return <div>Error: {error.message}</div>;
// Now run.payload and run.output are correctly typed
return <div>Run: {run.id}</div>;
}
```
### Common options
You can pass the following options to the all SWR hooks:
<ParamField path="revalidateOnFocus" type="boolean">
Revalidate the data when the window regains focus.
</ParamField>
<ParamField path="revalidateOnReconnect" type="boolean">
Revalidate the data when the browser regains a network connection.
</ParamField>
<ParamField path="refreshInterval" type="number">
Poll for updates at the specified interval (in milliseconds). Polling is not recommended for most
use-cases. Use the Realtime hooks instead.
</ParamField>
### Common return values
<ResponseField name="error" type="Error">
An error object if an error occurred while fetching the data.
</ResponseField>
<ResponseField name="isLoading" type="boolean">
A boolean indicating if the data is currently being fetched.
</ResponseField>
<ResponseField name="isValidating" type="boolean">
A boolean indicating if the data is currently being revalidated.
</ResponseField>
<ResponseField name="isError" type="boolean">
A boolean indicating if an error occurred while fetching the data.
</ResponseField>
## Realtime hooks
See our [Realtime hooks documentation](/frontend/react-hooks/realtime) for more information.
## Trigger Hooks
See our [Trigger hooks documentation](/frontend/react-hooks/triggering) for more information.
# Realtime hooks
Source: https://trigger.dev/docs/frontend/react-hooks/realtime
Get live updates from the Trigger.dev API in your frontend application.
These hooks allow you to subscribe to runs, batches, and streams using [Trigger.dev realtime](/realtime). Before reading this guide:
* Read our [Realtime documentation](/realtime) to understand how the Trigger.dev realtime API works.
* Read how to [setup and authenticate](/frontend/overview) using the `@trigger.dev/react-hooks` package.
## Hooks
### useRealtimeRun
The `useRealtimeRun` hook allows you to subscribe to a run by its ID.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRealtimeRun } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, error } = useRealtimeRun(runId, {
accessToken: publicAccessToken,
});
if (error) return <div>Error: {error.message}</div>;
return <div>Run: {run.id}</div>;
}
```
To correctly type the run's payload and output, you can provide the type of your task to the `useRealtimeRun` hook:
```tsx
import { useRealtimeRun } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, error } = useRealtimeRun<typeof myTask>(runId, {
accessToken: publicAccessToken,
});
if (error) return <div>Error: {error.message}</div>;
// Now run.payload and run.output are correctly typed
return <div>Run: {run.id}</div>;
}
```
You can supply an `onComplete` callback to the `useRealtimeRun` hook to be called when the run is completed or errored. This is useful if you want to perform some action when the run is completed, like navigating to a different page or showing a notification.
```tsx
import { useRealtimeRun } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, error } = useRealtimeRun(runId, {
accessToken: publicAccessToken,
onComplete: (run, error) => {
console.log("Run completed", run);
},
});
if (error) return <div>Error: {error.message}</div>;
return <div>Run: {run.id}</div>;
}
```
See our [Realtime documentation](/realtime) for more information about the type of the run object and more.
### useRealtimeRunsWithTag
The `useRealtimeRunsWithTag` hook allows you to subscribe to multiple runs with a specific tag.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRealtimeRunsWithTag } from "@trigger.dev/react-hooks";
export function MyComponent({ tag }: { tag: string }) {
const { runs, error } = useRealtimeRunsWithTag(tag);
if (error) return <div>Error: {error.message}</div>;
return (
<div>
{runs.map((run) => (
<div key={run.id}>Run: {run.id}</div>
))}
</div>
);
}
```
To correctly type the runs payload and output, you can provide the type of your task to the `useRealtimeRunsWithTag` hook:
```tsx
import { useRealtimeRunsWithTag } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
export function MyComponent({ tag }: { tag: string }) {
const { runs, error } = useRealtimeRunsWithTag<typeof myTask>(tag);
if (error) return <div>Error: {error.message}</div>;
// Now runs[i].payload and runs[i].output are correctly typed
return (
<div>
{runs.map((run) => (
<div key={run.id}>Run: {run.id}</div>
))}
</div>
);
}
```
If `useRealtimeRunsWithTag` could return multiple different types of tasks, you can pass a union of all the task types to the hook:
```tsx
import { useRealtimeRunsWithTag } from "@trigger.dev/react-hooks";
import type { myTask1, myTask2 } from "@/trigger/myTasks";
export function MyComponent({ tag }: { tag: string }) {
const { runs, error } = useRealtimeRunsWithTag<typeof myTask1 | typeof myTask2>(tag);
if (error) return <div>Error: {error.message}</div>;
// You can narrow down the type of the run based on the taskIdentifier
for (const run of runs) {
if (run.taskIdentifier === "my-task-1") {
// run is correctly typed as myTask1
} else if (run.taskIdentifier === "my-task-2") {
// run is correctly typed as myTask2
}
}
return (
<div>
{runs.map((run) => (
<div key={run.id}>Run: {run.id}</div>
))}
</div>
);
}
```
### useRealtimeBatch
The `useRealtimeBatch` hook allows you to subscribe to a batch of runs by its the batch ID.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRealtimeBatch } from "@trigger.dev/react-hooks";
export function MyComponent({ batchId }: { batchId: string }) {
const { runs, error } = useRealtimeBatch(batchId);
if (error) return <div>Error: {error.message}</div>;
return (
<div>
{runs.map((run) => (
<div key={run.id}>Run: {run.id}</div>
))}
</div>
);
}
```
See our [Realtime documentation](/realtime) for more information.
### useRealtimeRunWithStreams
The `useRealtimeRunWithStreams` hook allows you to subscribe to a run by its ID and also receive any streams that are emitted by the task. See our [Realtime documentation](/realtime#streams) for more information about emitting streams from a task.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRealtimeRunWithStreams } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, streams, error } = useRealtimeRunWithStreams(runId, {
accessToken: publicAccessToken,
});
if (error) return <div>Error: {error.message}</div>;
return (
<div>
<div>Run: {run.id}</div>
<div>
{Object.keys(streams).map((stream) => (
<div key={stream}>Stream: {stream}</div>
))}
</div>
</div>
);
}
```
You can provide the type of the streams to the `useRealtimeRunWithStreams` hook:
```tsx
import { useRealtimeRunWithStreams } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
type STREAMS = {
openai: string; // this is the type of each "part" of the stream
};
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, streams, error } = useRealtimeRunWithStreams<typeof myTask, STREAMS>(runId, {
accessToken: publicAccessToken,
});
if (error) return <div>Error: {error.message}</div>;
const text = streams.openai?.map((part) => part).join("");
return (
<div>
<div>Run: {run.id}</div>
<div>{text}</div>
</div>
);
}
```
As you can see above, each stream is an array of the type you provided, keyed by the stream name. If instead of a pure text stream you have a stream of objects, you can provide the type of the object:
```tsx
import type { TextStreamPart } from "ai";
import type { myTask } from "@/trigger/myTask";
type STREAMS = { openai: TextStreamPart<{}> };
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, streams, error } = useRealtimeRunWithStreams<typeof myTask, STREAMS>(runId, {
accessToken: publicAccessToken,
});
if (error) return <div>Error: {error.message}</div>;
const text = streams.openai
?.filter((stream) => stream.type === "text-delta")
?.map((part) => part.text)
.join("");
return (
<div>
<div>Run: {run.id}</div>
<div>{text}</div>
</div>
);
}
```
## Common options
### accessToken & baseURL
You can pass the `accessToken` option to the Realtime hooks to authenticate the subscription.
```tsx
import { useRealtimeRun } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { run, error } = useRealtimeRun(runId, {
accessToken: publicAccessToken,
baseURL: "https://my-self-hosted-trigger.com", // Optional if you are using a self-hosted Trigger.dev instance
});
if (error) return <div>Error: {error.message}</div>;
return <div>Run: {run.id}</div>;
}
```
### enabled
You can pass the `enabled` option to the Realtime hooks to enable or disable the subscription.
```tsx
import { useRealtimeRun } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
enabled,
}: {
runId: string;
publicAccessToken: string;
enabled: boolean;
}) {
const { run, error } = useRealtimeRun(runId, {
accessToken: publicAccessToken,
enabled,
});
if (error) return <div>Error: {error.message}</div>;
return <div>Run: {run.id}</div>;
}
```
This allows you to conditionally disable using the hook based on some state.
### id
You can pass the `id` option to the Realtime hooks to change the ID of the subscription.
```tsx
import { useRealtimeRun } from "@trigger.dev/react-hooks";
export function MyComponent({
id,
runId,
publicAccessToken,
enabled,
}: {
id: string;
runId: string;
publicAccessToken: string;
enabled: boolean;
}) {
const { run, error } = useRealtimeRun(runId, {
accessToken: publicAccessToken,
enabled,
id,
});
if (error) return <div>Error: {error.message}</div>;
return <div>Run: {run.id}</div>;
}
```
This allows you to change the ID of the subscription based on some state. Passing in a different ID will unsubscribe from the current subscription and subscribe to the new one (and remove any cached data).
### experimental\_throttleInMs
The `*withStreams` variants of the Realtime hooks accept an `experimental_throttleInMs` option to throttle the updates from the server. This can be useful if you are getting too many updates and want to reduce the number of updates.
```tsx
import { useRealtimeRunsWithStreams } from "@trigger.dev/react-hooks";
export function MyComponent({
runId,
publicAccessToken,
}: {
runId: string;
publicAccessToken: string;
}) {
const { runs, error } = useRealtimeRunsWithStreams(tag, {
accessToken: publicAccessToken,
experimental_throttleInMs: 1000, // Throttle updates to once per second
});
if (error) return <div>Error: {error.message}</div>;
return (
<div>
{runs.map((run) => (
<div key={run.id}>Run: {run.id}</div>
))}
</div>
);
}
```
# Trigger hooks
Source: https://trigger.dev/docs/frontend/react-hooks/triggering
Triggering tasks from your frontend application.
We provide a set of hooks that can be used to trigger tasks from your frontend application.
## Demo
We've created a [Demo application](https://github.com/triggerdotdev/realtime-llm-battle) that demonstrates how to use our React hooks to trigger tasks in a Next.js application. The application uses the `@trigger.dev/react-hooks` package to trigger a task and subscribe to the run in real-time.
## Installation
Install the `@trigger.dev/react-hooks` package in your project:
<CodeGroup>
```bash npm
npm add @trigger.dev/react-hooks
```
```bash pnpm
pnpm add @trigger.dev/react-hooks
```
```bash yarn
yarn install @trigger.dev/react-hooks
```
</CodeGroup>
## Authentication
To authenticate a trigger hook, you must provide a special one-time use "trigger" token. These tokens are very similar to [Public Access Tokens](/frontend/overview#authentication), but they can only be used once to trigger a task. You can generate a trigger token using the `auth.createTriggerPublicToken` function in your backend code:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const triggerToken = await auth.createTriggerPublicToken("my-task");
```
These tokens also expire, with the default expiration time being 15 minutes. You can specify a custom expiration time by passing a `expirationTime` parameter:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const triggerToken = await auth.createTriggerPublicToken("my-task", {
expirationTime: "24hr",
});
```
You can also pass multiple tasks to the `createTriggerPublicToken` function to create a token that can trigger multiple tasks:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const triggerToken = await auth.createTriggerPublicToken(["my-task-1", "my-task-2"]);
```
You can also pass the `multipleUse` parameter to create a token that can be used multiple times:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const triggerToken = await auth.createTriggerPublicToken("my-task", {
multipleUse: true, // âŒ Use this with caution!
});
```
<Note>
After generating the trigger token in your backend, you must pass it to your frontend application.
We have a guide on how to do this in the [React hooks
overview](/frontend/react-hooks/overview#passing-the-token-to-the-frontend).
</Note>
## Hooks
### useTaskTrigger
The `useTaskTrigger` hook allows you to trigger a task from your frontend application.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useTaskTrigger } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
//     ðŸ‘† This is the type of your task
export function MyComponent({ publicAccessToken }: { publicAccessToken: string }) {
//                         pass the type of your task here ðŸ‘‡
const { submit, handle, error, isLoading } = useTaskTrigger<typeof myTask>("my-task", {
accessToken: publicAccessToken, // ðŸ‘ˆ this is the "trigger" token
});
if (error) {
return <div>Error: {error.message}</div>;
}
if (handle) {
return <div>Run ID: {handle.id}</div>;
}
return (
<button onClick={() => submit({ foo: "bar" })} disabled={isLoading}>
{isLoading ? "Loading..." : "Trigger Task"}
</button>
);
}
```
`useTaskTrigger` returns an object with the following properties:
* `submit`: A function that triggers the task. It takes the payload of the task as an argument.
* `handle`: The run handle object. This object contains the ID of the run that was triggered, along with a Public Access Token that can be used to access the run.
* `isLoading`: A boolean that indicates whether the task is currently being triggered.
* `error`: An error object that contains any errors that occurred while triggering the task.
The `submit` function triggers the task with the specified payload. You can additionally pass an optional [options](/triggering#options) argument to the `submit` function:
```tsx
submit({ foo: "bar" }, { tags: ["tag1", "tag2"] });
```
#### Using the handle object
You can use the `handle` object to initiate a subsequent [realtime hook](/frontend/react-hooks/realtime#userealtimerun) to subscribe to the run.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useTaskTrigger, useRealtimeRun } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
//     ðŸ‘† This is the type of your task
export function MyComponent({ publicAccessToken }: { publicAccessToken: string }) {
//                         pass the type of your task here ðŸ‘‡
const { submit, handle, error, isLoading } = useTaskTrigger<typeof myTask>("my-task", {
accessToken: publicAccessToken, // ðŸ‘ˆ this is the "trigger" token
});
//     use the handle object to preserve type-safety ðŸ‘‡
const { run, error: realtimeError } = useRealtimeRun(handle, {
accessToken: handle?.publicAccessToken,
enabled: !!handle, // Only subscribe to the run if the handle is available
});
if (error) {
return <div>Error: {error.message}</div>;
}
if (handle) {
return <div>Run ID: {handle.id}</div>;
}
if (realtimeError) {
return <div>Error: {realtimeError.message}</div>;
}
if (run) {
return <div>Run ID: {run.id}</div>;
}
return (
<button onClick={() => submit({ foo: "bar" })} disabled={isLoading}>
{isLoading ? "Loading..." : "Trigger Task"}
</button>
);
}
```
We've also created some additional hooks that allow you to trigger tasks and subscribe to the run in one step:
### useRealtimeTaskTrigger
The `useRealtimeTaskTrigger` hook allows you to trigger a task from your frontend application and then subscribe to the run in using Realtime:
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRealtimeTaskTrigger } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
export function MyComponent({ publicAccessToken }: { publicAccessToken: string }) {
const { submit, run, error, isLoading } = useRealtimeTaskTrigger<typeof myTask>("my-task", {
accessToken: publicAccessToken,
});
if (error) {
return <div>Error: {error.message}</div>;
}
// This is the realtime run object, which will automatically update when the run changes
if (run) {
return <div>Run ID: {run.id}</div>;
}
return (
<button onClick={() => submit({ foo: "bar" })} disabled={isLoading}>
{isLoading ? "Loading..." : "Trigger Task"}
</button>
);
}
```
### useRealtimeTaskTriggerWithStreams
The `useRealtimeTaskTriggerWithStreams` hook allows you to trigger a task from your frontend application and then subscribe to the run in using Realtime, and also receive any streams that are emitted by the task.
```tsx
"use client"; // This is needed for Next.js App Router or other RSC frameworks
import { useRealtimeTaskTriggerWithStreams } from "@trigger.dev/react-hooks";
import type { myTask } from "@/trigger/myTask";
type STREAMS = {
openai: string; // this is the type of each "part" of the stream
};
export function MyComponent({ publicAccessToken }: { publicAccessToken: string }) {
const { submit, run, streams, error, isLoading } = useRealtimeTaskTriggerWithStreams<
typeof myTask,
STREAMS
>("my-task", {
accessToken: publicAccessToken,
});
if (error) {
return <div>Error: {error.message}</div>;
}
if (streams && run) {
const text = streams.openai?.map((part) => part).join("");
return (
<div>
<div>Run ID: {run.id}</div>
<div>{text}</div>
</div>
);
}
return (
<button onClick={() => submit({ foo: "bar" })} disabled={isLoading}>
{isLoading ? "Loading..." : "Trigger Task"}
</button>
);
}
```
# GitHub Actions
Source: https://trigger.dev/docs/github-actions
You can easily deploy your tasks with GitHub actions.
This simple GitHub action file will deploy your Trigger.dev tasks when new code is pushed to the `main` branch and the `trigger` directory has changes in it.
<Warning>
The deploy step will fail if any version mismatches are detected. Please see the [version
pinning](/github-actions#version-pinning) section for more details.
</Warning>
<CodeGroup>
```yaml .github/workflows/release-trigger-prod.yml
name: Deploy to Trigger.dev (prod)
on:
push:
branches:
- main
jobs:
deploy:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v4
- name: Use Node.js 20.x
uses: actions/setup-node@v4
with:
node-version: "20.x"
- name: Install dependencies
run: npm install
- name: ðŸš€ Deploy Trigger.dev
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
run: |
npx trigger.dev@latest deploy
```
```yaml .github/workflows/release-trigger-staging.yml
name: Deploy to Trigger.dev (staging)
# Requires manually calling the workflow from a branch / commit to deploy to staging
on:
workflow_dispatch:
jobs:
deploy:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v4
- name: Use Node.js 20.x
uses: actions/setup-node@v4
with:
node-version: "20.x"
- name: Install dependencies
run: npm install
- name: ðŸš€ Deploy Trigger.dev
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
run: |
npx trigger.dev@latest deploy --env staging
```
</CodeGroup>
If you already have a GitHub action file, you can just add the final step "ðŸš€ Deploy Trigger.dev" to your existing file.
## Creating a Personal Access Token
<Steps>
<Step title="Create a new access token">
Go to your profile page and click on the ["Personal Access
Tokens"](https://cloud.trigger.dev/account/tokens) tab.
</Step>
<Step title="Go to your repository on GitHub.">
Click on 'Settings' -> 'Secrets and variables' -> 'Actions' -> 'New repository secret'
</Step>
<Step title="Add the TRIGGER_ACCESS_TOKEN">
Add the name `TRIGGER_ACCESS_TOKEN` and the value of your access token. ![Add TRIGGER\_ACCESS\_TOKEN
in GitHub](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/github-access-token.png)
</Step>
</Steps>
## CLI Version pinning
The CLI and `@trigger.dev/*` package versions need to be in sync with the `trigger.dev` CLI, otherwise there will be errors and unpredictable behavior. Hence, the `deploy` command will automatically fail during CI on any version mismatches.
Tip: add the deploy command to your `package.json` file to keep versions managed in the same place. For example:
```json
{
"scripts": {
"deploy:trigger-prod": "npx trigger.dev@3.0.0 deploy",
"deploy:trigger": "npx trigger.dev@3.0.0 deploy --env staging"
}
}
```
Your workflow file will follow the version specified in the `package.json` script, like so:
```yaml .github/workflows/release-trigger.yml
- name: ðŸš€ Deploy Trigger.dev
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
run: |
npm run deploy:trigger
```
You should use the version you run locally during dev and manual deploy. The current version is displayed in the banner, but you can also check it by appending `--version` to any command.
## Self-hosting
When self-hosting, you will have to take a few additional steps:
* Specify the `TRIGGER_API_URL` environment variable. You can add it to the GitHub secrets the same way as the access token. This should point at your webapp domain, for example: `https://trigger.example.com`
* Setup docker as you will need to build and push the image to your registry. On [Trigger.dev Cloud](https://cloud.trigger.dev) this is all done remotely.
* Add your registry credentials to the GitHub secrets.
* Use the `--self-hosted` and `--push` flags when deploying.
Other than that, your GitHub action file will look very similar to the one above:
<CodeGroup>
```yaml .github/workflows/release-trigger-self-hosted.yml
name: Deploy to Trigger.dev (self-hosted)
on:
push:
branches:
- main
jobs:
deploy:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v4
- name: Use Node.js 20.x
uses: actions/setup-node@v4
with:
node-version: "20.x"
- name: Install dependencies
run: npm install
# docker setup - part 1
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
# docker setup - part 2
- name: Login to DockerHub
uses: docker/login-action@v3
with:
username: ${{ secrets.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: ðŸš€ Deploy Trigger.dev
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
# required when self-hosting
TRIGGER_API_URL: ${{ secrets.TRIGGER_API_URL }}
# deploy with additional flags
run: |
npx trigger.dev@latest deploy --self-hosted --push
```
</CodeGroup>
# GitHub repo
Source: https://trigger.dev/docs/github-repo
Trigger.dev is [Open Source on GitHub](https://github.com/triggerdotdev/trigger.dev). You can contribute to the project by submitting issues, pull requests, or simply by using it and providing feedback.
You can also [self-host](/open-source-self-hosting) the project if you want to run it on your own infrastructure.
# Generate and translate copy
Source: https://trigger.dev/docs/guides/ai-agents/generate-translate-copy
Create an AI agent workflow that generates and translates copy
## Overview
**Prompt chaining** is an AI workflow pattern that decomposes a complex task into a sequence of steps, where each LLM call processes the output of the previous one. This approach trades off latency for higher accuracy by making each LLM call an easier, more focused task, with the ability to add programmatic checks between steps to ensure the process remains on track.
![Generating and translating copy](https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/prompt-chaining.png)
## Example task
In this example, we'll create a workflow that generates and translates copy. This approach is particularly effective when tasks require different models or approaches for different inputs.
**This task:**
* Uses `generateText` from [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction) to interact with OpenAI models
* Uses `experimental_telemetry` to provide LLM logs
* Generates marketing copy based on subject and target word count
* Validates the generated copy meets word count requirements (Â±10 words)
* Translates the validated copy to the target language while preserving tone
```typescript
import { openai } from "@ai-sdk/openai";
import { task } from "@trigger.dev/sdk/v3";
import { generateText } from "ai";
export interface TranslatePayload {
marketingSubject: string;
targetLanguage: string;
targetWordCount: number;
}
export const generateAndTranslateTask = task({
id: "generate-and-translate-copy",
maxDuration: 300, // Stop executing after 5 mins of compute
run: async (payload: TranslatePayload) => {
// Step 1: Generate marketing copy
const generatedCopy = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content: "You are an expert copywriter.",
},
{
role: "user",
content: `Generate as close as possible to ${payload.targetWordCount} words of compelling marketing copy for ${payload.marketingSubject}`,
},
],
experimental_telemetry: {
isEnabled: true,
functionId: "generate-and-translate-copy",
},
});
// Gate: Validate the generated copy meets the word count target
const wordCount = generatedCopy.text.split(/\s+/).length;
if (
wordCount < payload.targetWordCount - 10 ||
wordCount > payload.targetWordCount + 10
) {
throw new Error(
`Generated copy length (${wordCount} words) is outside acceptable range of ${
payload.targetWordCount - 10
}-${payload.targetWordCount + 10} words`
);
}
// Step 2: Translate to target language
const translatedCopy = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content: `You are an expert translator specializing in marketing content translation into ${payload.targetLanguage}.`,
},
{
role: "user",
content: `Translate the following marketing copy to ${payload.targetLanguage}, maintaining the same tone and marketing impact:\n\n${generatedCopy}`,
},
],
experimental_telemetry: {
isEnabled: true,
functionId: "generate-and-translate-copy",
},
});
return {
englishCopy: generatedCopy,
translatedCopy,
};
},
});
```
## Run a test
On the Test page in the dashboard, select the `generate-and-translate-copy` task and include a payload like the following:
```json
{
marketingSubject: "The controversial new Jaguar electric concept car",
targetLanguage: "Spanish",
targetWordCount: 100,
}
```
This example payload generates copy and then translates it using sequential LLM calls. The translation only begins after the generated copy has been validated against the word count requirements.
<video src="https://content.trigger.dev/agent-prompt-chaining-3.mp4" controls muted autoPlay loop />
# AI agents overview
Source: https://trigger.dev/docs/guides/ai-agents/overview
Real world AI agent example tasks using Trigger.dev
## Overview
This guide will show you how to set up different types of AI agent workflows with Trigger.dev. The examples take inspiration from Athropic's blog post on [building effective agents](https://www.anthropic.com/research/building-effective-agents).
<CardGroup cols={2}>
<Card title="Prompt chaining" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/prompt-chaining.png" href="/guides/ai-agents/generate-translate-copy">Chain prompts together to generate and translate marketing copy automatically</Card>
<Card title="Routing" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/routing.png" href="/guides/ai-agents/route-question">Send questions to different AI models based on complexity analysis</Card>
<Card title="Parallelization" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/parallelization.png" href="/guides/ai-agents/respond-and-check-content">Simultaneously check for inappropriate content while responding to customer inquiries</Card>
<Card title="Orchestrator" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/orchestrator-workers.png" href="/guides/ai-agents/verify-news-article">Coordinate multiple AI workers to verify news article accuracy</Card>
<Card title="Evaluator-optimizer" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/evaluator-optimizer.png" href="/guides/ai-agents/translate-and-refine">Translate text and automatically improve quality through feedback loops</Card>
</CardGroup>
# Respond to customer inquiry and check for inappropriate content
Source: https://trigger.dev/docs/guides/ai-agents/respond-and-check-content
Create an AI agent workflow that responds to customer inquiries while checking if their text is inappropriate
## Overview
**Parallelization** is a workflow pattern where multiple tasks or processes run simultaneously instead of sequentially, allowing for more efficient use of resources and faster overall execution. It's particularly valuable when different parts of a task can be handled independently, such as running content analysis and response generation at the same time.
![Parallelization](https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/parallelization.png)
## Example task
In this example, we'll create a workflow that simultaneously checks content for issues while responding to customer inquiries. This approach is particularly effective when tasks require multiple perspectives or parallel processing streams, with the orchestrator synthesizing the results into a cohesive output.
**This task:**
* Uses `generateText` from [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction) to interact with OpenAI models
* Uses `experimental_telemetry` to provide LLM logs
* Uses [`batch.triggerByTaskAndWait`](/triggering#batch-triggerbytaskandwait) to run customer response and content moderation tasks in parallel
* Generates customer service responses using an AI model
* Simultaneously checks for inappropriate content while generating responses
```typescript
import { openai } from "@ai-sdk/openai";
import { batch, task } from "@trigger.dev/sdk/v3";
import { generateText } from "ai";
// Task to generate customer response
export const generateCustomerResponse = task({
id: "generate-customer-response",
run: async (payload: { question: string }) => {
const response = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content: "You are a helpful customer service representative.",
},
{ role: "user", content: payload.question },
],
experimental_telemetry: {
isEnabled: true,
functionId: "generate-customer-response",
},
});
return response.text;
},
});
// Task to check for inappropriate content
export const checkInappropriateContent = task({
id: "check-inappropriate-content",
run: async (payload: { text: string }) => {
const response = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content:
"You are a content moderator. Respond with 'true' if the content is inappropriate or contains harmful, threatening, offensive, or explicit content, 'false' otherwise.",
},
{ role: "user", content: payload.text },
],
experimental_telemetry: {
isEnabled: true,
functionId: "check-inappropriate-content",
},
});
return response.text.toLowerCase().includes("true");
},
});
// Main task that coordinates the parallel execution
export const handleCustomerQuestion = task({
id: "handle-customer-question",
run: async (payload: { question: string }) => {
const {
runs: [responseRun, moderationRun],
} = await batch.triggerByTaskAndWait([
{
task: generateCustomerResponse,
payload: { question: payload.question },
},
{
task: checkInappropriateContent,
payload: { text: payload.question },
},
]);
// Check moderation result first
if (moderationRun.ok && moderationRun.output === true) {
return {
response:
"I apologize, but I cannot process this request as it contains inappropriate content.",
wasInappropriate: true,
};
}
// Return the generated response if everything is ok
if (responseRun.ok) {
return {
response: responseRun.output,
wasInappropriate: false,
};
}
// Handle any errors
throw new Error("Failed to process customer question");
},
});
```
## Run a test
On the Test page in the dashboard, select the `handle-customer-question` task and include a payload like the following:
```json
{
"question": "Can you explain 2FA?"
}
```
When triggered with a question, the task simultaneously generates a response while checking for inappropriate content using two parallel LLM calls. The main task waits for both operations to complete before delivering the final response.
<video src="https://content.trigger.dev/agent-parallelization.mp4" controls muted autoPlay loop />
# Route a question to a different AI model
Source: https://trigger.dev/docs/guides/ai-agents/route-question
Create an AI agent workflow that routes a question to a different AI model depending on its complexity
## Overview
**Routing** is a workflow pattern that classifies an input and directs it to a specialized followup task. This pattern allows for separation of concerns and building more specialized prompts, which is particularly effective when there are distinct categories that are better handled separately. Without routing, optimizing for one kind of input can hurt performance on other inputs.
![Routing](https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/routing.png)
## Example task
In this example, we'll create a workflow that routes a question to a different AI model depending on its complexity. This approach is particularly effective when tasks require different models or approaches for different inputs.
**This task:**
* Uses `generateText` from [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction) to interact with OpenAI models
* Uses `experimental_telemetry` in the source verification and historical analysis tasks to provide LLM logs
* Routes questions using a lightweight model (`o1-mini`) to classify complexity
* Directs simple questions to `gpt-4o` and complex ones to `gpt-o3-mini`
* Returns both the answer and metadata about the routing decision
````typescript
import { openai } from "@ai-sdk/openai";
import { task } from "@trigger.dev/sdk/v3";
import { generateText } from "ai";
import { z } from "zod";
// Schema for router response
const routingSchema = z.object({
model: z.enum(["gpt-4o", "gpt-o3-mini"]),
reason: z.string(),
});
// Router prompt template
const ROUTER_PROMPT = `You are a routing assistant that determines the complexity of questions.
Analyze the following question and route it to the appropriate model:
- Use "gpt-4o" for simple, common, or straightforward questions
- Use "gpt-o3-mini" for complex, unusual, or questions requiring deep reasoning
Respond with a JSON object in this exact format:
{"model": "gpt-4o" or "gpt-o3-mini", "reason": "your reasoning here"}
Question: `;
export const routeAndAnswerQuestion = task({
id: "route-and-answer-question",
run: async (payload: { question: string }) => {
// Step 1: Route the question
const routingResponse = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content:
"You must respond with a valid JSON object containing only 'model' and 'reason' fields. No markdown, no backticks, no explanation.",
},
{
role: "user",
content: ROUTER_PROMPT + payload.question,
},
],
temperature: 0.1,
experimental_telemetry: {
isEnabled: true,
functionId: "route-and-answer-question",
},
});
// Add error handling and cleanup
let jsonText = routingResponse.text.trim();
if (jsonText.startsWith("```")) {
jsonText = jsonText.replace(/```json\n|\n```/g, "");
}
const routingResult = routingSchema.parse(JSON.parse(jsonText));
// Step 2: Get the answer using the selected model
const answerResult = await generateText({
model: openai(routingResult.model),
messages: [{ role: "user", content: payload.question }],
});
return {
answer: answerResult.text,
selectedModel: routingResult.model,
routingReason: routingResult.reason,
};
},
});
````
## Run a test
Triggering our task with a simple question shows it routing to the gpt-4o model and returning the answer with reasoning:
```json
{
"question": "How many planets are there in the solar system?"
}
```
<video src="https://content.trigger.dev/agent-routing.mp4" controls muted autoPlay loop />
# Translate text and refine it based on feedback
Source: https://trigger.dev/docs/guides/ai-agents/translate-and-refine
This guide will show you how to create a task that translates text and refines it based on feedback.
## Overview
This example is based on the **evaluator-optimizer** pattern, where one LLM generates a response while another provides evaluation and feedback in a loop. This is particularly effective for tasks with clear evaluation criteria where iterative refinement provides better results.
![Evaluator-optimizer](https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/evaluator-optimizer.png)
## Example task
This example task translates text into a target language and refines the translation over a number of iterations based on feedback provided by the LLM.
**This task:**
* Uses `generateText` from [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction) to generate the translation
* Uses `experimental_telemetry` to provide LLM logs on the Run page in the dashboard
* Runs for a maximum of 10 iterations
* Uses `generateText` again to evaluate the translation
* Recursively calls itself to refine the translation based on the feedback
```typescript
import { task } from "@trigger.dev/sdk/v3";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";
interface TranslationPayload {
text: string;
targetLanguage: string;
previousTranslation?: string;
feedback?: string;
rejectionCount?: number;
}
export const translateAndRefine = task({
id: "translate-and-refine",
run: async (payload: TranslationPayload) => {
const rejectionCount = payload.rejectionCount || 0;
// Bail out if we've hit the maximum attempts
if (rejectionCount >= 10) {
return {
finalTranslation: payload.previousTranslation,
iterations: rejectionCount,
status: "MAX_ITERATIONS_REACHED",
};
}
// Generate translation (or refinement if we have previous feedback)
const translationPrompt = payload.feedback
? `Previous translation: "${payload.previousTranslation}"\n\nFeedback received: "${payload.feedback}"\n\nPlease provide an improved translation addressing this feedback.`
: `Translate this text into ${payload.targetLanguage}, preserving style and meaning: "${payload.text}"`;
const translation = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content: `You are an expert literary translator into ${payload.targetLanguage}.
Focus on accuracy first, then style and natural flow.`,
},
{
role: "user",
content: translationPrompt,
},
],
experimental_telemetry: {
isEnabled: true,
functionId: "translate-and-refine",
},
});
// Evaluate the translation
const evaluation = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content: `You are an expert literary critic and translator focused on practical, high-quality translations.
Your goal is to ensure translations are accurate and natural, but not necessarily perfect.
This is iteration ${
rejectionCount + 1
} of a maximum 5 iterations.
RESPONSE FORMAT:
- If the translation meets 90%+ quality: Respond with exactly "APPROVED" (nothing else)
- If improvements are needed: Provide only the specific issues that must be fixed
Evaluation criteria:
- Accuracy of meaning (primary importance)
- Natural flow in the target language
- Preservation of key style elements
DO NOT provide detailed analysis, suggestions, or compliments.
DO NOT include the translation in your response.
IMPORTANT RULES:
- First iteration MUST receive feedback for improvement
- Be very strict on accuracy in early iterations
- After 3 iterations, lower quality threshold to 85%`,
},
{
role: "user",
content: `Original: "${payload.text}"
Translation: "${translation.text}"
Target Language: ${payload.targetLanguage}
Iteration: ${rejectionCount + 1}
Previous Feedback: ${
payload.feedback ? `"${payload.feedback}"` : "None"
}
${
rejectionCount === 0
? "This is the first attempt. Find aspects to improve."
: 'Either respond with exactly "APPROVED" or provide only critical issues that must be fixed.'
}`,
},
],
experimental_telemetry: {
isEnabled: true,
functionId: "translate-and-refine",
},
});
// If approved, return the final result
if (evaluation.text.trim() === "APPROVED") {
return {
finalTranslation: translation.text,
iterations: rejectionCount,
status: "APPROVED",
};
}
// If not approved, recursively call the task with feedback
await translateAndRefine
.triggerAndWait({
text: payload.text,
targetLanguage: payload.targetLanguage,
previousTranslation: translation.text,
feedback: evaluation.text,
rejectionCount: rejectionCount + 1,
})
.unwrap();
},
});
```
## Run a test
On the Test page in the dashboard, select the `translate-and-refine` task and include a payload like the following:
```json
{
"text": "In the twilight of his years, the old clockmaker's hands, once steady as the timepieces he crafted, now trembled like autumn leaves in the wind.",
"targetLanguage": "French"
}
```
This example payload translates the text into French and should be suitably difficult to require a few iterations, depending on the model used and the prompt criteria you set.
<video src="https://content.trigger.dev/agent-evaluator-optimizer.mp4" controls muted autoPlay loop />
# Verify a news article
Source: https://trigger.dev/docs/guides/ai-agents/verify-news-article
Create an AI agent workflow that verifies the facts in a news article
## Overview
This example demonstrates the **orchestrator-workers** pattern, where a central AI agent dynamically breaks down complex tasks and delegates them to specialized worker agents. This pattern is particularly effective when tasks require multiple perspectives or parallel processing streams, with the orchestrator synthesizing the results into a cohesive output.
![Orchestrator](https://mintlify.s3.us-west-1.amazonaws.com/trigger/guides/ai-agents/orchestrator-workers.png)
## Example task
Our example task uses multiple LLM calls to extract claims from a news article and analyze them in parallel, combining source verification and historical context to assess their credibility.
**This task:**
* Uses `generateText` from [Vercel's AI SDK](https://sdk.vercel.ai/docs/introduction) to interact with OpenAI models
* Uses `experimental_telemetry` to provide LLM logs
* Uses [`batch.triggerByTaskAndWait`](/triggering#batch-triggerbytaskandwait) to orchestrate parallel processing of claims
* Extracts factual claims from news articles using the `o1-mini` model
* Evaluates claims against recent sources and analyzes historical context in parallel
* Combines results into a structured analysis report
```typescript
import { openai } from "@ai-sdk/openai";
import { batch, logger, task } from "@trigger.dev/sdk/v3";
import { CoreMessage, generateText } from "ai";
// Define types for our workers' outputs
interface Claim {
id: number;
text: string;
}
interface SourceVerification {
claimId: number;
isVerified: boolean;
confidence: number;
explanation: string;
}
interface HistoricalAnalysis {
claimId: number;
feasibility: number;
historicalContext: string;
}
// Worker 1: Claim Extractor
export const extractClaims = task({
id: "extract-claims",
run: async ({ article }: { article: string }) => {
try {
const messages: CoreMessage[] = [
{
role: "system",
content:
"Extract distinct factual claims from the news article. Format as numbered claims.",
},
{
role: "user",
content: article,
},
];
const response = await generateText({
model: openai("o1-mini"),
messages,
});
const claims = response.text
.split("\n")
.filter((line: string) => line.trim())
.map((claim: string, index: number) => ({
id: index + 1,
text: claim.replace(/^\d+\.\s*/, ""),
}));
logger.info("Extracted claims", { claimCount: claims.length });
return claims;
} catch (error) {
logger.error("Error in claim extraction", {
error: error instanceof Error ? error.message : "Unknown error",
});
throw error;
}
},
});
// Worker 2: Source Verifier
export const verifySource = task({
id: "verify-source",
run: async (claim: Claim) => {
const response = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content:
"Verify this claim by considering recent news sources and official statements. Assess reliability.",
},
{
role: "user",
content: claim.text,
},
],
experimental_telemetry: {
isEnabled: true,
functionId: "verify-source",
},
});
return {
claimId: claim.id,
isVerified: false,
confidence: 0.7,
explanation: response.text,
};
},
});
// Worker 3: Historical Context Analyzer
export const analyzeHistory = task({
id: "analyze-history",
run: async (claim: Claim) => {
const response = await generateText({
model: openai("o1-mini"),
messages: [
{
role: "system",
content:
"Analyze this claim in historical context, considering past announcements and technological feasibility.",
},
{
role: "user",
content: claim.text,
},
],
experimental_telemetry: {
isEnabled: true,
functionId: "analyze-history",
},
});
return {
claimId: claim.id,
feasibility: 0.8,
historicalContext: response.text,
};
},
});
// Orchestrator
export const newsFactChecker = task({
id: "news-fact-checker",
run: async ({ article }: { article: string }) => {
// Step 1: Extract claims
const claimsResult = await batch.triggerByTaskAndWait([
{ task: extractClaims, payload: { article } },
]);
if (!claimsResult.runs[0].ok) {
logger.error("Failed to extract claims", {
error: claimsResult.runs[0].error,
runId: claimsResult.runs[0].id,
});
throw new Error(
`Failed to extract claims: ${claimsResult.runs[0].error}`
);
}
const claims = claimsResult.runs[0].output;
// Step 2: Process claims in parallel
const parallelResults = await batch.triggerByTaskAndWait([
...claims.map((claim) => ({ task: verifySource, payload: claim })),
...claims.map((claim) => ({ task: analyzeHistory, payload: claim })),
]);
// Split and process results
const verifications = parallelResults.runs
.filter(
(run): run is typeof run & { ok: true } =>
run.ok && run.taskIdentifier === "verify-source"
)
.map((run) => run.output as SourceVerification);
const historicalAnalyses = parallelResults.runs
.filter(
(run): run is typeof run & { ok: true } =>
run.ok && run.taskIdentifier === "analyze-history"
)
.map((run) => run.output as HistoricalAnalysis);
return { claims, verifications, historicalAnalyses };
},
});
```
## Run a test
On the Test page in the dashboard, select the `news-fact-checker` task and include a payload like the following:
```json
{
"article": "Tesla announced a new breakthrough in battery technology today. The company claims their new batteries will have 50% more capacity and cost 30% less to produce. Elon Musk stated this development will enable electric vehicles to achieve price parity with gasoline cars by 2024. The new batteries are scheduled to enter production next quarter at the Texas Gigafactory."
}
```
This example payload verifies the claims in the news article and provides a report on the results.
<video src="https://content.trigger.dev/agent-orchestrator-workers.mp4" controls muted autoPlay loop />
# dotenvx
Source: https://trigger.dev/docs/guides/community/dotenvx
A dotenvx package for Trigger.dev.
This is a community developed package from [dotenvx](https://dotenvx.com/) that enables you to use dotenvx with Trigger.dev.
[View the docs](https://dotenvx.com/docs/background-jobs/triggerdotdev)
# Fatima
Source: https://trigger.dev/docs/guides/community/fatima
A Fatima package for Trigger.dev.
This is a community developed package from [@Fgc17](https://github.com/Fgc17) that enables you to use Fatima with Trigger.dev.
[View the Fatima docs](https://fatimajs.vercel.app/docs/adapters/trigger)
[View the repo](https://github.com/Fgc17/fatima)
# Rate limiter
Source: https://trigger.dev/docs/guides/community/rate-limiter
A rate limiter for Trigger.dev.
This is a community developed package from [@ian](https://github.com/ian) that uses Redis to rate limit Trigger.dev tasks.
[View the repo](https://github.com/ian/trigger-rate-limiting)
# SvelteKit setup guide
Source: https://trigger.dev/docs/guides/community/sveltekit
A plugin for SvelteKit to integrate with Trigger.dev.
export const framework_0 = "SvelteKit"
This is a community developed Vite plugin from [@cptCrunch\_](https://x.com/cptCrunch_) that enables seamless integration between SvelteKit and Trigger.dev by allowing you to use your SvelteKit functions directly in your Trigger.dev projects.
## Features
* Use SvelteKit functions directly in Trigger.dev tasks
* Automatic function discovery and export
* TypeScript support with type preservation
* Works with Trigger.dev V3
* Configurable directory scanning
## Prerequisites
* Setup a project in {framework_0}
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
## Setup
[View setup guide on npm](https://www.npmjs.com/package/triggerkit)
```bash
npm i triggerkit
```
<UsefulNextSteps />
# Next.js Batch LLM Evaluator
Source: https://trigger.dev/docs/guides/example-projects/batch-llm-evaluator
This example Next.js project evaluates multiple LLM models using the Vercel AI SDK and streams updates to the frontend using Trigger.dev Realtime.
## Overview
This demo is a full stack example that uses the following:
* A [Next.js](https://nextjs.org/) app with [Prisma](https://www.prisma.io/) for the database.
* Trigger.dev [Realtime](https://trigger.dev/launchweek/0/realtime) to stream updates to the frontend.
* Work with multiple LLM models using the Vercel [AI SDK](https://sdk.vercel.ai/docs/introduction). (OpenAI, Anthropic, XAI)
* Distribute tasks across multiple tasks using the new [`batch.triggerByTaskAndWait`](https://trigger.dev/docs/triggering#batch-triggerbytaskandwait) method.
## GitHub repo
<Card title="View the Batch LLM Evaluator repo" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/batch-llm-evaluator">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Video
<video controls className="w-full aspect-video" src="https://content.trigger.dev/batch-llm-evaluator.mp4" />
## Relevant code
* View the Trigger.dev task code in the [src/trigger/batch.ts](https://github.com/triggerdotdev/examples/blob/main/batch-llm-evaluator/src/trigger/batch.ts) file.
* The `evaluateModels` task uses the `batch.triggerByTaskAndWait` method to distribute the task to the different LLM models.
* It then passes the results through to a `summarizeEvals` task that calculates some dummy "tags" for each LLM response.
* We use a [useRealtimeRunsWithTag](https://trigger.dev/docs/frontend/react-hooks/realtime#userealtimerunswithtag) hook to subscribe to the different evaluation tasks runs in the [src/components/llm-evaluator.tsx](https://github.com/triggerdotdev/examples/blob/main/batch-llm-evaluator/src/components/llm-evaluator.tsx) file.
* We then pass the relevant run down into three different components for the different models:
* The `AnthropicEval` component: [src/components/evals/Anthropic.tsx](https://github.com/triggerdotdev/examples/blob/main/batch-llm-evaluator/src/components/evals/Anthropic.tsx)
* The `XAIEval` component: [src/components/evals/XAI.tsx](https://github.com/triggerdotdev/examples/blob/main/batch-llm-evaluator/src/components/evals/XAI.tsx)
* The `OpenAIEval` component: [src/components/evals/OpenAI.tsx](https://github.com/triggerdotdev/examples/blob/main/batch-llm-evaluator/src/components/evals/OpenAI.tsx)
* Each of these components then uses [useRealtimeRunWithStreams](https://trigger.dev/docs/frontend/react-hooks/realtime#userealtimerunwithstreams) to subscribe to the different LLM responses.
## Learn more about Trigger.dev Realtime
To learn more, take a look at the following resources:
* [Trigger.dev Realtime](/realtime) - learn more about how to subscribe to runs and get real-time updates
* [Realtime streaming](/realtime/streams) - learn more about streaming data from your tasks
* [Batch Triggering](/triggering#tasks-batchtrigger) - learn more about how to trigger tasks in batches
* [React hooks](/frontend/react-hooks) - learn more about using React hooks to interact with the Trigger.dev API
# Claude 3.7 thinking chatbot
Source: https://trigger.dev/docs/guides/example-projects/claude-thinking-chatbot
This example Next.js project uses Vercel's AI SDK and Anthropic's Claude 3.7 model to create a thinking chatbot.
## Overview
This demo is a full stack example that uses the following:
* A [Next.js](https://nextjs.org/) app for the chat interface
* [Trigger.dev Realtime](/realtime/overview) to stream AI responses and thinking/reasoning process to the frontend
* [Claude 3.7 Sonnet](https://www.anthropic.com/claude) for generating AI responses
* [AI SDK](https://sdk.vercel.ai/docs/introduction) for working with the Claude model
## GitHub repo
<Card title="View the Claude thinking chatbot repo" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/claude-thinking-chatbot">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Video
<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/053739a5-9736-4cb5-ab1c-81c35b69f4c4" />
## Relevant code
* **Claude Stream Task**: View the Trigger.dev task code in the [src/trigger/claude-stream.ts](https://github.com/triggerdotdev/examples/tree/main/claude-thinking-chatbot/src/trigger/claude-stream.ts) file, which sets up the streaming connection with Claude.
* **Chat Component**: The main chat interface is in [app/components/claude-chat.tsx](https://github.com/triggerdotdev/examples/tree/main/claude-thinking-chatbot/app/components/claude-chat.tsx), which handles:
* Message state management
* User input handling
* Rendering of message bubbles
* Integration with Trigger.dev for streaming
* **Stream Response**: The `StreamResponse` component within the chat component handles:
* Displaying streaming text from Claude
* Showing/hiding the thinking process with an animated toggle
* Auto-scrolling as new content arrives
## Learn more about Trigger.dev Realtime
To learn more, take a look at the following resources:
* [Trigger.dev Realtime](/realtime) - learn more about how to subscribe to runs and get real-time updates
* [Realtime streaming](/realtime/streams) - learn more about streaming data from your tasks
* [Batch Triggering](/triggering#tasks-batchtrigger) - learn more about how to trigger tasks in batches
* [React hooks](/frontend/react-hooks) - learn more about using React hooks to interact with the Trigger.dev API
# Next.js Realtime CSV Importer
Source: https://trigger.dev/docs/guides/example-projects/realtime-csv-importer
This example Next.js project demonstrates how to use Trigger.dev Realtime to build a CSV Uploader with progress updates streamed to the frontend.
## Overview
The frontend is a Next.js app that allows users to upload a CSV file, which is then processed in the background using Trigger.dev tasks. The progress of the task is streamed back to the frontend in real-time using Trigger.dev Realtime.
* A [Next.js](https://nextjs.org/) app with [Trigger.dev](https://trigger.dev/) for the background tasks.
* [UploadThing](https://uploadthing.com/) to handle CSV file uploads
* Trigger.dev [Realtime](https://trigger.dev/launchweek/0/realtime) to stream updates to the frontend.
## GitHub repo
<Card title="View the Realtime CSV Importer repo" icon="GitHub" href="https://github.com/triggerdotdev/examples/blob/main/realtime-csv-importer/README.md">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Video
<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/25160343-6663-452c-8a27-819c3fd7b8df" />
## Relevant code
* View the Trigger.dev task code in the [src/trigger/csv.ts](https://github.com/triggerdotdev/examples/blob/main/realtime-csv-importer/src/trigger/csv.ts) file.
* The parent task `csvValidator` downloads the CSV file, parses it, and then splits the rows into multiple batches. It then does a `batch.triggerAndWait` to distribute the work the `handleCSVRow` task.
* The `handleCSVRow` task "simulates" checking the row for a valid email address and then updates the progress of the parent task using `metadata.parent`. See the [Trigger.dev docs](/runs/metadata#parent-and-root-updates) for more information on how to use the `metadata.parent` object.
* The `useRealtimeCSVValidator` hook in the [src/hooks/useRealtimeCSVValidator.ts](https://github.com/triggerdotdev/examples/blob/main/realtime-csv-importer/src/hooks/useRealtimeCSVValidator.ts) file handles the call to `useRealtimeRun` to get the progress of the parent task.
* The `CSVProcessor` component in the [src/components/CSVProcessor.tsx](https://github.com/triggerdotdev/examples/blob/main/realtime-csv-importer/src/components/CSVProcessor.tsx) file handles the file upload and displays the progress bar, and uses the `useRealtimeCSVValidator` hook to get the progress updates.
## Learn more about Trigger.dev Realtime
To learn more, take a look at the following resources:
* [Trigger.dev Realtime](/realtime) - learn more about how to subscribe to runs and get real-time updates
* [Realtime streaming](/realtime/streams) - learn more about streaming data from your tasks
* [Batch Triggering](/triggering#tasks-batchtrigger) - learn more about how to trigger tasks in batches
* [React hooks](/frontend/react-hooks) - learn more about using React hooks to interact with the Trigger.dev API
# Image generation with Fal.ai and Trigger.dev Realtime
Source: https://trigger.dev/docs/guides/example-projects/realtime-fal-ai
This example Next.js project generates an image from a prompt using Fal.ai and shows the progress of the task on the frontend using Trigger.dev Realtime.
## Overview
This full stack Next.js project showcases the following:
* A Trigger.dev task which [generates an image from a prompt using Fal.ai](https://github.com/triggerdotdev/examples/blob/main/realtime-fal-ai-image-generation/src/trigger/realtime-generate-image.ts)
* When a [form is submitted](https://github.com/triggerdotdev/examples/blob/main/realtime-fal-ai-image-generation/src/app/page.tsx) in the UI, triggering the task using a [server action](https://github.com/triggerdotdev/examples/blob/main/realtime-fal-ai-image-generation/src/app/actions/process-image.ts)
* Showing the [progress of the task](https://github.com/triggerdotdev/examples/blob/main/realtime-fal-ai-image-generation/src/app/processing/%5Bid%5D/ProcessingContent.tsx) on the frontend using Trigger.dev Realtime. This also includes error handling and a fallback UI
* Once the task is completed, showing the generated image on the frontend next to the original image
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/realtime-fal-ai-image-generation">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Walkthrough video
This video walks through the process of creating this task in a Next.js project.
<iframe width="100" height="315" src="https://www.youtube.com/embed/BWZqYfUaigg?si=XpqVUEIf1j4bsYZ4" title="Trigger.dev walkthrough" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen />
## Learn more about Trigger.dev Realtime
To learn more, take a look at the following resources:
* [Trigger.dev Realtime](/realtime) - learn more about how to subscribe to runs and get real-time updates
* [Realtime streaming](/realtime/streams) - learn more about streaming data from your tasks
* [Batch Triggering](/triggering#tasks-batchtrigger) - learn more about how to trigger tasks in batches
* [React hooks](/frontend/react-hooks) - learn more about using React hooks to interact with the Trigger.dev API
# Turborepo monorepo with Prisma
Source: https://trigger.dev/docs/guides/example-projects/turborepo-monorepo-prisma
Two example projects demonstrating how to use Prisma and Trigger.dev in a Turborepo monorepo setup.
## Overview
These examples demonstrate two different ways of using Prisma and Trigger.dev in a Turborepo monorepo. In both examples, a task is triggered from a Next.js app using a server action, which uses Prisma to add a user to a database table. The examples differ in how Trigger.dev is installed and configured.
* Example 1: Turborepo monorepo demo with Trigger.dev and Prisma packages
* Example 2: Turborepo monorepo demo with a Prisma package and Trigger.dev installed in a Next.js app
<Note>
You can either fork the repos below, or simply check out the project structures and code to get an idea of how to set up Trigger.dev in your own monorepos.
</Note>
## Example 1: Turborepo monorepo demo with Trigger.dev and Prisma packages
This simple example demonstrates how to use Trigger.dev and Prisma as packages inside a monorepo created with Turborepo. The Trigger.dev task is triggered by a button click in a Next.js app which triggers the task via a server action.
### GitHub repo
Fork the GitHub repo below to get started with this example project.
<Card title="Check out the Turborepo monorepo demo with Trigger.dev and Prisma packages" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
### Features
* This monorepo has been created using the [Turborepo CLI](https://turbo.build/repo), following the official [Prisma and Turborepo docs](https://www.prisma.io/docs/guides/turborepo), and then adapted for use with Trigger.dev.
* [pnpm](https://pnpm.io/) has been used as the package manager.
* A tasks package (`@repo/tasks`) using [Trigger.dev](https://trigger.dev) is used to create and execute tasks from an app inside the monorepo.
* A database package (`@repo/db`) using [Prisma ORM](https://www.prisma.io/docs/orm/) is used to interact with the database. You can use any popular Postgres database supported by Prisma, e.g. [Supabase](https://supabase.com/), [Neon](https://neon.tech/), etc.
* A [Next.js](https://nextjs.org/) example app (`apps/web`) to show how to trigger the task via a server action.
### Project structure
Simplified project structure for this example:
```
|
| â€” apps/
|   | â€” web/                    # Next.js frontend application
|   |   | â€” app/                # Next.js app router
|   |   |   | â€” api/
|   |   |   |   | â€” actions.ts  # Server actions for triggering tasks
|   |   |   | â€” page.tsx        # Main page with "Add new user" button
|   |   |   | â€” layout.tsx      # App layout
|   |   | â€” package.json        # Dependencies including @repo/db and @repo/tasks
|   |
|   | â€” docs/                   # Documentation app (not fully implemented)
|
| â€” packages/
|   | â€” database/               # Prisma database package (@repo/db)
|   |   | â€” prisma/
|   |   |   | â€” schema.prisma   # Database schema definition
|   |   | â€” generated/          # Generated Prisma client (gitignored)
|   |   | â€” src/
|   |   |   | â€” index.ts        # Exports from the database package
|   |   | â€” package.json        # Database package dependencies
|   |
|   | â€” tasks/                  # Trigger.dev tasks package (@repo/tasks)
|   |   | â€” src/
|   |   |   | â€” index.ts        # Exports from the tasks package
|   |   |   | â€” trigger/
|   |   |       | â€” index.ts    # Exports the tasks
|   |   |       | â€” addNewUser.ts # Task implementation for adding users
|   |   | â€” trigger.config.ts   # Trigger.dev configuration
|   |   | â€” package.json        # Tasks package dependencies
|   |
|   | â€” ui/                     # UI components package (referenced but not detailed)
|
| â€” turbo.json                  # Turborepo configuration
| â€” package.json                # Root package.json with workspace config
```
### Relevant files and code
#### Database package
* Prisma is added as a package in [`/packages/database`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/database/) and exported as `@repo/db` in the [`package.json`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/database/package.json) file.
* The schema is defined in the [`prisma/schema.prisma`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/database/prisma/schema.prisma) file.
#### Tasks package
<Note>
to run `pnpm dlx trigger.dev@latest init` in a blank packages folder, you have to add a `package.json` file first, otherwise it will attempt to add Trigger.dev files in the root of your monorepo.
</Note>
* Trigger.dev is added as a package in [`/packages/tasks`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks) and exported as `@repo/tasks` in the [`package.json`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks/package.json) file.
* The [`addNewUser.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks/src/trigger/addNewUser.ts) task adds a new user to the database.
* The [`packages/tasks/src/index.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks/src/index.ts) file exports values and types from the Trigger.dev SDK, and is exported from the package via the [`package.json`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks/package.json) file.
* The [`packages/tasks/src/trigger/index.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks/src/trigger/index.ts) file exports the task from the package. Every task must be exported from the package like this.
* The [`trigger.config.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/packages/tasks/trigger.config.ts) file configures the Trigger.dev project settings. This is where the Trigger.dev [Prisma build extension](https://trigger.dev/docs/config/extensions/prismaExtension) is added, which is required to use Prisma in the Trigger.dev task.
<Info>
You must include the version of Prisma you are using in the `trigger.config.ts` file, otherwise the Prisma build extension will not work. Learn more about our [Prisma build extension](/config/extensions/prismaExtension).
</Info>
#### The Next.js app `apps/web`
* The app is a simple Next.js app using the App Router, that uses the `@repo/db` package to interact with the database and the `@repo/tasks` package to trigger the task. These are both added as dependencies in the [`package.json`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/apps/web/package.json) file.
* The task is triggered from a button click in the app in [`page.tsx`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/apps/web/app/page.tsx), which uses a server action in [`/app/api/actions.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/apps/web/app/api/actions.ts) to trigger the task with an example payload.
### Running the example
To run this example, check out the full instructions [in the GitHub repo README file](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package/README.md).
## Example 2: Turborepo monorepo demo with a Prisma package and Trigger.dev installed in a Next.js app
This example demonstrates how to use Trigger.dev and Prisma in a monorepo created with Turborepo. Prisma has been added as a package, and Trigger.dev has been installed in a Next.js app. The task is triggered by a button click in the app via a server action.
### GitHub repo
Fork the GitHub repo below to get started with this example project.
<Card title="Check out the Turborepo monorepo demo with a Prisma package and Trigger.dev installed in a Next.js app" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
### Features
* This monorepo has been created using the [Turborepo CLI](https://turbo.build/repo), following the official [Prisma and Turborepo docs](https://www.prisma.io/docs/guides/turborepo), and then adapted for use with Trigger.dev.
* [pnpm](https://pnpm.io/) has been used as the package manager.
* A database package (`@repo/db`) using [Prisma ORM](https://www.prisma.io/docs/orm/) is used to interact with the database. You can use any popular Postgres database supported by Prisma, e.g. [Supabase](https://supabase.com/), [Neon](https://neon.tech/), etc.
* A [Next.js](https://nextjs.org/) example app (`apps/web`) to show how to trigger the task via a server action.
* Trigger.dev initialized and an `addNewUser` task created in the `web` app.
### Project structure
Simplified project structure for this example:
```
|
| â€” apps/
|   | â€” web/                       # Next.js frontend application
|   |   | â€” app/                   # Next.js app router
|   |   |   | â€” api/
|   |   |   |   | â€” actions.ts     # Server actions for triggering tasks
|   |   |   | â€” page.tsx           # Main page with "Add new user" button
|   |   | â€” src/
|   |   |   | â€” trigger/
|   |   |       | â€” addNewUser.ts  # Task implementation for adding users
|   |   | â€” trigger.config.ts      # Trigger.dev configuration
|   |   | â€” package.json           # Dependencies including @repo/db
|   |
|   | â€” docs/                      # Documentation app
|       | â€” app/
|           | â€” page.tsx           # Docs landing page
|
| â€” packages/
|   | â€” database/                  # Prisma database package (@repo/db)
|   |   | â€” prisma/
|   |   |   | â€” schema.prisma      # Database schema definition
|   |
|   | â€” ui/                        # UI components package
|
| â€” turbo.json                     # Turborepo configuration
| â€” package.json                   # Root package.json with workspace config
```
## Relevant files and code
### Database package (`@repo/db`)
* Located in [`/packages/database/`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/packages/database/) and exported as `@repo/db`
* Schema defined in [`schema.prisma`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/packages/database/prisma/schema.prisma)
* Provides database access to other packages and apps
### Next.js app (`apps/web`)
* Contains Trigger.dev configuration in [`trigger.config.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/apps/web/trigger.config.ts)
* Trigger.dev tasks are defined in [`src/trigger/`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/apps/web/src/trigger/) (e.g., [`addNewUser.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/apps/web/src/trigger/addNewUser.ts))
* Demonstrates triggering tasks via server actions in [`app/api/actions.ts`](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/apps/web/app/api/actions.ts)
### Running the example
To run this example, check out the full instructions [in the GitHub repo README file](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-trigger/README.md).
# Vercel AI SDK image generator
Source: https://trigger.dev/docs/guides/example-projects/vercel-ai-sdk-image-generator
This example Next.js project uses the Vercel AI SDK to generate images from a prompt.
## Overview
This demo is a full stack example that uses the following:
* A [Next.js](https://nextjs.org/) app using [shadcn](https://ui.shadcn.com/) for the UI
* Our 'useRealtimeRun' [React hook](https://trigger.dev/docs/frontend/react-hooks/realtime) to subscribe to the run and show updates on the frontend
* The [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction) to [generate images](https://sdk.vercel.ai/docs/ai-sdk-core/image-generation) using OpenAI's DALL-E models
## GitHub repo
<Card title="View the Vercel AI SDK image generator repo" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/vercel-ai-sdk-image-generator">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Video
<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/960edcb6-e225-4983-a48c-6fa697295dec" />
## Relevant code
* View the Trigger.dev task code which generates the image using the Vercel AI SDK in [src/trigger/realtime-generate-image.ts](https://github.com/triggerdotdev/examples/tree/main/vercel-ai-sdk-image-generator/src/trigger/realtime-generate-image.ts).
* We use a [useRealtimeRun](https://trigger.dev/docs/frontend/react-hooks/realtime#userealtimerun) hook to subscribe to the run in [src/app/processing/\[id\]/ProcessingContent.tsx](https://github.com/triggerdotdev/examples/tree/main/vercel-ai-sdk-image-generator/src/app/processing/\[id]/ProcessingContent.tsx).
## Learn more about Trigger.dev Realtime
To learn more, take a look at the following resources:
* [Trigger.dev Realtime](/realtime) - learn more about how to subscribe to runs and get real-time updates
* [Realtime streaming](/realtime/streams) - learn more about streaming data from your tasks
* [Batch Triggering](/triggering#tasks-batchtrigger) - learn more about how to trigger tasks in batches
* [React hooks](/frontend/react-hooks) - learn more about using React hooks to interact with the Trigger.dev API
# Generate an image using DALLÂ·E 3
Source: https://trigger.dev/docs/guides/examples/dall-e3-generate-image
This example will show you how to generate an image using DALLÂ·E 3 and text using GPT-4o with Trigger.dev.
## Overview
This example demonstrates how to use Trigger.dev to make reliable calls to AI APIs, specifically OpenAI's GPT-4o and DALL-E 3. It showcases automatic retrying with a maximum of 3 attempts, built-in error handling to avoid timeouts, and the ability to trace and monitor API calls.
## Task code
```ts trigger/generateContent.ts
import { task } from "@trigger.dev/sdk/v3";
import OpenAI from "openai";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
type Payload = {
theme: string;
description: string;
};
export const generateContent = task({
id: "generate-content",
retry: {
maxAttempts: 3, // Retry up to 3 times
},
run: async ({ theme, description }: Payload) => {
// Generate text
const textResult = await openai.chat.completions.create({
model: "gpt-4o",
messages: generateTextPrompt(theme, description),
});
if (!textResult.choices[0]) {
throw new Error("No content, retryingâ€¦");
}
// Generate image
const imageResult = await openai.images.generate({
model: "dall-e-3",
prompt: generateImagePrompt(theme, description),
});
if (!imageResult.data[0]) {
throw new Error("No image, retryingâ€¦");
}
return {
text: textResult.choices[0],
image: imageResult.data[0].url,
};
},
});
function generateTextPrompt(theme: string, description: string): any {
return `Theme: ${theme}\n\nDescription: ${description}`;
}
function generateImagePrompt(theme: string, description: string): any {
return `Theme: ${theme}\n\nDescription: ${description}`;
}
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"theme": "A beautiful sunset",
"description": "A sunset over the ocean with a tiny yacht in the distance."
}
```
# Transcribe audio using Deepgram
Source: https://trigger.dev/docs/guides/examples/deepgram-transcribe-audio
This example will show you how to transcribe audio using Deepgram's speech recognition API with Trigger.dev.
## Overview
Transcribe audio using [Deepgram's](https://developers.deepgram.com/docs/introduction) speech recognition API.
## Key Features
* Transcribe audio from a URL
* Use the Nova 2 model for transcription
## Task code
```ts trigger/deepgramTranscription.ts
import { createClient } from "@deepgram/sdk";
import { logger, task } from "@trigger.dev/sdk/v3";
// Initialize the Deepgram client, using your Deepgram API key (you can find this in your Deepgram account settings).
const deepgram = createClient(process.env.DEEPGRAM_SECRET_KEY);
export const deepgramTranscription = task({
id: "deepgram-transcribe-audio",
run: async (payload: { audioUrl: string }) => {
const { audioUrl } = payload;
logger.log("Transcribing audio from URL", { audioUrl });
// Transcribe the audio using Deepgram
const { result, error } = await deepgram.listen.prerecorded.transcribeUrl(
{
url: audioUrl,
},
{
model: "nova-2", // Use the Nova 2 model for the transcription
smart_format: true, // Automatically format transcriptions to improve readability
diarize: true, // Recognize speaker changes and assign a speaker to each word in the transcript
}
);
if (error) {
logger.error("Failed to transcribe audio", { error });
throw error;
}
console.dir(result, { depth: null });
// Extract the transcription from the result
const transcription = result.results.channels[0].alternatives[0].paragraphs?.transcript;
logger.log(`Generated transcription: ${transcription}`);
return {
result,
};
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"audioUrl": "https://dpgr.am/spacewalk.wav"
}
```
# Convert an image to a cartoon using Fal.ai
Source: https://trigger.dev/docs/guides/examples/fal-ai-image-to-cartoon
This example task generates an image from a URL using Fal.ai and uploads it to Cloudflare R2.
## Walkthrough
This video walks through the process of creating this task in a Next.js project.
<iframe width="100%" height="315" src="https://www.youtube.com/embed/AyRT4X8dHK0?si=ugA172V_3TMjik9h" title="Trigger.dev walkthrough" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen />
## Prerequisites
* An existing project
* A [Trigger.dev account](https://cloud.trigger.dev) with Trigger.dev [initialized in your project](/quick-start)
* A [Fal.ai](https://fal.ai/) account
* A [Cloudflare](https://developers.cloudflare.com/r2/) account with an R2 bucket setup
## Task code
This task converts an image to a cartoon using Fal.ai, and uploads the result to Cloudflare R2.
```ts trigger/fal-ai-image-to-cartoon.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import * as fal from "@fal-ai/serverless-client";
import fetch from "node-fetch";
import { z } from "zod";
// Initialize fal.ai client
fal.config({
credentials: process.env.FAL_KEY, // Get this from your fal.ai dashboard
});
// Initialize S3-compatible client for Cloudflare R2
const s3Client = new S3Client({
// How to authenticate to R2: https://developers.cloudflare.com/r2/api/s3/tokens/
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const FalResult = z.object({
images: z.tuple([z.object({ url: z.string() })]),
});
export const falAiImageToCartoon = task({
id: "fal-ai-image-to-cartoon",
run: async (payload: { imageUrl: string; fileName: string }) => {
logger.log("Converting image to cartoon", payload);
// Convert image to cartoon using fal.ai
const result = await fal.subscribe("fal-ai/flux/dev/image-to-image", {
input: {
prompt: "Turn the image into a cartoon in the style of a Pixar character",
image_url: payload.imageUrl,
},
onQueueUpdate: (update) => {
logger.info("Fal.ai processing update", { update });
},
});
const $result = FalResult.parse(result);
const [{ url: cartoonImageUrl }] = $result.images;
// Download the cartoon image
const imageResponse = await fetch(cartoonImageUrl);
const imageBuffer = await imageResponse.arrayBuffer().then(Buffer.from);
// Upload to Cloudflare R2
const r2Key = `cartoons/${payload.fileName}`;
const uploadParams = {
Bucket: process.env.R2_BUCKET, // Create a bucket in your Cloudflare dashboard
Key: r2Key,
Body: imageBuffer,
ContentType: "image/png",
};
logger.log("Uploading cartoon to R2", { key: r2Key });
await s3Client.send(new PutObjectCommand(uploadParams));
logger.log("Cartoon uploaded to R2", { key: r2Key });
return {
originalUrl: payload.imageUrl,
cartoonUrl: `File uploaded to storage at: ${r2Key}`,
};
},
});
```
### Testing your task
You can test your task by triggering it from the Trigger.dev dashboard.
```json
"imageUrl": "<image-url>", // Replace with the URL of the image you want to convert to a cartoon
"fileName": "<file-name>" // Replace with the name you want to save the file as in Cloudflare R2
```
# Generate an image from a prompt using Fal.ai and Trigger.dev Realtime
Source: https://trigger.dev/docs/guides/examples/fal-ai-realtime
This example task generates an image from a prompt using Fal.ai and shows the progress of the task on the frontend using Trigger.dev Realtime.
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/realtime-fal-ai-image-generation">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Walkthrough
This video walks through the process of creating this task in a Next.js project.
<iframe width="100%" height="315" src="https://www.youtube.com/embed/BWZqYfUaigg?si=XpqVUEIf1j4bsYZ4" title="Trigger.dev walkthrough" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen />
## Prerequisites
* An existing project
* A [Trigger.dev account](https://cloud.trigger.dev) with Trigger.dev [initialized in your project](/quick-start)
* A [Fal.ai](https://fal.ai/) account
## Task code
This task generates an image from a prompt using Fal.ai.
```ts trigger/fal-ai-image-from-prompt-realtime.ts
import * as fal from "@fal-ai/serverless-client";
import { logger, schemaTask } from "@trigger.dev/sdk/v3";
import { z } from "zod";
export const FalResult = z.object({
images: z.tuple([z.object({ url: z.string() })]),
});
export const payloadSchema = z.object({
imageUrl: z.string().url(),
prompt: z.string(),
});
export const realtimeImageGeneration = schemaTask({
id: "realtime-image-generation",
schema: payloadSchema,
run: async (payload) => {
const result = await fal.subscribe("fal-ai/flux/dev/image-to-image", {
input: {
image_url: payload.imageUrl,
prompt: payload.prompt,
},
onQueueUpdate: (update) => {
logger.info("Fal.ai processing update", { update });
},
});
const $result = FalResult.parse(result);
const [{ url: cartoonUrl }] = $result.images;
return {
imageUrl: cartoonUrl,
};
},
});
```
### Testing your task
You can test your task by triggering it from the Trigger.dev dashboard. Here's an example payload:
```json
{
"imageUrl": "https://static.vecteezy.com/system/resources/previews/005/857/332/non_2x/funny-portrait-of-cute-corgi-dog-outdoors-free-photo.jpg",
"prompt": "Dress this dog for Christmas"
}
```
# Video processing with FFmpeg
Source: https://trigger.dev/docs/guides/examples/ffmpeg-video-processing
These examples show you how to process videos in various ways using FFmpeg with Trigger.dev.
export const packages_0 = "ffmpeg"
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [FFmpeg](https://www.ffmpeg.org/download.html) installed on your machine
### Adding the FFmpeg build extension
To use these example tasks, you'll first need to add our FFmpeg extension to your project configuration like this:
```ts trigger.config.ts
import { ffmpeg } from "@trigger.dev/build/extensions/core";
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [ffmpeg()],
},
});
```
<Note>
[Build extensions](/config/extensions/overview) allow you to hook into the build system and
customize the build process or the resulting bundle and container image (in the case of
deploying). You can use pre-built extensions or create your own.
</Note>
You'll also need to add `@trigger.dev/build` to your `package.json` file under `devDependencies` if you don't already have it there.
If you are modifying this example and using popular FFmpeg libraries like `fluent-ffmpeg` you'll also need to add them to [`external`](/config/config-file#external) in your `trigger.config.ts` file.
## Compress a video using FFmpeg
This task demonstrates how to use FFmpeg to compress a video, reducing its file size while maintaining reasonable quality, and upload the compressed video to R2 storage.
### Key Features
* Fetches a video from a given URL
* Compresses the video using FFmpeg with various compression settings
* Uploads the compressed video to R2 storage
### Task code
```ts trigger/ffmpeg-compress-video.ts
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { logger, task } from "@trigger.dev/sdk/v3";
import ffmpeg from "fluent-ffmpeg";
import fs from "fs/promises";
import fetch from "node-fetch";
import { Readable } from "node:stream";
import os from "os";
import path from "path";
// Initialize S3 client
const s3Client = new S3Client({
// How to authenticate to R2: https://developers.cloudflare.com/r2/api/s3/tokens/
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const ffmpegCompressVideo = task({
id: "ffmpeg-compress-video",
run: async (payload: { videoUrl: string }) => {
const { videoUrl } = payload;
// Generate temporary file names
const tempDirectory = os.tmpdir();
const outputPath = path.join(tempDirectory, `output_${Date.now()}.mp4`);
// Fetch the video
const response = await fetch(videoUrl);
// Compress the video
await new Promise((resolve, reject) => {
if (!response.body) {
return reject(new Error("Failed to fetch video"));
}
ffmpeg(Readable.from(response.body))
.outputOptions([
"-c:v libx264", // Use H.264 codec
"-crf 28", // Higher CRF for more compression (28 is near the upper limit for acceptable quality)
"-preset veryslow", // Slowest preset for best compression
"-vf scale=iw/2:ih/2", // Reduce resolution to 320p width (height auto-calculated)
"-c:a aac", // Use AAC for audio
"-b:a 64k", // Reduce audio bitrate to 64k
"-ac 1", // Convert to mono audio
])
.output(outputPath)
.on("end", resolve)
.on("error", reject)
.run();
});
// Read the compressed video
const compressedVideo = await fs.readFile(outputPath);
const compressedSize = compressedVideo.length;
// Log compression results
logger.log(`Compressed video size: ${compressedSize} bytes`);
logger.log(`Temporary compressed video file created`, { outputPath });
// Create the r2Key for the extracted audio, using the base name of the output path
const r2Key = `processed-videos/${path.basename(outputPath)}`;
const uploadParams = {
Bucket: process.env.R2_BUCKET,
Key: r2Key,
Body: compressedVideo,
};
// Upload the video to R2 and get the URL
await s3Client.send(new PutObjectCommand(uploadParams));
logger.log(`Compressed video saved to your r2 bucket`, { r2Key });
// Delete the temporary compressed video file
await fs.unlink(outputPath);
logger.log(`Temporary compressed video file deleted`, { outputPath });
// Return the compressed video buffer and r2 key
return {
Bucket: process.env.R2_BUCKET,
r2Key,
};
},
});
```
### Testing your task
To test this task, use this payload structure:
```json
{
"videoUrl": "<video-url>" // Replace <a-video-url> with the URL of the video you want to upload
}
```
## Extract audio from a video using FFmpeg
This task demonstrates how to use FFmpeg to extract audio from a video, convert it to WAV format, and upload it to R2 storage.
### Key Features
* Fetches a video from a given URL
* Extracts the audio from the video using FFmpeg
* Converts the extracted audio to WAV format
* Uploads the extracted audio to R2 storage
### Task code
```ts trigger/ffmpeg-extract-audio.ts
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { logger, task } from "@trigger.dev/sdk/v3";
import ffmpeg from "fluent-ffmpeg";
import fs from "fs/promises";
import fetch from "node-fetch";
import { Readable } from "node:stream";
import os from "os";
import path from "path";
// Initialize S3 client
const s3Client = new S3Client({
// How to authenticate to R2: https://developers.cloudflare.com/r2/api/s3/tokens/
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const ffmpegExtractAudio = task({
id: "ffmpeg-extract-audio",
run: async (payload: { videoUrl: string }) => {
const { videoUrl } = payload;
// Generate temporary file names
const tempDirectory = os.tmpdir();
const outputPath = path.join(tempDirectory, `audio_${Date.now()}.wav`);
// Fetch the video
const response = await fetch(videoUrl);
// Extract the audio
await new Promise((resolve, reject) => {
if (!response.body) {
return reject(new Error("Failed to fetch video"));
}
ffmpeg(Readable.from(response.body))
.outputOptions([
"-vn", // Disable video output
"-acodec pcm_s16le", // Use PCM 16-bit little-endian encoding
"-ar 44100", // Set audio sample rate to 44.1 kHz
"-ac 2", // Set audio channels to stereo
])
.output(outputPath)
.on("end", resolve)
.on("error", reject)
.run();
});
// Read the extracted audio
const audioBuffer = await fs.readFile(outputPath);
const audioSize = audioBuffer.length;
// Log audio extraction results
logger.log(`Extracted audio size: ${audioSize} bytes`);
logger.log(`Temporary audio file created`, { outputPath });
// Create the r2Key for the extracted audio, using the base name of the output path
const r2Key = `extracted-audio/${path.basename(outputPath)}`;
const uploadParams = {
Bucket: process.env.R2_BUCKET,
Key: r2Key,
Body: audioBuffer,
};
// Upload the audio to R2 and get the URL
await s3Client.send(new PutObjectCommand(uploadParams));
logger.log(`Extracted audio saved to your R2 bucket`, { r2Key });
// Delete the temporary audio file
await fs.unlink(outputPath);
logger.log(`Temporary audio file deleted`, { outputPath });
// Return the audio file path, size, and R2 URL
return {
Bucket: process.env.R2_BUCKET,
r2Key,
};
},
});
```
### Testing your task
To test this task, use this payload structure:
<Warning>
Make sure to provide a video URL that contains audio. If the video does not have audio, the task
will fail.
</Warning>
```json
{
"videoUrl": "<video-url>" // Replace <a-video-url> with the URL of the video you want to upload
}
```
## Generate a thumbnail from a video using FFmpeg
This task demonstrates how to use FFmpeg to generate a thumbnail from a video at a specific time and upload the generated thumbnail to R2 storage.
### Key Features
* Fetches a video from a given URL
* Generates a thumbnail from the video at the 5-second mark
* Uploads the generated thumbnail to R2 storage
### Task code
```ts trigger/ffmpeg-generate-thumbnail.ts
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { logger, task } from "@trigger.dev/sdk/v3";
import ffmpeg from "fluent-ffmpeg";
import fs from "fs/promises";
import fetch from "node-fetch";
import { Readable } from "node:stream";
import os from "os";
import path from "path";
// Initialize S3 client
const s3Client = new S3Client({
// How to authenticate to R2: https://developers.cloudflare.com/r2/api/s3/tokens/
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const ffmpegGenerateThumbnail = task({
id: "ffmpeg-generate-thumbnail",
run: async (payload: { videoUrl: string }) => {
const { videoUrl } = payload;
// Generate output file name
const tempDirectory = os.tmpdir();
const outputPath = path.join(tempDirectory, `thumbnail_${Date.now()}.jpg`);
// Fetch the video
const response = await fetch(videoUrl);
// Generate the thumbnail
await new Promise((resolve, reject) => {
if (!response.body) {
return reject(new Error("Failed to fetch video"));
}
ffmpeg(Readable.from(response.body))
.screenshots({
count: 1,
folder: "/tmp",
filename: path.basename(outputPath),
size: "320x240",
timemarks: ["5"], // 5 seconds
})
.on("end", resolve)
.on("error", reject);
});
// Read the generated thumbnail
const thumbnail = await fs.readFile(outputPath);
// Create the r2Key for the extracted audio, using the base name of the output path
const r2Key = `thumbnails/${path.basename(outputPath)}`;
const uploadParams = {
Bucket: process.env.R2_BUCKET,
Key: r2Key,
Body: thumbnail,
};
// Upload the thumbnail to R2 and get the URL
await s3Client.send(new PutObjectCommand(uploadParams));
const r2Url = `https://${process.env.R2_ACCOUNT_ID}.r2.cloudflarestorage.com/${process.env.R2_BUCKET}/${r2Key}`;
logger.log("Thumbnail uploaded to R2", { url: r2Url });
// Delete the temporary file
await fs.unlink(outputPath);
// Log thumbnail generation results
logger.log(`Thumbnail uploaded to S3: ${r2Url}`);
// Return the thumbnail buffer, path, and R2 URL
return {
thumbnailBuffer: thumbnail,
thumbnailPath: outputPath,
r2Url,
};
},
});
```
### Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"videoUrl": "<video-url>" // Replace <a-video-url> with the URL of the video you want to upload
}
```
## Local development
To test this example task locally, be sure to install any packages from the build extensions you added to your `trigger.config.ts` file to your local machine. In this case, you need to install {packages_0}.
# Crawl a URL using Firecrawl
Source: https://trigger.dev/docs/guides/examples/firecrawl-url-crawl
This example demonstrates how to crawl a URL using Firecrawl with Trigger.dev.
## Overview
Firecrawl is a tool for crawling websites and extracting clean markdown that's structured in an LLM-ready format.
Here are two examples of how to use Firecrawl with Trigger.dev:
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* A [Firecrawl](https://firecrawl.dev/) account
## Example 1: crawl an entire website with Firecrawl
This task crawls a website and returns the `crawlResult` object. You can set the `limit` parameter to control the number of URLs that are crawled.
```ts trigger/firecrawl-url-crawl.ts
import FirecrawlApp from "@mendable/firecrawl-js";
import { task } from "@trigger.dev/sdk/v3";
// Initialize the Firecrawl client with your API key
const firecrawlClient = new FirecrawlApp({
apiKey: process.env.FIRECRAWL_API_KEY, // Get this from your Firecrawl dashboard
});
export const firecrawlCrawl = task({
id: "firecrawl-crawl",
run: async (payload: { url: string }) => {
const { url } = payload;
// Crawl: scrapes all the URLs of a web page and return content in LLM-ready format
const crawlResult = await firecrawlClient.crawlUrl(url, {
limit: 100, // Limit the number of URLs to crawl
scrapeOptions: {
formats: ["markdown", "html"],
},
});
if (!crawlResult.success) {
throw new Error(`Failed to crawl: ${crawlResult.error}`);
}
return {
data: crawlResult,
};
},
});
```
### Testing your task
You can test your task by triggering it from the Trigger.dev dashboard.
```json
"url": "<url-to-crawl>" // Replace with the URL you want to crawl
```
## Example 2: scrape a single URL with Firecrawl
This task scrapes a single URL and returns the `scrapeResult` object.
```ts trigger/firecrawl-url-scrape.ts
import FirecrawlApp, { ScrapeResponse } from "@mendable/firecrawl-js";
import { task } from "@trigger.dev/sdk/v3";
// Initialize the Firecrawl client with your API key
const firecrawlClient = new FirecrawlApp({
apiKey: process.env.FIRECRAWL_API_KEY, // Get this from your Firecrawl dashboard
});
export const firecrawlScrape = task({
id: "firecrawl-scrape",
run: async (payload: { url: string }) => {
const { url } = payload;
// Scrape: scrapes a URL and get its content in LLM-ready format (markdown, structured data via LLM Extract, screenshot, html)
const scrapeResult = (await firecrawlClient.scrapeUrl(url, {
formats: ["markdown", "html"],
})) as ScrapeResponse;
if (!scrapeResult.success) {
throw new Error(`Failed to scrape: ${scrapeResult.error}`);
}
return {
data: scrapeResult,
};
},
});
```
### Testing your task
You can test your task by triggering it from the Trigger.dev dashboard.
```json
"url": "<url-to-scrape>" // Replace with the URL you want to scrape
```
# Convert documents to PDF using LibreOffice
Source: https://trigger.dev/docs/guides/examples/libreoffice-pdf-conversion
This example demonstrates how to convert documents to PDF using LibreOffice with Trigger.dev.
export const packages_0 = "libreoffice"
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [LibreOffice](https://www.libreoffice.org/download/libreoffice-fresh/) installed on your machine
* A [Cloudflare R2](https://developers.cloudflare.com) account and bucket
### Using our `aptGet` build extension to add the LibreOffice package
To deploy this task, you'll need to add LibreOffice to your project configuration, like this:
```ts trigger.config.ts
import { aptGet } from "@trigger.dev/build/extensions/core";
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
aptGet({
packages: ["libreoffice"],
}),
],
},
});
```
<Note>
[Build extensions](/config/extensions/overview) allow you to hook into the build system and
customize the build process or the resulting bundle and container image (in the case of
deploying). You can use pre-built extensions or create your own.
</Note>
You'll also need to add `@trigger.dev/build` to your `package.json` file under `devDependencies` if you don't already have it there.
## Convert a document to PDF using LibreOffice and upload to R2
This task demonstrates how to use LibreOffice to convert a document (.doc or .docx) to PDF and upload the PDF to an R2 storage bucket.
### Key Features
* Fetches a document from a given URL
* Converts the document to PDF
* Uploads the PDF to R2 storage
### Task code
```ts trigger/libreoffice-pdf-convert.ts
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { task } from "@trigger.dev/sdk/v3";
import libreoffice from "libreoffice-convert";
import { promisify } from "node:util";
import path from "path";
import fs from "fs";
const convert = promisify(libreoffice.convert);
// Initialize S3 client
const s3Client = new S3Client({
// How to authenticate to R2: https://developers.cloudflare.com/r2/api/s3/tokens/
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const libreOfficePdfConvert = task({
id: "libreoffice-pdf-convert",
run: async (payload: { documentUrl: string }, { ctx }) => {
// Set LibreOffice path for production environment
if (ctx.environment.type !== "DEVELOPMENT") {
process.env.LIBREOFFICE_PATH = "/usr/bin/libreoffice";
}
try {
// Create temporary file paths
const inputPath = path.join(process.cwd(), `input_${Date.now()}.docx`);
const outputPath = path.join(process.cwd(), `output_${Date.now()}.pdf`);
// Download file from URL
const response = await fetch(payload.documentUrl);
const buffer = Buffer.from(await response.arrayBuffer());
fs.writeFileSync(inputPath, buffer);
const inputFile = fs.readFileSync(inputPath);
// Convert to PDF using LibreOffice
const pdfBuffer = await convert(inputFile, ".pdf", undefined);
fs.writeFileSync(outputPath, pdfBuffer);
// Upload to R2
const key = `converted-pdfs/output_${Date.now()}.pdf`;
await s3Client.send(
new PutObjectCommand({
Bucket: process.env.R2_BUCKET,
Key: key,
Body: fs.readFileSync(outputPath),
})
);
// Cleanup temporary files
fs.unlinkSync(inputPath);
fs.unlinkSync(outputPath);
return { pdfLocation: key };
} catch (error) {
console.error("Error converting PDF:", error);
throw error;
}
},
});
```
### Testing your task
To test this task, use this payload structure:
```json
{
"documentUrl": "<a-document-url>" // Replace <a-document-url> with the URL of the document you want to convert
}
```
## Local development
To test this example task locally, be sure to install any packages from the build extensions you added to your `trigger.config.ts` file to your local machine. In this case, you need to install {packages_0}.
# Call OpenAI with retrying
Source: https://trigger.dev/docs/guides/examples/open-ai-with-retrying
This example will show you how to call OpenAI with retrying using Trigger.dev.
## Overview
Sometimes OpenAI calls can take a long time to complete, or they can fail. This task will retry if the API call fails completely or if the response is empty.
## Task code
```ts trigger/openai.ts
import { task } from "@trigger.dev/sdk/v3";
import OpenAI from "openai";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
export const openaiTask = task({
id: "openai-task",
//specifying retry options overrides the defaults defined in your trigger.config file
retry: {
maxAttempts: 10,
factor: 1.8,
minTimeoutInMs: 500,
maxTimeoutInMs: 30_000,
randomize: false,
},
run: async (payload: { prompt: string }) => {
//if this fails, it will throw an error and retry
const chatCompletion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
});
if (chatCompletion.choices[0]?.message.content === undefined) {
//sometimes OpenAI returns an empty response, let's retry by throwing an error
throw new Error("OpenAI call failed");
}
return chatCompletion.choices[0].message.content;
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"prompt": "What is the meaning of life?"
}
```
# Turn a PDF into an image using MuPDF
Source: https://trigger.dev/docs/guides/examples/pdf-to-image
This example will show you how to turn a PDF into an image using MuPDF and Trigger.dev.
export const packages_0 = "mupdf-tools from MuPDF"
## Overview
This example demonstrates how to use Trigger.dev to turn a PDF into a series of images using MuPDF and upload them to Cloudflare R2.
## Update your build configuration
To use this example, add these build settings below to your `trigger.config.ts` file. They ensure that the `mutool` and `curl` packages are installed when you deploy your task. You can learn more about this and see more build settings [here](/config/extensions/aptGet).
```ts trigger.config.ts
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [aptGet({ packages: ["mupdf-tools", "curl"] })],
},
});
```
## Task code
```ts trigger/pdfToImage.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { execSync } from "child_process";
import fs from "fs";
import path from "path";
// Initialize S3 client
const s3Client = new S3Client({
region: "auto",
endpoint: process.env.S3_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const pdfToImage = task({
id: "pdf-to-image",
run: async (payload: { pdfUrl: string; documentId: string }) => {
logger.log("Converting PDF to images", payload);
const pdfPath = `/tmp/${payload.documentId}.pdf`;
const outputDir = `/tmp/${payload.documentId}`;
// Download PDF and convert to images using MuPDF
execSync(`curl -s -o ${pdfPath} ${payload.pdfUrl}`);
fs.mkdirSync(outputDir, { recursive: true });
execSync(`mutool convert -o ${outputDir}/page-%d.png ${pdfPath}`);
// Upload images to R2
const uploadedUrls = [];
for (const file of fs.readdirSync(outputDir)) {
const s3Key = `images/${payload.documentId}/${file}`;
const uploadParams = {
Bucket: process.env.S3_BUCKET,
Key: s3Key,
Body: fs.readFileSync(path.join(outputDir, file)),
ContentType: "image/png",
};
logger.log("Uploading to R2", uploadParams);
await s3Client.send(new PutObjectCommand(uploadParams));
const s3Url = `https://${process.env.S3_BUCKET}.r2.cloudflarestorage.com/${s3Key}`;
uploadedUrls.push(s3Url);
logger.log("Image uploaded to R2", { url: s3Url });
}
// Clean up
fs.rmSync(outputDir, { recursive: true, force: true });
fs.unlinkSync(pdfPath);
logger.log("All images uploaded to R2", { urls: uploadedUrls });
return {
imageUrls: uploadedUrls,
};
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"pdfUrl": "https://pdfobject.com/pdf/sample.pdf",
"documentId": "unique-document-id"
}
```
## Local development
To test this example task locally, be sure to install any packages from the build extensions you added to your `trigger.config.ts` file to your local machine. In this case, you need to install {packages_0}.
# Puppeteer
Source: https://trigger.dev/docs/guides/examples/puppeteer
These examples demonstrate how to use Puppeteer with Trigger.dev.
export const packages_0 = "the Puppeteer library."
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [Puppeteer](https://pptr.dev/guides/installation) installed on your machine
## Overview
There are 3 example tasks to follow on this page:
1. [Basic example](/guides/examples/puppeteer#basic-example)
2. [Generate a PDF from a web page](/guides/examples/puppeteer#generate-a-pdf-from-a-web-page)
3. [Scrape content from a web page](/guides/examples/puppeteer#scrape-content-from-a-web-page)
<Warning>
**WEB SCRAPING:** When web scraping, you MUST use a proxy to comply with our terms of service. Direct scraping of third-party websites without the site owner's permission using Trigger.dev Cloud is prohibited and will result in account suspension. See [this example](/guides/examples/puppeteer#scrape-content-from-a-web-page) which uses a proxy.
</Warning>
## Build configuration
To use all examples on this page, you'll first need to add these build settings to your `trigger.config.ts` file:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { puppeteer } from "@trigger.dev/build/extensions/puppeteer";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
// This is required to use the Puppeteer library
extensions: [puppeteer()],
},
});
```
Learn more about the [trigger.config.ts](/config/config-file) file including setting default retry settings, customizing the build environment, and more.
## Set an environment variable
Set the following environment variable in your [Trigger.dev dashboard](/deploy-environment-variables) or [using the SDK](/deploy-environment-variables#in-your-code):
```bash
PUPPETEER_EXECUTABLE_PATH: "/usr/bin/google-chrome-stable",
```
## Basic example
### Overview
In this example we use [Puppeteer](https://pptr.dev/) to log out the title of a web page, in this case from the [Trigger.dev](https://trigger.dev) landing page.
### Task code
```ts trigger/puppeteer-basic-example.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import puppeteer from "puppeteer";
export const puppeteerTask = task({
id: "puppeteer-log-title",
run: async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
await page.goto("https://trigger.dev");
const content = await page.title();
logger.info("Content", { content });
await browser.close();
},
});
```
### Testing your task
There's no payload required for this task so you can just click "Run test" from the Test page in the dashboard. Learn more about testing tasks [here](/run-tests).
## Generate a PDF from a web page
### Overview
In this example we use [Puppeteer](https://pptr.dev/) to generate a PDF from the [Trigger.dev](https://trigger.dev) landing page and upload it to [Cloudflare R2](https://developers.cloudflare.com/r2/).
### Task code
```ts trigger/puppeteer-generate-pdf.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import puppeteer from "puppeteer";
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
// Initialize S3 client
const s3Client = new S3Client({
region: "auto",
endpoint: process.env.S3_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const puppeteerWebpageToPDF = task({
id: "puppeteer-webpage-to-pdf",
run: async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
const response = await page.goto("https://trigger.dev");
const url = response?.url() ?? "No URL found";
// Generate PDF from the web page
const generatePdf = await page.pdf();
logger.info("PDF generated from URL", { url });
await browser.close();
// Upload to R2
const s3Key = `pdfs/test.pdf`;
const uploadParams = {
Bucket: process.env.S3_BUCKET,
Key: s3Key,
Body: generatePdf,
ContentType: "application/pdf",
};
logger.log("Uploading to R2 with params", uploadParams);
// Upload the PDF to R2 and return the URL.
await s3Client.send(new PutObjectCommand(uploadParams));
const s3Url = `https://${process.env.S3_BUCKET}.s3.amazonaws.com/${s3Key}`;
logger.log("PDF uploaded to R2", { url: s3Url });
return { pdfUrl: s3Url };
},
});
```
### Testing your task
There's no payload required for this task so you can just click "Run test" from the Test page in the dashboard. Learn more about testing tasks [here](/run-tests).
## Scrape content from a web page
### Overview
In this example we use [Puppeteer](https://pptr.dev/) with a [BrowserBase](https://www.browserbase.com/) proxy to scrape the GitHub stars count from the [Trigger.dev](https://trigger.dev) landing page and log it out. See [this list](/guides/examples/puppeteer#proxying) for more proxying services we recommend.
<Warning>
When web scraping, you MUST use the technique below which uses a proxy with Puppeteer. Direct
scraping without using `browserWSEndpoint` is prohibited and will result in account suspension.
</Warning>
### Task code
```ts trigger/scrape-website.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import puppeteer from "puppeteer-core";
export const puppeteerScrapeWithProxy = task({
id: "puppeteer-scrape-with-proxy",
run: async () => {
const browser = await puppeteer.connect({
browserWSEndpoint: `wss://connect.browserbase.com?apiKey=${process.env.BROWSERBASE_API_KEY}`,
});
const page = await browser.newPage();
try {
// Navigate to the target website
await page.goto("https://trigger.dev", { waitUntil: "networkidle0" });
// Scrape the GitHub stars count
const starCount = await page.evaluate(() => {
const starElement = document.querySelector(".github-star-count");
const text = starElement?.textContent ?? "0";
const numberText = text.replace(/[^0-9]/g, "");
return parseInt(numberText);
});
logger.info("GitHub star count", { starCount });
return { starCount };
} catch (error) {
logger.error("Error during scraping", {
error: error instanceof Error ? error.message : String(error),
});
throw error;
} finally {
await browser.close();
}
},
});
```
### Testing your task
There's no payload required for this task so you can just click "Run test" from the Test page in the dashboard. Learn more about testing tasks [here](/run-tests).
## Local development
To test this example task locally, be sure to install any packages from the build extensions you added to your `trigger.config.ts` file to your local machine. In this case, you need to install {packages_0}.
## Proxying
If you're using Trigger.dev Cloud and Puppeteer or any other tool to scrape content from websites you don't own, you'll need to proxy your requests. **If you don't you'll risk getting our IP address blocked and we will ban you from our service. You must always have permission from the website owner to scrape their content.**
Here are a list of proxy services we recommend:
* [Browserbase](https://www.browserbase.com/)
* [Brightdata](https://brightdata.com/)
* [Browserless](https://browserless.io/)
* [Oxylabs](https://oxylabs.io/)
* [ScrapingBee](https://scrapingbee.com/)
* [Smartproxy](https://smartproxy.com/)
# Generate a PDF using react-pdf and save it to R2
Source: https://trigger.dev/docs/guides/examples/react-pdf
This example will show you how to generate a PDF using Trigger.dev.
## Overview
This example demonstrates how to use Trigger.dev to generate a PDF using `react-pdf` and save it to Cloudflare R2.
## Task code
<Info> This example must be a .tsx file to use React components.</Info>
```ts trigger/generateResumePDF.tsx
import { logger, task } from "@trigger.dev/sdk/v3";
import { renderToBuffer, Document, Page, Text, View } from "@react-pdf/renderer";
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
// Initialize R2 client
const r2Client = new S3Client({
// How to authenticate to R2: https://developers.cloudflare.com/r2/api/s3/tokens/
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const generateResumePDF = task({
id: "generate-resume-pdf",
run: async (payload: { text: string }) => {
// Log the payload
logger.log("Generating PDF resume", payload);
// Render the ResumeDocument component to a PDF buffer
const pdfBuffer = await renderToBuffer(
<Document>
<Page size="A4">
<View>
<Text>{payload.text}</Text>
</View>
</Page>
</Document>
);
// Generate a unique filename based on the text and current timestamp
const filename = `${payload.text.replace(/\s+/g, "-").toLowerCase()}-${Date.now()}.pdf`;
// Set the R2 key for the PDF file
const r2Key = `resumes/${filename}`;
// Set the upload parameters for R2
const uploadParams = {
Bucket: process.env.R2_BUCKET,
Key: r2Key,
Body: pdfBuffer,
ContentType: "application/pdf",
};
// Log the upload parameters
logger.log("Uploading to R2 with params", uploadParams);
// Upload the PDF to R2
await r2Client.send(new PutObjectCommand(uploadParams));
// Return the Bucket and R2 key for the uploaded PDF
return {
Bucket: process.env.R2_BUCKET,
Key: r2Key,
};
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"text": "Hello, world!"
}
```
# Send a sequence of emails using Resend
Source: https://trigger.dev/docs/guides/examples/resend-email-sequence
This example will show you how to send a sequence of emails over several days using Resend with Trigger.dev.
## Overview
Each email is wrapped in retry.onThrow. This will retry the block of code if an error is thrown. This is useful when you donâ€™t want to retry the whole task, but just a part of it. The entire task will use the default retrying, so can also retry.
Additionally this task uses wait.for to wait for a certain amount of time before sending the next email. During the waiting time, the task will be paused and will not consume any resources.
## Task code
```ts trigger/email-sequence.ts
import { Resend } from "resend";
const resend = new Resend(process.env.RESEND_ASP_KEY);
export const emailSequence = task({
id: "email-sequence",
run: async (payload: { userId: string; email: string; name: string }) => {
console.log(`Start email sequence for user ${payload.userId}`, payload);
// Send the first email immediately
const firstEmailResult = await retry.onThrow(
async ({ attempt }) => {
const { data, error } = await resend.emails.send({
from: "hello@trigger.dev",
to: payload.email,
subject: "Welcome to Trigger.dev",
html: `<p>Hello ${payload.name},</p><p>Welcome to Trigger.dev</p>`,
});
if (error) {
// Throwing an error will trigger a retry of this block
throw error;
}
return data;
},
{ maxAttempts: 3 }
);
// Then wait 3 days
await wait.for({ days: 3 });
// Send the second email
const secondEmailResult = await retry.onThrow(
async ({ attempt }) => {
const { data, error } = await resend.emails.send({
from: "hello@trigger.dev",
to: payload.email,
subject: "Some tips for you",
html: `<p>Hello ${payload.name},</p><p>Here are some tips for youâ€¦</p>`,
});
if (error) {
// Throwing an error will trigger a retry of this block
throw error;
}
return data;
},
{ maxAttempts: 3 }
);
//etc...
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"userId": "123",
"email": "<your-test-email>", // Replace with your test email
"name": "Alice Testington"
}
```
# Scrape the top 3 articles from Hacker News and email yourself a summary every weekday
Source: https://trigger.dev/docs/guides/examples/scrape-hacker-news
This example demonstrates how to scrape the top 3 articles from Hacker News using BrowserBase and Puppeteer, summarize them with ChatGPT and send a nicely formatted email summary to yourself every weekday using Resend.
export const packages_0 = "the Puppeteer library"
<iframe width="100%" height="315" src="https://www.youtube.com/embed/6azvzrZITKY?si=muKtsBiS9TJGGKWg" title="YouTube video player" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen />
## Overview
In this example we'll be using a number of different tools and features to:
1. Scrape the content of the top 3 articles from Hacker News
2. Summarize each article
3. Email the summaries to yourself
And we'll be using the following tools and features:
* [Schedules](/tasks/scheduled) to run the task every weekday at 9 AM
* [Batch Triggering](/triggering#yourtask-batchtriggerandwait) to run separate child tasks for each article while the parent task waits for them all to complete
* [idempotencyKey](/triggering#idempotencykey) to prevent tasks being triggered multiple times
* [BrowserBase](https://browserbase.com/) to proxy the scraping of the Hacker News articles
* [Puppeteer](https://pptr.dev/) to scrape the articles linked from Hacker News
* [OpenAI](https://platform.openai.com/docs/overview) to summarize the articles
* [Resend](https://resend.com/) to send a nicely formatted email summary
<Warning>
**WEB SCRAPING:** When web scraping, you MUST use a proxy to comply with our terms of service. Direct scraping of third-party websites without the site owner's permission using Trigger.dev Cloud is prohibited and will result in account suspension. See [this example](/guides/examples/puppeteer#scrape-content-from-a-web-page) which uses a proxy.
</Warning>
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [Puppeteer](https://pptr.dev/guides/installation) installed on your machine
* A [BrowserBase](https://browserbase.com/) account
* An [OpenAI](https://platform.openai.com/docs/overview) account
* A [Resend](https://resend.com/) account
## Build configuration
First up, add these build settings to your `trigger.config.ts` file:
```tsx trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { puppeteer } from "@trigger.dev/build/extensions/puppeteer";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
// This is required to use the Puppeteer library
extensions: [puppeteer()],
},
});
```
Learn more about the [trigger.config.ts](/config/config-file) file including setting default retry settings, customizing the build environment, and more.
### Environment variables
Set the following environment variable in your local `.env` file to run this task locally. And before deploying your task, set them in the [Trigger.dev dashboard](/deploy-environment-variables) or [using the SDK](/deploy-environment-variables#in-your-code):
```bash
BROWSERBASE_API_KEY: "<your BrowserBase API key>"
OPENAI_API_KEY: "<your OpenAI API key>"
RESEND_API_KEY: "<your Resend API key>"
```
### Task code
```ts trigger/scrape-hacker-news.ts
import { render } from "@react-email/render";
import { logger, schedules, task, wait } from "@trigger.dev/sdk/v3";
import { OpenAI } from "openai";
import puppeteer from "puppeteer-core";
import { Resend } from "resend";
import { HNSummaryEmail } from "./summarize-hn-email";
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const resend = new Resend(process.env.RESEND_API_KEY);
// Parent task (scheduled to run 9AM every weekday)
export const summarizeHackerNews = schedules.task({
id: "summarize-hacker-news",
cron: {
pattern: "0 9 * * 1-5",
timezone: "Europe/London",
}, // Run at 9 AM, Monday to Friday
run: async () => {
// Connect to BrowserBase to proxy the scraping of the Hacker News articles
const browser = await puppeteer.connect({
browserWSEndpoint: `wss://connect.browserbase.com?apiKey=${process.env.BROWSERBASE_API_KEY}`,
});
logger.info("Connected to Browserbase");
const page = await browser.newPage();
// Navigate to Hacker News and scrape top 3 articles
await page.goto("https://news.ycombinator.com/news", {
waitUntil: "networkidle0",
});
logger.info("Navigated to Hacker News");
const articles = await page.evaluate(() => {
const items = document.querySelectorAll(".athing");
return Array.from(items)
.slice(0, 3)
.map((item) => {
const titleElement = item.querySelector(".titleline > a");
const link = titleElement?.getAttribute("href");
const title = titleElement?.textContent;
return { title, link };
});
});
logger.info("Scraped top 3 articles", { articles });
await browser.close();
await wait.for({ seconds: 5 });
// Use batchTriggerAndWait to process articles
const summaries = await scrapeAndSummarizeArticle
.batchTriggerAndWait(
articles.map((article) => ({
payload: { title: article.title!, link: article.link! },
}))
)
.then((batch) => batch.runs.filter((run) => run.ok).map((run) => run.output));
// Send email using Resend
await resend.emails.send({
from: "Hacker News Summary <hi@demo.tgr.dev>",
to: ["james@trigger.dev"],
subject: "Your morning HN summary",
html: render(<HNSummaryEmail articles={summaries} />),
});
logger.info("Email sent successfully");
},
});
// Child task for scraping and summarizing individual articles
export const scrapeAndSummarizeArticle = task({
id: "scrape-and-summarize-articles",
retry: {
maxAttempts: 3,
minTimeoutInMs: 5000,
maxTimeoutInMs: 10000,
factor: 2,
randomize: true,
},
run: async ({ title, link }: { title: string; link: string }) => {
logger.info(`Summarizing ${title}`);
const browser = await puppeteer.connect({
browserWSEndpoint: `wss://connect.browserbase.com?apiKey=${process.env.BROWSERBASE_API_KEY}`,
});
const page = await browser.newPage();
// Prevent all assets from loading, images, stylesheets etc
await page.setRequestInterception(true);
page.on("request", (request) => {
if (["script", "stylesheet", "image", "media", "font"].includes(request.resourceType())) {
request.abort();
} else {
request.continue();
}
});
await page.goto(link, { waitUntil: "networkidle0" });
logger.info(`Navigated to article: ${title}`);
// Extract the main content of the article
const content = await page.evaluate(() => {
const articleElement = document.querySelector("article") || document.body;
return articleElement.innerText.trim().slice(0, 1500); // Limit to 1500 characters
});
await browser.close();
logger.info(`Extracted content for article: ${title}`, { content });
// Summarize the content using ChatGPT
const response = await openai.chat.completions.create({
model: "gpt-4o",
messages: [
{
role: "user",
content: `Summarize this article in 2-3 concise sentences:\n\n${content}`,
},
],
});
logger.info(`Generated summary for article: ${title}`);
return {
title,
link,
summary: response.choices[0].message.content,
};
},
});
```
## Create your email template using React Email
To prevent the main example from becoming too cluttered, we'll create a separate file for our email template. It's formatted using [React Email](https://react.email/docs/introduction) components so you'll need to install the package to use it.
Notice how this file is imported into the main task code and passed to Resend to send the email.
```tsx summarize-hn-email.tsx
import { Html, Head, Body, Container, Section, Heading, Text, Link } from "@react-email/components";
interface Article {
title: string;
link: string;
summary: string | null;
}
export const HNSummaryEmail: React.FC<{ articles: Article[] }> = ({ articles }) => (
<Html>
<Head />
<Body style={{ fontFamily: "Arial, sans-serif", padding: "20px" }}>
<Container>
<Heading as="h1">Your Morning HN Summary</Heading>
{articles.map((article, index) => (
<Section key={index} style={{ marginBottom: "20px" }}>
<Heading as="h3">
<Link href={article.link}>{article.title}</Link>
</Heading>
<Text>{article.summary || "No summary available"}</Text>
</Section>
))}
</Container>
</Body>
</Html>
);
```
## Local development
To test this example task locally, be sure to install any packages from the build extensions you added to your `trigger.config.ts` file to your local machine. In this case, you need to install {packages_0}.
## Testing your task
To test this task in the dashboard, use the Test page and set the schedule date to "Now" to ensure the task triggers immediately. Then click "Run test" and wait for the task to complete.
# Track errors with Sentry
Source: https://trigger.dev/docs/guides/examples/sentry-error-tracking
This example demonstrates how to track errors with Sentry using Trigger.dev.
## Overview
Automatically send errors and source maps to your Sentry project from your Trigger.dev tasks. Sending source maps to Sentry allows for more detailed stack traces when errors occur, as Sentry can map the minified code back to the original source code.
## Prerequisites
* A [Sentry](https://sentry.io) account and project
* A [Trigger.dev](https://trigger.dev) account and project
## Build configuration
To send errors to Sentry when there are errors in your tasks, you'll need to add this build configuration to your `trigger.config.ts` file. This will then run every time you deploy your project.
<Note>
You will need to set the `SENTRY_AUTH_TOKEN` and `SENTRY_DSN` environment variables. You can find
the `SENTRY_AUTH_TOKEN` in your Sentry dashboard, in settings -> developer settings -> auth tokens
and the `SENTRY_DSN` in your Sentry dashboard, in settings -> projects -> your project -> client
keys (DSN). Add these to your `.env` file, and in your [Trigger.dev
dashboard](https://cloud.trigger.dev), under environment variables in your project's sidebar.
</Note>
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { esbuildPlugin } from "@trigger.dev/build/extensions";
import { sentryEsbuildPlugin } from "@sentry/esbuild-plugin";
import * as Sentry from "@sentry/node";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
esbuildPlugin(
sentryEsbuildPlugin({
org: "<your-sentry-org>",
project: "<your-sentry-project>",
// Find this auth token in settings -> developer settings -> auth tokens
authToken: process.env.SENTRY_AUTH_TOKEN,
}),
{ placement: "last", target: "deploy" }
),
],
},
init: async () => {
Sentry.init({
// The Data Source Name (DSN) is a unique identifier for your Sentry project.
dsn: process.env.SENTRY_DSN,
// Update this to match the environment you want to track errors for
environment: process.env.NODE_ENV === "production" ? "production" : "development",
});
},
onFailure: async (payload, error, { ctx }) => {
Sentry.captureException(error, {
extra: {
payload,
ctx,
},
});
},
});
```
<Note>
[Build extensions](/config/extensions/overview) allow you to hook into the build system and
customize the build process or the resulting bundle and container image (in the case of
deploying). You can use pre-built extensions or create your own.
</Note>
## Testing that errors are being sent to Sentry
To test that errors are being sent to Sentry, you need to create a task that will fail.
This task takes no payload, and will throw an error.
```ts trigger/sentry-error-test.ts
import { task } from "@trigger.dev/sdk/v3";
export const sentryErrorTest = task({
id: "sentry-error-test",
retry: {
// Only retry once
maxAttempts: 1,
},
run: async () => {
const error = new Error("This is a custom error that Sentry will capture");
error.cause = { additionalContext: "This is additional context" };
throw error;
},
});
```
After creating the task, deploy your project.
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
Once deployed, navigate to the `test` page in the sidebar of your [Trigger.dev dashboard](https://cloud.trigger.dev), click on your `prod` environment, and select the `sentryErrorTest` task.
Run a test task with an empty payload by clicking the `Run test` button.
Your run should then fail, and if everything is set up correctly, you will see an error in the Sentry project dashboard shortly after.
# Process images using Sharp
Source: https://trigger.dev/docs/guides/examples/sharp-image-processing
This example demonstrates how to process images using the Sharp library with Trigger.dev.
export const packages_0 = "the Sharp image processing library"
## Overview
This task processes and watermarks an image using the Sharp library, and then uploads it to R2 storage.
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* The [Sharp](https://sharp.pixelplumbing.com/install) library installed on your machine
* An R2-compatible object storage service, such as [Cloudflare R2](https://developers.cloudflare.com/r2)
## Adding the build configuration
To use this example, you'll first need to add these build settings to your `trigger.config.ts` file:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
// This is required to use the Sharp library
external: ["sharp"],
},
});
```
<Note>
Any packages that install or build a native binary should be added to external, as native binaries
cannot be bundled.
</Note>
## Key features
* Resizes a JPEG image to 800x800 pixels
* Adds a watermark to the image, positioned in the bottom-right corner, using a PNG image
* Uploads the processed image to R2 storage
## Task code
```ts trigger/sharp-image-processing.ts
import { S3Client } from "@aws-sdk/client-s3";
import { Upload } from "@aws-sdk/lib-storage";
import { logger, task } from "@trigger.dev/sdk/v3";
import fs from "fs/promises";
import os from "os";
import path from "path";
import sharp from "sharp";
// Initialize R2 client using your R2 account details
const r2Client = new S3Client({
region: "auto",
endpoint: process.env.R2_ENDPOINT,
credentials: {
accessKeyId: process.env.R2_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.R2_SECRET_ACCESS_KEY ?? "",
},
});
export const sharpProcessImage = task({
id: "sharp-process-image",
retry: { maxAttempts: 1 },
run: async (payload: { imageUrl: string; watermarkUrl: string }) => {
const { imageUrl, watermarkUrl } = payload;
const outputPath = path.join(os.tmpdir(), `output_${Date.now()}.jpg`);
const [imageResponse, watermarkResponse] = await Promise.all([
fetch(imageUrl),
fetch(watermarkUrl),
]);
const imageBuffer = await imageResponse.arrayBuffer();
const watermarkBuffer = await watermarkResponse.arrayBuffer();
await sharp(Buffer.from(imageBuffer))
.resize(800, 800) // Resize the image to 800x800px
.composite([
{
input: Buffer.from(watermarkBuffer),
gravity: "southeast", // Position the watermark in the bottom-right corner
},
])
.jpeg() // Convert to jpeg
.toBuffer() // Convert to buffer
.then(async (outputBuffer) => {
await fs.writeFile(outputPath, outputBuffer); // Write the buffer to file
const r2Key = `processed-images/${path.basename(outputPath)}`;
const uploadParams = {
Bucket: process.env.R2_BUCKET,
Key: r2Key,
Body: await fs.readFile(outputPath),
};
const upload = new Upload({
client: r2Client,
params: uploadParams,
});
await upload.done();
logger.log("Image uploaded to R2 storage.", {
path: `/${process.env.R2_BUCKET}/${r2Key}`,
});
await fs.unlink(outputPath); // Clean up the temporary file
return { r2Key };
});
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"imageUrl": "<an-image-url.jpg>", // Replace with a URL to a JPEG image
"watermarkUrl": "<an-image-url.png>" // Replace with a URL to a PNG watermark image
}
```
## Local development
To test this example task locally, be sure to install any packages from the build extensions you added to your `trigger.config.ts` file to your local machine. In this case, you need to install {packages_0}.
# Trigger a task from Stripe webhook events
Source: https://trigger.dev/docs/guides/examples/stripe-webhook
This example demonstrates how to handle Stripe webhook events using Trigger.dev.
## Overview
This example shows how to set up a webhook handler in your existing app for incoming Stripe events. The handler triggers a task when a `checkout.session.completed` event is received. This is easily customisable to handle other Stripe events.
## Key features
* Shows how to create a Stripe webhook handler in your app
* Triggers a task from your backend when a `checkout.session.completed` event is received
## Environment variables
You'll need to configure the following environment variables for this example to work:
* `STRIPE_WEBHOOK_SECRET` The secret key used to verify the Stripe webhook signature.
* `TRIGGER_API_URL` Your Trigger.dev API url: `https://api.trigger.dev`
* `TRIGGER_SECRET_KEY` Your Trigger.dev secret key
## Setting up the Stripe webhook handler
First you'll need to create a [Stripe webhook](https://stripe.com/docs/webhooks) handler route that listens for POST requests and verifies the Stripe signature.
Here are examples of how you can set up a handler using different frameworks:
<CodeGroup>
```ts Next.js
// app/api/stripe-webhook/route.ts
import { NextResponse } from "next/server";
import { tasks } from "@trigger.dev/sdk/v3";
import Stripe from "stripe";
import type { stripeCheckoutCompleted } from "@/trigger/stripe-checkout-completed";
//     ðŸ‘† **type-only** import
export async function POST(request: Request) {
const signature = request.headers.get("stripe-signature");
const payload = await request.text();
if (!signature || !payload) {
return NextResponse.json(
{ error: "Invalid Stripe payload/signature" },
{
status: 400,
}
);
}
const event = Stripe.webhooks.constructEvent(
payload,
signature,
process.env.STRIPE_WEBHOOK_SECRET as string
);
// Perform the check based on the event type
switch (event.type) {
case "checkout.session.completed": {
// Trigger the task only if the event type is "checkout.session.completed"
const { id } = await tasks.trigger<typeof stripeCheckoutCompleted>(
"stripe-checkout-completed",
event.data.object
);
return NextResponse.json({ runId: id });
}
default: {
// Return a response indicating that the event is not handled
return NextResponse.json(
{ message: "Event not handled" },
{
status: 200,
}
);
}
}
}
```
```ts Remix
// app/webhooks.stripe.ts
import { type ActionFunctionArgs, json } from "@remix-run/node";
import type { stripeCheckoutCompleted } from "src/trigger/stripe-webhook";
//     ðŸ‘† **type-only** import
import { tasks } from "@trigger.dev/sdk/v3";
import Stripe from "stripe";
export async function action({ request }: ActionFunctionArgs) {
// Validate the Stripe webhook payload
const signature = request.headers.get("stripe-signature");
const payload = await request.text();
if (!signature || !payload) {
return json({ error: "Invalid Stripe payload/signature" }, { status: 400 });
}
const event = Stripe.webhooks.constructEvent(
payload,
signature,
process.env.STRIPE_WEBHOOK_SECRET as string
);
// Perform the check based on the event type
switch (event.type) {
case "checkout.session.completed": {
// Trigger the task only if the event type is "checkout.session.completed"
const { id } = await tasks.trigger<typeof stripeCheckoutCompleted>(
"stripe-checkout-completed",
event.data.object
);
return json({ runId: id });
}
default: {
// Return a response indicating that the event is not handled
return json({ message: "Event not handled" }, { status: 200 });
}
}
}
```
</CodeGroup>
## Task code
This task is triggered when a `checkout.session.completed` event is received from Stripe.
```ts trigger/stripe-checkout-completed.ts
import { task } from "@trigger.dev/sdk/v3";
import type stripe from "stripe";
export const stripeCheckoutCompleted = task({
id: "stripe-checkout-completed",
run: async (payload: stripe.Checkout.Session) => {
// Add your custom logic for handling the checkout.session.completed event here
},
});
```
## Testing your task locally
To test everything is working you can use the Stripe CLI to send test events to your endpoint:
1. Install the [Stripe CLI](https://stripe.com/docs/stripe-cli#install), and login
2. Follow the instructions to [test your handler](https://docs.stripe.com/webhooks#test-webhook). This will include a temporary `STRIPE_WEBHOOK_SECRET` that you can use for testing.
3. When triggering the event, use the `checkout.session.completed` event type. With the Stripe CLI: `stripe trigger checkout.session.completed`
4. If your endpoint is set up correctly, you should see the Stripe events logged in your console with a status of `200`.
5. Then, check the [Trigger.dev](https://cloud.trigger.dev) dashboard and you should see the successful run of the `stripe-webhook` task.
For more information on setting up and testing Stripe webhooks, refer to the [Stripe Webhook Documentation](https://stripe.com/docs/webhooks).
# Supabase database operations using Trigger.dev
Source: https://trigger.dev/docs/guides/examples/supabase-database-operations
These examples demonstrate how to run basic CRUD operations on a table in a Supabase database using Trigger.dev.
## Add a new user to a table in a Supabase database
This is a basic task which inserts a new row into a table from a Trigger.dev task.
### Key features
* Shows how to set up a Supabase client using the `@supabase/supabase-js` library
* Shows how to add a new row to a table using `insert`
### Prerequisites
* A [Supabase account](https://supabase.com/dashboard/) and a project set up
* In your Supabase project, create a table called `user_subscriptions`.
* In your `user_subscriptions` table, create a new column:
* `user_id`, with the data type: `text`
### Task code
```ts trigger/supabase-database-insert.ts
import { createClient } from "@supabase/supabase-js";
import { task } from "@trigger.dev/sdk/v3";
// Generate the Typescript types using the Supabase CLI: https://supabase.com/docs/guides/api/rest/generating-types
import { Database } from "database.types";
// Create a single Supabase client for interacting with your database
// 'Database' supplies the type definitions to supabase-js
const supabase = createClient<Database>(
// These details can be found in your Supabase project settings under `API`
process.env.SUPABASE_PROJECT_URL as string, // e.g. https://abc123.supabase.co - replace 'abc123' with your project ID
process.env.SUPABASE_SERVICE_ROLE_KEY as string // Your service role secret key
);
export const supabaseDatabaseInsert = task({
id: "add-new-user",
run: async (payload: { userId: string }) => {
const { userId } = payload;
// Insert a new row into the user_subscriptions table with the provided userId
const { error } = await supabase.from("user_subscriptions").insert({
user_id: userId,
});
// If there was an error inserting the new user, throw an error
if (error) {
throw new Error(`Failed to insert new user: ${error.message}`);
}
return {
message: `New user added successfully: ${userId}`,
};
},
});
```
<Note>
This task uses your service role secret key to bypass Row Level Security. There are different ways
of configuring your [RLS
policies](https://supabase.com/docs/guides/database/postgres/row-level-security), so always make
sure you have the correct permissions set up for your project.
</Note>
### Testing your task
To test this task in the [Trigger.dev dashboard](https://cloud.trigger.dev), you can use the following payload:
```json
{
"userId": "user_12345"
}
```
If the task completes successfully, you will see a new row in your `user_subscriptions` table with the `user_id` set to `user_12345`.
## Update a user's subscription on a table in a Supabase database
This task shows how to update a user's subscription on a table. It checks if the user already has a subscription and either inserts a new row or updates an existing row with the new plan.
This type of task is useful for managing user subscriptions, updating user details, or performing other operations you might need to do on a database table.
### Key features
* Shows how to set up a Supabase client using the `@supabase/supabase-js` library
* Adds a new row to the table if the user doesn't exist using `insert`
* Checks if the user already has a plan, and if they do updates the existing row using `update`
* Demonstrates how to use [AbortTaskRunError](https://trigger.dev/docs/errors-retrying#using-aborttaskrunerror) to stop the task run without retrying if an invalid plan type is provided
### Prerequisites
* A [Supabase account](https://supabase.com/dashboard/) and a project set up
* In your Supabase project, create a table called `user_subscriptions` (if you haven't already)
* In your `user_subscriptions` table, create these columns (if they don't already exist):
* `user_id`, with the data type: `text`
* `plan`, with the data type: `text`
* `updated_at`, with the data type: `timestamptz`
### Task code
```ts trigger/supabase-update-user-subscription.ts
import { createClient } from "@supabase/supabase-js";
import { AbortTaskRunError, task } from "@trigger.dev/sdk/v3";
// Generate the Typescript types using the Supabase CLI: https://supabase.com/docs/guides/api/rest/generating-types
import { Database } from "database.types";
// Define the allowed plan types
type PlanType = "hobby" | "pro" | "enterprise";
// Create a single Supabase client for interacting with your database
// 'Database' supplies the type definitions to supabase-js
const supabase = createClient<Database>(
// These details can be found in your Supabase project settings under `API`
process.env.SUPABASE_PROJECT_URL as string, // e.g. https://abc123.supabase.co - replace 'abc123' with your project ID
process.env.SUPABASE_SERVICE_ROLE_KEY as string // Your service role secret key
);
export const supabaseUpdateUserSubscription = task({
id: "update-user-subscription",
run: async (payload: { userId: string; newPlan: PlanType }) => {
const { userId, newPlan } = payload;
// Abort the task run without retrying if the new plan type is invalid
if (!["hobby", "pro", "enterprise"].includes(newPlan)) {
throw new AbortTaskRunError(
`Invalid plan type: ${newPlan}. Allowed types are 'hobby', 'pro', or 'enterprise'.`
);
}
// Query the user_subscriptions table to check if the user already has a subscription
const { data: existingSubscriptions } = await supabase
.from("user_subscriptions")
.select("user_id")
.eq("user_id", userId);
if (!existingSubscriptions || existingSubscriptions.length === 0) {
// If there are no existing users with the provided userId and plan, insert a new row
const { error: insertError } = await supabase.from("user_subscriptions").insert({
user_id: userId,
plan: newPlan,
updated_at: new Date().toISOString(),
});
// If there was an error inserting the new subscription, throw an error
if (insertError) {
throw new Error(`Failed to insert user subscription: ${insertError.message}`);
}
} else {
// If the user already has a subscription, update their existing row
const { error: updateError } = await supabase
.from("user_subscriptions")
// Set the plan to the new plan and update the timestamp
.update({ plan: newPlan, updated_at: new Date().toISOString() })
.eq("user_id", userId);
// If there was an error updating the subscription, throw an error
if (updateError) {
throw new Error(`Failed to update user subscription: ${updateError.message}`);
}
}
// Return an object with the userId and newPlan
return {
userId,
newPlan,
};
},
});
```
<Note>
This task uses your service role secret key to bypass Row Level Security. There are different ways
of configuring your [RLS
policies](https://supabase.com/docs/guides/database/postgres/row-level-security), so always make
sure you have the correct permissions set up for your project.
</Note>
### Testing your task
To test this task in the [Trigger.dev dashboard](https://cloud.trigger.dev), you can use the following payload:
```json
{
"userId": "user_12345",
"newPlan": "pro"
}
```
If the task completes successfully, you will see a new row in your `user_subscriptions` table with the `user_id` set to `user_12345`, the `plan` set to `pro`, and the `updated_at` timestamp updated to the current time.
## Learn more about Supabase and Trigger.dev
### Full walkthrough guides from development to deployment
<CardGroup cols={2}>
<Card title="Edge function hello world guide" icon="book" href="/guides/frameworks/supabase-edge-functions-basic">
Learn how to trigger a task from a Supabase edge function when a URL is visited.
</Card>
<Card title="Database webhooks guide" icon="book" href="/guides/frameworks/supabase-edge-functions-database-webhooks">
Learn how to trigger a task from a Supabase edge function when an event occurs in your database.
</Card>
</CardGroup>
### Task examples with code you can copy and paste
<CardGroup cols={2}>
<Card title="Supabase database operations" icon="bolt" href="/guides/examples/supabase-database-operations">
Run basic CRUD operations on a table in a Supabase database using Trigger.dev.
</Card>
<Card title="Supabase Storage upload" icon="bolt" href="/guides/examples/supabase-storage-upload">
Download a video from a URL and upload it to Supabase Storage using S3.
</Card>
</CardGroup>
# Uploading files to Supabase Storage
Source: https://trigger.dev/docs/guides/examples/supabase-storage-upload
This example demonstrates how to upload files to Supabase Storage using Trigger.dev.
## Overview
This example shows how to upload a video file to Supabase Storage using two different methods.
* [Upload to Supabase Storage using the Supabase client](/guides/examples/supabase-storage-upload#example-1-upload-to-supabase-storage-using-the-supabase-storage-client)
* [Upload to Supabase Storage using the AWS S3 client](/guides/examples/supabase-storage-upload#example-2-upload-to-supabase-storage-using-the-aws-s3-client)
## Upload to Supabase Storage using the Supabase client
This task downloads a video from a provided URL and uploads it to Supabase Storage using the Supabase client.
### Task code
```ts trigger/supabase-storage-upload.ts
import { createClient } from "@supabase/supabase-js";
import { logger, task } from "@trigger.dev/sdk/v3";
import fetch from "node-fetch";
// Initialize Supabase client
const supabase = createClient(
process.env.SUPABASE_PROJECT_URL ?? "",
process.env.SUPABASE_SERVICE_ROLE_KEY ?? ""
);
export const supabaseStorageUpload = task({
id: "supabase-storage-upload",
run: async (payload: { videoUrl: string }) => {
const { videoUrl } = payload;
const bucket = "my_bucket"; // Replace "my_bucket" with your bucket name
const objectKey = `video_${Date.now()}.mp4`;
// Download video data as a buffer
const response = await fetch(videoUrl);
if (!response.ok) {
throw new Error(`HTTP error! status: ${response.status}`);
}
const videoBuffer = await response.buffer();
// Upload the video directly to Supabase Storage
const { error } = await supabase.storage.from(bucket).upload(objectKey, videoBuffer, {
contentType: "video/mp4",
upsert: true,
});
if (error) {
throw new Error(`Error uploading video: ${error.message}`);
}
logger.log(`Video uploaded to Supabase Storage bucket`, { objectKey });
// Return the video object key and bucket
return {
objectKey,
bucket: bucket,
};
},
});
```
### Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"videoUrl": "<a-video-url>" // Replace <a-video-url> with the URL of the video you want to upload
}
```
## Upload to Supabase Storage using the AWS S3 client
This task downloads a video from a provided URL, saves it to a temporary file, and then uploads the video file to Supabase Storage using the AWS S3 client.
### Key features
* Fetches a video from a provided URL
* Uploads the video file to Supabase Storage using S3
### Task code
```ts trigger/supabase-storage-upload-s3.ts
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { logger, task } from "@trigger.dev/sdk/v3";
import fetch from "node-fetch";
// Initialize S3 client for Supabase Storage
const s3Client = new S3Client({
region: process.env.SUPABASE_REGION, // Your Supabase project's region e.g. "us-east-1"
endpoint: `https://${process.env.SUPABASE_PROJECT_ID}.supabase.co/storage/v1/s3`,
credentials: {
// These credentials can be found in your supabase storage settings, under 'S3 access keys'
accessKeyId: process.env.SUPABASE_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.SUPABASE_SECRET_ACCESS_KEY ?? "",
},
});
export const supabaseStorageUploadS3 = task({
id: "supabase-storage-upload-s3",
run: async (payload: { videoUrl: string }) => {
const { videoUrl } = payload;
// Fetch the video as an ArrayBuffer
const response = await fetch(videoUrl);
const videoArrayBuffer = await response.arrayBuffer();
const videoBuffer = Buffer.from(videoArrayBuffer);
const bucket = "my_bucket"; // Replace "my_bucket" with your bucket name
const objectKey = `video_${Date.now()}.mp4`;
// Upload the video directly to Supabase Storage
await s3Client.send(
new PutObjectCommand({
Bucket: bucket,
Key: objectKey,
Body: videoBuffer,
})
);
logger.log(`Video uploaded to Supabase Storage bucket`, { objectKey });
// Return the video object key
return {
objectKey,
bucket: bucket,
};
},
});
```
### Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"videoUrl": "<a-video-url>" // Replace <a-video-url> with the URL of the video you want to upload
}
```
## Learn more about Supabase and Trigger.dev
### Full walkthrough guides from development to deployment
<CardGroup cols={2}>
<Card title="Edge function hello world guide" icon="book" href="/guides/frameworks/supabase-edge-functions-basic">
Learn how to trigger a task from a Supabase edge function when a URL is visited.
</Card>
<Card title="Database webhooks guide" icon="book" href="/guides/frameworks/supabase-edge-functions-database-webhooks">
Learn how to trigger a task from a Supabase edge function when an event occurs in your database.
</Card>
</CardGroup>
### Task examples with code you can copy and paste
<CardGroup cols={2}>
<Card title="Supabase database operations" icon="bolt" href="/guides/examples/supabase-database-operations">
Run basic CRUD operations on a table in a Supabase database using Trigger.dev.
</Card>
<Card title="Supabase Storage upload" icon="bolt" href="/guides/examples/supabase-storage-upload">
Download a video from a URL and upload it to Supabase Storage using S3.
</Card>
</CardGroup>
# Using the Vercel AI SDK
Source: https://trigger.dev/docs/guides/examples/vercel-ai-sdk
This example demonstrates how to use the Vercel AI SDK with Trigger.dev.
## Overview
The [Vercel AI SDK](https://www.npmjs.com/package/ai) is a simple way to use AI models from many different providers, including OpenAI, Microsoft Azure, Google Generative AI, Anthropic, Amazon Bedrock, Groq, Perplexity and [more](https://sdk.vercel.ai/providers/ai-sdk-providers).
It provides a consistent interface to interact with the different AI models, so you can easily switch between them without needing to change your code.
## Generate text using OpenAI
This task shows how to use the Vercel AI SDK to generate text from a prompt with OpenAI.
### Task code
```ts trigger/vercel-ai-sdk-openai.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import { generateText } from "ai";
// Install the package of the AI model you want to use, in this case OpenAI
import { openai } from "@ai-sdk/openai"; // Ensure OPENAI_API_KEY environment variable is set
export const openaiTask = task({
id: "openai-text-generate",
run: async (payload: { prompt: string }) => {
const chatCompletion = await generateText({
model: openai("gpt-4-turbo"),
// Add a system message which will be included with the prompt
system: "You are a friendly assistant!",
// The prompt passed in from the payload
prompt: payload.prompt,
});
// Log the generated text
logger.log("chatCompletion text:" + chatCompletion.text);
return chatCompletion;
},
});
```
## Testing your task
To test this task in the dashboard, you can use the following payload:
```json
{
"prompt": "What is the meaning of life?"
}
```
## Learn more about Next.js and Trigger.dev
### Walk-through guides from development to deployment
<CardGroup cols={2}>
<Card title="Next.js - setup guide" icon="N" href="/guides/frameworks/nextjs">
Learn how to setup Trigger.dev with Next.js, using either the pages or app router.
</Card>
<Card title="Next.js - triggering tasks using webhooks" icon="N" href="/guides/frameworks/nextjs-webhooks">
Learn how to create a webhook handler for incoming webhooks in a Next.js app, and trigger a task from it.
</Card>
</CardGroup>
### Task examples
<CardGroup cols={2}>
<Card title="Fal.ai with Realtime in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-realtime-thumbnail.png" href="/guides/examples/fal-ai-realtime">
Generate an image from a prompt using Fal.ai and Trigger.dev Realtime.
</Card>
<Card title="Generate a cartoon using Fal.ai in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-generate-cartoon-thumbnail.png" href="/guides/examples/fal-ai-image-to-cartoon">
Convert an image to a cartoon using Fal.ai.
</Card>
<Card title="Vercel sync environment variables" icon="code" href="/guides/examples/vercel-sync-env-vars">
Learn how to automatically sync environment variables from your Vercel projects to Trigger.dev.
</Card>
<Card title="Vercel AI SDK" icon="code" href="/guides/examples/vercel-ai-sdk">
Learn how to use the Vercel AI SDK, which is a simple way to use AI models from different
providers, including OpenAI, Anthropic, Amazon Bedrock, Groq, Perplexity etc.
</Card>
</CardGroup>
# Syncing environment variables from your Vercel projects
Source: https://trigger.dev/docs/guides/examples/vercel-sync-env-vars
This example demonstrates how to sync environment variables from your Vercel project to Trigger.dev.
## Build configuration
To sync environment variables, you just need to add our build extension to your `trigger.config.ts` file. This extension will then automatically run every time you deploy your Trigger.dev project.
<Note>
You need to set the `VERCEL_ACCESS_TOKEN` and `VERCEL_PROJECT_ID` environment variables, or pass
in the token and project ID as arguments to the `syncVercelEnvVars` build extension. If you're
working with a team project, you'll also need to set `VERCEL_TEAM_ID`, which can be found in your
team settings. You can find / generate the `VERCEL_ACCESS_TOKEN` in your Vercel
[dashboard](https://vercel.com/account/settings/tokens). Make sure the scope of the token covers
the project with the environment variables you want to sync.
</Note>
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncVercelEnvVars } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
// Add the syncVercelEnvVars build extension
extensions: [syncVercelEnvVars()],
},
});
```
<Note>
[Build extensions](/config/extensions/overview) allow you to hook into the build system and
customize the build process or the resulting bundle and container image (in the case of
deploying). You can use pre-built extensions or create your own.
</Note>
## Running the sync operation
To sync the environment variables, all you need to do is run our `deploy` command. You should see some output in the console indicating that the environment variables have been synced, and they should now be available in your Trigger.dev dashboard.
```bash
npx trigger.dev@latest deploy
```
## Learn more about Next.js and Trigger.dev
### Walk-through guides from development to deployment
<CardGroup cols={2}>
<Card title="Next.js - setup guide" icon="N" href="/guides/frameworks/nextjs">
Learn how to setup Trigger.dev with Next.js, using either the pages or app router.
</Card>
<Card title="Next.js - triggering tasks using webhooks" icon="N" href="/guides/frameworks/nextjs-webhooks">
Learn how to create a webhook handler for incoming webhooks in a Next.js app, and trigger a task from it.
</Card>
</CardGroup>
### Task examples
<CardGroup cols={2}>
<Card title="Fal.ai with Realtime in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-realtime-thumbnail.png" href="/guides/examples/fal-ai-realtime">
Generate an image from a prompt using Fal.ai and Trigger.dev Realtime.
</Card>
<Card title="Generate a cartoon using Fal.ai in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-generate-cartoon-thumbnail.png" href="/guides/examples/fal-ai-image-to-cartoon">
Convert an image to a cartoon using Fal.ai.
</Card>
<Card title="Vercel sync environment variables" icon="code" href="/guides/examples/vercel-sync-env-vars">
Learn how to automatically sync environment variables from your Vercel projects to Trigger.dev.
</Card>
<Card title="Vercel AI SDK" icon="code" href="/guides/examples/vercel-ai-sdk">
Learn how to use the Vercel AI SDK, which is a simple way to use AI models from different
providers, including OpenAI, Anthropic, Amazon Bedrock, Groq, Perplexity etc.
</Card>
</CardGroup>
# Bun guide
Source: https://trigger.dev/docs/guides/frameworks/bun
This guide will show you how to setup Trigger.dev with Bun
export const framework_0 = "Bun"
<Warning>A specific Bun version is currently required for the dev command to work. This is due to a [bug](https://github.com/oven-sh/bun/issues/13799) with IPC. Please use Bun version 1.1.24 or lower: `curl -fsSL https://bun.sh/install | bash -s -- bun-v1.1.24`</Warning>
We now have experimental support for Bun. This guide will show you have to setup Trigger.dev in your existing Bun project, test an example task, and view the run.
<Warn>
The trigger.dev CLI does not yet support Bun. So you will need to run the CLI using Node.js. But
Bun will still be used to execute your tasks, even in the `dev` environment.
</Warn>
## Prerequisites
* Setup a project in {framework_0}
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
## Initial setup
<Steps>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init --runtime bun
```
```bash pnpm
pnpm dlx trigger.dev@latest init --runtime bun
```
```bash yarn
yarn dlx trigger.dev@latest init --runtime bun
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/src/trigger` directory with an example task, `/src/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Update example.ts to use Bun">
Open the `/src/trigger/example.ts` file and replace the contents with the following:
```ts example.ts
import { Database } from "bun:sqlite";
import { task } from "@trigger.dev/sdk/v3";
export const bunTask = task({
id: "bun-task",
run: async (payload: { query: string }) => {
const db = new Database(":memory:");
const query = db.query("select 'Hello world' as message;");
console.log(query.get()); // => { message: "Hello world" }
return {
message: "Query executed",
};
},
});
```
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Known issues
* Certain OpenTelemetry instrumentation will not work with Bun, because Bun does not support Node's `register` hook. This means that some libraries that rely on this hook will not work with Bun.
# Drizzle setup guide
Source: https://trigger.dev/docs/guides/frameworks/drizzle
This guide will show you how to set up Drizzle ORM with Trigger.dev
## Overview
This guide will show you how to set up [Drizzle ORM](https://orm.drizzle.team/) with Trigger.dev, test and view an example task run.
## Prerequisites
* An existing Node.js project with a `package.json` file
* Ensure TypeScript is installed
* A [PostgreSQL](https://www.postgresql.org/) database server running locally, or accessible via a connection string
* Drizzle ORM [installed and initialized](https://orm.drizzle.team/docs/get-started) in your project
* A `DATABASE_URL` environment variable set in your `.env` file, pointing to your PostgreSQL database (e.g. `postgresql://user:password@localhost:5432/dbname`)
## Initial setup (optional)
Follow these steps if you don't already have Trigger.dev set up in your project.
<Steps>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Creating a task using Drizzle and deploying it to production
<Steps>
<Step title="The task using Drizzle">
First, create a new task file in your `trigger` folder.
This is a simple task that will add a new user to your database, we will call it `drizzle-add-new-user`.
<Note>
For this task to work correctly, you will need to have a `users` table schema defined with Drizzle
that includes `name`, `age` and `email` fields.
</Note>
```ts /trigger/drizzle-add-new-user.ts
import { eq } from "drizzle-orm";
import { task } from "@trigger.dev/sdk/v3";
import { users } from "src/db/schema";
import { drizzle } from "drizzle-orm/node-postgres";
// Initialize Drizzle client
const db = drizzle(process.env.DATABASE_URL!);
export const addNewUser = task({
id: "drizzle-add-new-user",
run: async (payload: typeof users.$inferInsert) => {
// Create new user
const [user] = await db.insert(users).values(payload).returning();
return {
createdUser: user,
message: "User created and updated successfully",
};
},
});
```
</Step>
<Step title="Configuring the build">
Next, in your `trigger.config.js` file, add `pg` to the `externals` array. `pg` is a non-blocking PostgreSQL client for Node.js.
It is marked as an external to ensure that it is not bundled into the task's bundle, and instead will be installed and loaded from `node_modules` at runtime.
```js /trigger.config.js
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>", // Your project reference
// Your other config settings...
build: {
externals: ["pg"],
},
});
```
</Step>
<Step title="Deploying your task">
Once the build configuration is added, you can now deploy your task using the Trigger.dev CLI.
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
</Step>
<Step title="Adding your DATABASE_URL environment variable to Trigger.dev">
In your Trigger.dev dashboard sidebar click "Environment Variables" <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />, and then the "New environment variable" button <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" />.
![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-page.jpg)
You can add values for your local dev environment, staging and prod. in this case we will add the `DATABASE_URL` for the production environment.
![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-panel.jpg)
</Step>
<Step title="Running your task">
To test this task, go to the 'test' page in the Trigger.dev dashboard and run the task with the following payload:
```json
{
"name": "<a-name>", // e.g. "John Doe"
"age": "<an-age>", // e.g. 25
"email": "<an-email>" // e.g. "john@doe.test"
}
```
Congratulations! You should now see a new completed run, and a new user with the credentials you provided should be added to your database.
</Step>
</Steps>
## Useful next steps
<CardGroup cols={2}>
<Card title="Tasks overview" icon="diagram-subtask" href="/tasks/overview">
Learn what tasks are and their options
</Card>
<Card title="Writing tasks" icon="pen-nib" href="/writing-tasks-introduction">
Learn how to write your own tasks
</Card>
<Card title="Deploy using the CLI" icon="terminal" href="/cli-deploy">
Learn how to deploy your task manually using the CLI
</Card>
<Card title="Deploy using GitHub actions" icon="github" href="/github-actions">
Learn how to deploy your task using GitHub actions
</Card>
</CardGroup>
# Next.js setup guide
Source: https://trigger.dev/docs/guides/frameworks/nextjs
This guide will show you how to setup Trigger.dev in your existing Next.js project, test an example task, and view the run.
export const framework_0 = "Next.js"
<Note>This guide can be followed for both App and Pages router as well as Server Actions.</Note>
## Prerequisites
* Setup a project in {framework_0}
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
## Initial setup
<Steps>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
<Tip>
Instead of running your Next.js app and Trigger.dev dev server in separate terminals, you can run them concurrently. First, add these scripts to your `package.json`:
```json
{
"scripts": {
"trigger:dev": "npx trigger.dev@latest dev",
"dev": "npx concurrently --kill-others --names \"next,trigger\" --prefix-colors \"yellow,blue\" \"next dev\" \"npm run trigger:dev\""
}
}
```
Then, in your terminal, you can start both servers with a single command:
```bash
npm run dev
```
This will run both your Next.js app and Trigger.dev dev server in the same terminal window, with color-coded output to distinguish between them.
</Tip>
## Set your secret key locally
Set your `TRIGGER_SECRET_KEY` environment variable in your `.env.local` file if using the Next.js App router or `.env` file if using Pages router. This key is used to authenticate with Trigger.dev, so you can trigger runs from your Next.js app. Visit the API Keys page in the dashboard and select the DEV secret key.
![How to find your secret key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/api-keys.png)
For more information on authenticating with Trigger.dev, see the [API keys page](/apikeys).
## Triggering your task in Next.js
Here are the steps to trigger your task in the Next.js App and Pages router and Server Actions. Alternatively, check out this repo for a [full working example](https://github.com/triggerdotdev/example-projects/tree/main/nextjs/server-actions/my-app) of a Next.js app with a Trigger.dev task triggered using a Server Action.
<Tabs>
<Tab title="App Router">
<Steps>
<Step title="Create a Route Handler">
Add a Route Handler by creating a `route.ts` file (or `route.js` file) in the `app/api` directory like this: `app/api/hello-world/route.ts`.
</Step>
<Step title="Add your task">
Add this code to your `route.ts` file which imports your task along with `NextResponse` to handle the API route response:
```ts app/api/hello-world/route.ts
// Next.js API route support: https://nextjs.org/docs/api-routes/introduction
import type { helloWorldTask } from "@/trigger/example";
import { tasks } from "@trigger.dev/sdk/v3";
import { NextResponse } from "next/server";
//tasks.trigger also works with the edge runtime
//export const runtime = "edge";
export async function GET() {
const handle = await tasks.trigger<typeof helloWorldTask>(
"hello-world",
"James"
);
return NextResponse.json(handle);
}
```
</Step>
<Step title="Trigger your task">
Run your Next.js app:
<CodeGroup>
```bash npm
npm run dev
```
```bash pnpm
pnpm run dev
```
```bash yarn
yarn dev
```
</CodeGroup>
Run the dev server from Step 2. of the [Initial Setup](/guides/frameworks/nextjs#initial-setup) section above if it's not already running:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
Now visit the URL in your browser to trigger the task. Ensure the port number is the same as the one you're running your Next.js app on. For example, if you're running your Next.js app on port 3000, visit:
```bash
http://localhost:3000/api/hello-world
```
You should see the CLI log the task run with a link to view the logs in the dashboard.
![Trigger.dev CLI showing a successful run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/trigger-cli-run-success.png)
Visit the [Trigger.dev dashboard](https://cloud.trigger.dev) to see your run.
</Step>
</Steps>
</Tab>
<Tab title="App Router (Server Actions)">
<Steps>
<Step title="Create an `actions.ts` file">
Create an `actions.ts` file in the `app/api` directory and add this code which imports your `helloWorldTask()` task. Make sure to include `"use server";` at the top of the file.
```ts app/api/actions.ts
"use server";
import type { helloWorldTask } from "@/trigger/example";
import { tasks } from "@trigger.dev/sdk/v3";
export async function myTask() {
try {
const handle = await tasks.trigger<typeof helloWorldTask>(
"hello-world",
"James"
);
return { handle };
} catch (error) {
console.error(error);
return {
error: "something went wrong",
};
}
}
```
</Step>
<Step title="Create a button to trigger your task">
For the purposes of this guide, we'll create a button with an `onClick` event that triggers your task. We'll add this to the `page.tsx` file so we can trigger the task by clicking the button. Make sure to import your task and include `"use client";` at the top of your file.
```ts app/page.tsx
"use client";
import { myTask } from "./actions";
export default function Home() {
return (
<main className="flex min-h-screen flex-col items-center justify-center p-24">
<button
onClick={async () => {
await myTask();
}}
>
Trigger my task
</button>
</main>
);
}
```
</Step>
<Step title="Trigger your task">
Run your Next.js app:
<CodeGroup>
```bash npm
npm run dev
```
```bash pnpm
pnpm run dev
```
```bash yarn
yarn dev
```
</CodeGroup>
Open your app in a browser, making sure the port number is the same as the one you're running your Next.js app on. For example, if you're running your Next.js app on port 3000, visit:
```bash
http://localhost:3000
```
Run the dev server from Step 2. of the [Initial Setup](/guides/frameworks/nextjs#initial-setup) section above if it's not already running:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
Then click the button we created in your app to trigger the task. You should see the CLI log the task run with a link to view the logs.
![Trigger.dev CLI showing a successful run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/trigger-cli-run-success.png)
Visit the [Trigger.dev dashboard](https://cloud.trigger.dev) to see your run.
</Step>
</Steps>
</Tab>
<Tab title="Pages Router">
<Steps>
<Step title="Create an API route">
Create an API route in the `pages/api` directory. Then create a `hello-world .ts` (or `hello-world.js`) file for your task and copy this code example:
```ts pages/api/hello-world.ts
// Next.js API route support: https://nextjs.org/docs/api-routes/introduction
import { helloWorldTask } from "@/trigger/example";
import { tasks } from "@trigger.dev/sdk/v3";
import type { NextApiRequest, NextApiResponse } from "next";
export default async function handler(
req: NextApiRequest,
res: NextApiResponse<{ id: string }>
) {
const handle = await tasks.trigger<typeof helloWorldTask>(
"hello-world",
"James"
);
res.status(200).json(handle);
}
```
</Step>
<Step title="Trigger your task">
Run your Next.js app:
<CodeGroup>
```bash npm
npm run dev
```
```bash pnpm
pnpm run dev
```
```bash yarn
yarn dev
```
</CodeGroup>
Run the dev server from Step 2. of the [Initial Setup](/guides/frameworks/nextjs#initial-setup) section above if it's not already running:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
Now visit the URL in your browser to trigger the task. Ensure the port number is the same as the one you're running your Next.js app on. For example, if you're running your Next.js app on port 3000, visit:
```bash
http://localhost:3000/api/hello-world
```
You should see the CLI log the task run with a link to view the logs in the dashboard.
![Trigger.dev CLI showing a successful run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/trigger-cli-run-success.png)
Visit the [Trigger.dev dashboard](https://cloud.trigger.dev) to see your run.
</Step>
</Steps>
</Tab>
</Tabs>
## Automatically sync environment variables from your Vercel project (optional)
If you want to automatically sync environment variables from your Vercel project to Trigger.dev, you can add our `syncVercelEnvVars` build extension to your `trigger.config.ts` file.
<Note>
You need to set the `VERCEL_ACCESS_TOKEN` and `VERCEL_PROJECT_ID` environment variables, or pass
in the token and project ID as arguments to the `syncVercelEnvVars` build extension. If you're
working with a team project, you'll also need to set `VERCEL_TEAM_ID`, which can be found in your
team settings. You can find / generate the `VERCEL_ACCESS_TOKEN` in your Vercel
[dashboard](https://vercel.com/account/settings/tokens). Make sure the scope of the token covers
the project with the environment variables you want to sync.
</Note>
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncVercelEnvVars } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [syncVercelEnvVars()],
},
});
```
<Note>
For more information, see our [Vercel sync environment
variables](/guides/examples/vercel-sync-env-vars) guide.
</Note>
## Manually add your environment variables (optional)
If you have any environment variables in your tasks, be sure to add them in the dashboard so deployed code runs successfully. In Node.js, these environment variables are accessed in your code using `process.env.MY_ENV_VAR`.
In the sidebar select the "Environment Variables" page, then press the "New environment variable"
button. ![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-page.jpg)
You can add values for your local dev environment, staging and prod. ![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-panel.jpg)
You can also add environment variables in code by following the steps on the [Environment Variables page](/deploy-environment-variables#in-your-code).
## Deploying your task to Trigger.dev
For this guide, we'll manually deploy your task by running the [CLI deploy command](/cli-deploy) below. Other ways to deploy are listed in the next section.
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
### Other ways to deploy
<Tabs>
<Tab title="GitHub Actions">
Use GitHub Actions to automatically deploy your tasks whenever new code is pushed and when the `trigger` directory has changes in it. Follow [this guide](/github-actions) to set up GitHub Actions.
</Tab>
<Tab title="Vercel Integration">
We're working on adding an official [Vercel integration](/vercel-integration) which you can follow the progress of [here](https://feedback.trigger.dev/p/vercel-integration-3).
</Tab>
</Tabs>
## Troubleshooting & extra resources
### Revalidation from your Trigger.dev tasks
[Revalidation](https://vercel.com/docs/incremental-static-regeneration/quickstart#on-demand-revalidation) allows you to purge the cache for an ISR route. To revalidate an ISR route from a Trigger.dev task, you have to set up a handler for the `revalidate` event. This is an API route that you can add to your Next.js app.
This handler will run the `revalidatePath` function from Next.js, which purges the cache for the given path.
The handlers are slightly different for the App and Pages router:
#### Revalidation handler: App Router
If you are using the App router, create a new revalidation route at `app/api/revalidate/path/route.ts`:
```ts app/api/revalidate/path/route.ts
import { NextRequest, NextResponse } from "next/server";
import { revalidatePath } from "next/cache";
export async function POST(request: NextRequest) {
try {
const { path, type, secret } = await request.json();
// Create a REVALIDATION_SECRET and set it in your environment variables
if (secret !== process.env.REVALIDATION_SECRET) {
return NextResponse.json({ message: "Invalid secret" }, { status: 401 });
}
if (!path) {
return NextResponse.json({ message: "Path is required" }, { status: 400 });
}
revalidatePath(path, type);
return NextResponse.json({ revalidated: true });
} catch (err) {
console.error("Error revalidating path:", err);
return NextResponse.json({ message: "Error revalidating path" }, { status: 500 });
}
}
```
#### Revalidation handler: Pages Router
If you are using the Pages router, create a new revalidation route at `pages/api/revalidate/path.ts`:
```ts pages/api/revalidate/path.ts
import type { NextApiRequest, NextApiResponse } from "next";
export default async function handler(req: NextApiRequest, res: NextApiResponse) {
try {
if (req.method !== "POST") {
return res.status(405).json({ message: "Method not allowed" });
}
const { path, secret } = req.body;
if (secret !== process.env.REVALIDATION_SECRET) {
return res.status(401).json({ message: "Invalid secret" });
}
if (!path) {
return res.status(400).json({ message: "Path is required" });
}
await res.revalidate(path);
return res.json({ revalidated: true });
} catch (err) {
console.error("Error revalidating path:", err);
return res.status(500).json({ message: "Error revalidating path" });
}
}
```
#### Revalidation task
This task takes a `path` as a payload and will revalidate the path you specify, using the handler you set up previously.
<Note>
To run this task locally you will need to set the `REVALIDATION_SECRET` environment variable in your `.env.local` file (or `.env` file if using Pages router).
To run this task in production, you will need to set the `REVALIDATION_SECRET` environment variable in Vercel, in your project settings, and also in your environment variables in the Trigger.dev dashboard.
</Note>
```ts trigger/revalidate-path.ts
import { logger, task } from "@trigger.dev/sdk/v3";
const NEXTJS_APP_URL = process.env.NEXTJS_APP_URL; // e.g. "http://localhost:3000" or "https://my-nextjs-app.vercel.app"
const REVALIDATION_SECRET = process.env.REVALIDATION_SECRET; // Create a REVALIDATION_SECRET and set it in your environment variables
export const revalidatePath = task({
id: "revalidate-path",
run: async (payload: { path: string }) => {
const { path } = payload;
try {
const response = await fetch(`${NEXTJS_APP_URL}/api/revalidate/path`, {
method: "POST",
headers: {
"Content-Type": "application/json",
},
body: JSON.stringify({
path: `${NEXTJS_APP_URL}/${path}`,
secret: REVALIDATION_SECRET,
}),
});
if (response.ok) {
logger.log("Path revalidation successful", { path });
return { success: true };
} else {
logger.error("Path revalidation failed", {
path,
statusCode: response.status,
statusText: response.statusText,
});
return {
success: false,
error: `Revalidation failed with status ${response.status}: ${response.statusText}`,
};
}
} catch (error) {
logger.error("Path revalidation encountered an error", {
path,
error: error instanceof Error ? error.message : String(error),
});
return {
success: false,
error: `Failed to revalidate path due to an unexpected error`,
};
}
},
});
```
#### Testing the revalidation task
You can test your revalidation task in the Trigger.dev dashboard on the testing page, using the following payload.
```json
{
"path": "<path-to-revalidate>" // e.g. "blog"
}
```
### Next.js build failing due to missing API key in GitHub CI
This issue occurs during the Next.js app build process on GitHub CI where the Trigger.dev SDK is expecting the TRIGGER\_SECRET\_KEY environment variable to be set at build time. Next.js attempts to compile routes and creates static pages, which can cause issues with SDKs that require runtime environment variables. The solution is to mark the relevant pages as dynamic to prevent Next.js from trying to make them static. You can do this by adding the following line to the route file:
```ts
export const dynamic = "force-dynamic";
```
### Correctly passing event handlers to React components
An issue can sometimes arise when you try to pass a function directly to the `onClick` prop. This is because the function may require specific arguments or context that are not available when the event occurs. By wrapping the function call in an arrow function, you ensure that the handler is called with the correct context and any necessary arguments. For example:
This works:
```tsx
<Button onClick={() => myTask()}>Trigger my task</Button>
```
Whereas this does not work:
```tsx
<Button onClick={myTask}>Trigger my task</Button>
```
<WorkerFailedToStartWhenRunningDevCommand />
## Learn more about Next.js and Trigger.dev
### Walk-through guides from development to deployment
<CardGroup cols={2}>
<Card title="Next.js - setup guide" icon="N" href="/guides/frameworks/nextjs">
Learn how to setup Trigger.dev with Next.js, using either the pages or app router.
</Card>
<Card title="Next.js - triggering tasks using webhooks" icon="N" href="/guides/frameworks/nextjs-webhooks">
Learn how to create a webhook handler for incoming webhooks in a Next.js app, and trigger a task from it.
</Card>
</CardGroup>
### Task examples
<CardGroup cols={2}>
<Card title="Fal.ai with Realtime in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-realtime-thumbnail.png" href="/guides/examples/fal-ai-realtime">
Generate an image from a prompt using Fal.ai and Trigger.dev Realtime.
</Card>
<Card title="Generate a cartoon using Fal.ai in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-generate-cartoon-thumbnail.png" href="/guides/examples/fal-ai-image-to-cartoon">
Convert an image to a cartoon using Fal.ai.
</Card>
<Card title="Vercel sync environment variables" icon="code" href="/guides/examples/vercel-sync-env-vars">
Learn how to automatically sync environment variables from your Vercel projects to Trigger.dev.
</Card>
<Card title="Vercel AI SDK" icon="code" href="/guides/examples/vercel-ai-sdk">
Learn how to use the Vercel AI SDK, which is a simple way to use AI models from different
providers, including OpenAI, Anthropic, Amazon Bedrock, Groq, Perplexity etc.
</Card>
</CardGroup>
## Useful next steps
<CardGroup cols={2}>
<Card title="Tasks overview" icon="diagram-subtask" href="/tasks/overview">
Learn what tasks are and their options
</Card>
<Card title="Writing tasks" icon="pen-nib" href="/writing-tasks-introduction">
Learn how to write your own tasks
</Card>
<Card title="Deploy using the CLI" icon="terminal" href="/cli-deploy">
Learn how to deploy your task manually using the CLI
</Card>
<Card title="Deploy using GitHub actions" icon="github" href="/github-actions">
Learn how to deploy your task using GitHub actions
</Card>
</CardGroup>
# Triggering tasks with webhooks in Next.js
Source: https://trigger.dev/docs/guides/frameworks/nextjs-webhooks
Learn how to trigger a task from a webhook in a Next.js app.
## Prerequisites
* [A Next.js project, set up with Trigger.dev](/guides/frameworks/nextjs)
* [cURL](https://curl.se/) installed on your local machine. This will be used to send a POST request to your webhook handler.
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/nextjs-webhooks/my-app">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Adding the webhook handler
The webhook handler in this guide will be an API route.
This will be different depending on whether you are using the Next.js pages router or the app router.
### Pages router: creating the webhook handler
Create a new file `pages/api/webhook-handler.ts` or `pages/api/webhook-hander.js`.
In your new file, add the following code:
```ts /pages/api/webhook-handler.ts
import { helloWorldTask } from "@/trigger/example";
import { tasks } from "@trigger.dev/sdk/v3";
import type { NextApiRequest, NextApiResponse } from "next";
export default async function handler(req: NextApiRequest, res: NextApiResponse) {
// Parse the webhook payload
const payload = req.body;
// Trigger the helloWorldTask with the webhook data as the payload
await tasks.trigger<typeof helloWorldTask>("hello-world", payload);
res.status(200).json({ message: "OK" });
}
```
This code will handle the webhook payload and trigger the 'Hello World' task.
### App router: creating the webhook handler
Create a new file in the `app/api/webhook-handler/route.ts` or `app/api/webhook-handler/route.js`.
In your new file, add the following code:
```ts /app/api/webhook-handler/route.ts
import type { helloWorldTask } from "@/trigger/example";
import { tasks } from "@trigger.dev/sdk/v3";
import { NextResponse } from "next/server";
export async function POST(req: Request) {
// Parse the webhook payload
const payload = await req.json();
// Trigger the helloWorldTask with the webhook data as the payload
await tasks.trigger<typeof helloWorldTask>("hello-world", payload);
return NextResponse.json("OK", { status: 200 });
}
```
This code will handle the webhook payload and trigger the 'Hello World' task.
## Triggering the task locally
Now that you have your webhook handler set up, you can trigger the 'Hello World' task from it. We will do this locally using cURL.
<Steps>
<Step title="Run your Next.js app and the Trigger.dev dev server">
First, run your Next.js app.
<CodeGroup>
```bash npm
npm run dev
```
```bash pnpm
pnpm run dev
```
```bash yarn
yarn dev
```
</CodeGroup>
Then, open up a second terminal window and start the Trigger.dev dev server:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Trigger the webhook with some dummy data">
To send a POST request to your webhook handler, open up a terminal window on your local machine and run the following command:
<Tip>
If `http://localhost:3000` isn't the URL of your locally running Next.js app, replace the URL in
the below command with that URL instead.
</Tip>
```bash
curl -X POST -H "Content-Type: application/json" -d '{"Name": "John Doe", "Age": "87"}' http://localhost:3000/api/webhook-handler
```
This will send a POST request to your webhook handler, with a JSON payload.
</Step>
<Step title="Check the task ran successfully">
After running the command, you should see a successful dev run and a 200 response in your terminals.
If you now go to your [Trigger.dev dashboard](https://cloud.trigger.dev), you should also see a successful run for the 'Hello World' task, with the payload you sent, in this case; `{"name": "John Doe", "age": "87"}`.
</Step>
</Steps>
## Learn more about Next.js and Trigger.dev
### Walk-through guides from development to deployment
<CardGroup cols={2}>
<Card title="Next.js - setup guide" icon="N" href="/guides/frameworks/nextjs">
Learn how to setup Trigger.dev with Next.js, using either the pages or app router.
</Card>
<Card title="Next.js - triggering tasks using webhooks" icon="N" href="/guides/frameworks/nextjs-webhooks">
Learn how to create a webhook handler for incoming webhooks in a Next.js app, and trigger a task from it.
</Card>
</CardGroup>
### Task examples
<CardGroup cols={2}>
<Card title="Fal.ai with Realtime in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-realtime-thumbnail.png" href="/guides/examples/fal-ai-realtime">
Generate an image from a prompt using Fal.ai and Trigger.dev Realtime.
</Card>
<Card title="Generate a cartoon using Fal.ai in Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/fal-generate-cartoon-thumbnail.png" href="/guides/examples/fal-ai-image-to-cartoon">
Convert an image to a cartoon using Fal.ai.
</Card>
<Card title="Vercel sync environment variables" icon="code" href="/guides/examples/vercel-sync-env-vars">
Learn how to automatically sync environment variables from your Vercel projects to Trigger.dev.
</Card>
<Card title="Vercel AI SDK" icon="code" href="/guides/examples/vercel-ai-sdk">
Learn how to use the Vercel AI SDK, which is a simple way to use AI models from different
providers, including OpenAI, Anthropic, Amazon Bedrock, Groq, Perplexity etc.
</Card>
</CardGroup>
# Node.js setup guide
Source: https://trigger.dev/docs/guides/frameworks/nodejs
This guide will show you how to setup Trigger.dev in your existing Node.js project, test an example task, and view the run.
export const framework_0 = "Node.js"
## Prerequisites
* Setup a project in {framework_0}
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
## Initial setup
<Steps>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Useful next steps
<CardGroup cols={2}>
<Card title="Tasks overview" icon="diagram-subtask" href="/tasks/overview">
Learn what tasks are and their options
</Card>
<Card title="Writing tasks" icon="pen-nib" href="/writing-tasks-introduction">
Learn how to write your own tasks
</Card>
<Card title="Deploy using the CLI" icon="terminal" href="/cli-deploy">
Learn how to deploy your task manually using the CLI
</Card>
<Card title="Deploy using GitHub actions" icon="github" href="/github-actions">
Learn how to deploy your task using GitHub actions
</Card>
</CardGroup>
# Prisma setup guide
Source: https://trigger.dev/docs/guides/frameworks/prisma
This guide will show you how to set up Prisma with Trigger.dev
## Overview
This guide will show you how to set up [Prisma](https://www.prisma.io/) with Trigger.dev, test and view an example task run.
## Prerequisites
* An existing Node.js project with a `package.json` file
* Ensure TypeScript is installed
* A [PostgreSQL](https://www.postgresql.org/) database server running locally, or accessible via a connection string
* Prisma ORM [installed and initialized](https://www.prisma.io/docs/getting-started/quickstart) in your project
* A `DATABASE_URL` environment variable set in your `.env` file, pointing to your PostgreSQL database (e.g. `postgresql://user:password@localhost:5432/dbname`)
## Initial setup (optional)
Follow these steps if you don't already have Trigger.dev set up in your project.
<Steps>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Creating a task using Prisma and deploying it to production
<Steps>
<Step title="Writing the Prisma task">
First, create a new task file in your `trigger` folder.
This is a simple task that will add a new user to the database.
<Note>
For this task to work correctly, you will need to have a `user` model in your Prisma schema with
an `id` field, a `name` field, and an `email` field.
</Note>
```ts /trigger/prisma-add-new-user.ts
import { PrismaClient } from "@prisma/client";
import { task } from "@trigger.dev/sdk/v3";
// Initialize Prisma client
const prisma = new PrismaClient();
export const addNewUser = task({
id: "prisma-add-new-user",
run: async (payload: { name: string; email: string; id: number }) => {
const { name, email, id } = payload;
// This will create a new user in the database
const user = await prisma.user.create({
data: {
name: name,
email: email,
id: id,
},
});
return {
message: `New user added successfully: ${user.id}`,
};
},
});
```
</Step>
<Step title="Configuring the build extension">
Next, configure the Prisma [build extension](https://trigger.dev/docs/config/extensions/overview) in the `trigger.config.js` file to include the Prisma client in the build.
This will ensure that the Prisma client is available when the task runs.
For a full list of options available in the Prisma build extension, see the [Prisma build extension documentation](https://trigger.dev/docs/config/extensions/prismaExtension).
```js /trigger.config.js
export default defineConfig({
project: "<project ref>", // Your project reference
// Your other config settings...
build: {
extensions: [
prismaExtension({
version: "5.20.0", // optional, we'll automatically detect the version if not provided
// update this to the path of your Prisma schema file
schema: "prisma/schema.prisma",
}),
],
},
});
```
<Note>
[Build extensions](/config/extensions/overview) allow you to hook into the build system and
customize the build process or the resulting bundle and container image (in the case of
deploying). You can use pre-built extensions or create your own.
</Note>
</Step>
<Step title="Optional: adding Prisma instrumentation">
We use OpenTelemetry to [instrument](https://trigger.dev/docs/config/config-file#instrumentations) our tasks and collect telemetry data.
If you want to automatically log all Prisma queries and mutations, you can use the Prisma instrumentation extension.
```js /trigger.config.js
import { defineConfig } from "@trigger.dev/sdk/v3";
import { PrismaInstrumentation } from "@prisma/instrumentation";
import { OpenAIInstrumentation } from "@traceloop/instrumentation-openai";
export default defineConfig({
//..other stuff
instrumentations: [new PrismaInstrumentation(), new OpenAIInstrumentation()],
});
```
This provides much more detailed information about your tasks with minimal effort.
</Step>
<Step title="Deploying your task">
With the build extension and task configured, you can now deploy your task using the Trigger.dev CLI.
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
</Step>
<Step title="Adding your DATABASE_URL environment variable to Trigger.dev">
In your Trigger.dev dashboard sidebar click "Environment Variables" <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />, and then the "New environment variable" button <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" />.
You can add values for your local dev environment, staging and prod. in this case we will add the `DATABASE_URL` for the production environment.
![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-panel.jpg)
</Step>
<Step title="Running your task">
To test this task, go to the 'test' page in the Trigger.dev dashboard and run the task with the following payload:
```json
{
"name": "<a-name>", // e.g. "John Doe"
"email": "<a-email>", // e.g. "john@doe.test"
"id": <a-number> // e.g. 12345
}
```
Congratulations! You should now see a new completed run, and a new user with the credentials you provided should be added to your database.
</Step>
</Steps>
## Useful next steps
<CardGroup cols={2}>
<Card title="Tasks overview" icon="diagram-subtask" href="/tasks/overview">
Learn what tasks are and their options
</Card>
<Card title="Writing tasks" icon="pen-nib" href="/writing-tasks-introduction">
Learn how to write your own tasks
</Card>
<Card title="Deploy using the CLI" icon="terminal" href="/cli-deploy">
Learn how to deploy your task manually using the CLI
</Card>
<Card title="Deploy using GitHub actions" icon="github" href="/github-actions">
Learn how to deploy your task using GitHub actions
</Card>
</CardGroup>
# Remix setup guide
Source: https://trigger.dev/docs/guides/frameworks/remix
This guide will show you how to setup Trigger.dev in your existing Remix project, test an example task, and view the run.
export const framework_0 = "Remix"
## Prerequisites
* Setup a project in {framework_0}
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
## Initial setup
<Steps>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Set your secret key locally
Set your `TRIGGER_SECRET_KEY` environment variable in your `.env` file. This key is used to authenticate with Trigger.dev, so you can trigger runs from your Remix app. Visit the API Keys page in the dashboard and select the DEV secret key.
![How to find your secret key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/api-keys.png)
For more information on authenticating with Trigger.dev, see the [API keys page](/apikeys).
## Triggering your task in Remix
<Steps>
<Step title="Create an API route">
Create a new file called `api.hello-world.ts` (or `api.hello-world.js`) in the `app/routes` directory like this: `app/routes/api.hello-world.ts`.
</Step>
<Step title="Add your task">
Add this code to your `api.hello-world.ts` file which imports your task:
```ts app/routes/api.hello-world.ts
import type { helloWorldTask } from "../../src/trigger/example";
import { tasks } from "@trigger.dev/sdk/v3";
export async function loader() {
const handle = await tasks.trigger<typeof helloWorldTask>("hello-world", "James");
return new Response(JSON.stringify(handle), {
headers: { "Content-Type": "application/json" },
});
}
```
</Step>
<Step title="Trigger your task">
Run your Remix app:
<CodeGroup>
```bash npm
npm run dev
```
```bash pnpm
pnpm run dev
```
```bash yarn
yarn dev
```
</CodeGroup>
Run the dev server from Step 2. of the [Initial Setup](/guides/frameworks/remix#initial-setup) section above if it's not already running:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
Now visit the URL in your browser to trigger the task. Ensure the port number is the same as the one you're running your Remix app on. For example, if you're running your Remix app on port 3000, visit:
```bash
http://localhost:3000/api/trigger
```
You should see the CLI log the task run with a link to view the logs in the dashboard.
![Trigger.dev CLI showing a successful run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/trigger-cli-run-success.png)
Visit the [Trigger.dev dashboard](https://cloud.trigger.dev) to see your run.
</Step>
</Steps>
## Manually add your environment variables (optional)
If you have any environment variables in your tasks, be sure to add them in the dashboard so deployed code runs successfully. In Node.js, these environment variables are accessed in your code using `process.env.MY_ENV_VAR`.
In the sidebar select the "Environment Variables" page, then press the "New environment variable"
button. ![Environment variables page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-page.jpg)
You can add values for your local dev environment, staging and prod. ![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-panel.jpg)
You can also add environment variables in code by following the steps on the [Environment Variables page](/deploy-environment-variables#in-your-code).
## Deploying your task to Trigger.dev
For this guide, we'll manually deploy your task by running the [CLI deploy command](/cli-deploy) below. Other ways to deploy are listed in the next section.
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
### Other ways to deploy
<Tabs>
<Tab title="GitHub Actions">
Use GitHub Actions to automatically deploy your tasks whenever new code is pushed and when the `trigger` directory has changes in it. Follow [this guide](/github-actions) to set up GitHub Actions.
</Tab>
<Tab title="Vercel Integration">
We're working on adding an official [Vercel integration](/vercel-integration) which you can follow the progress of [here](https://feedback.trigger.dev/p/vercel-integration-3).
</Tab>
</Tabs>
## Deploying to Vercel Edge Functions
Before we start, it's important to note that:
* We'll be using a type-only import for the task to ensure compatibility with the edge runtime.
* The `@trigger.dev/sdk/v3` package supports the edge runtime out of the box.
There are a few extra steps to follow to deploy your `/api/hello-world` API endpoint to Vercel Edge Functions.
<Steps>
<Step title="Update your API route">
Update your API route to use the `runtime: "edge"` option and change it to an `action()` so we can trigger the task from a curl request later on.
```ts app/routes/api.hello-world.ts
import { tasks } from "@trigger.dev/sdk/v3";
import type { helloWorldTask } from "../../src/trigger/example";
//      ðŸ‘† **type-only** import
// include this at the top of your API route file
export const config = {
runtime: "edge",
};
export async function action({ request }: { request: Request }) {
// This is where you'd authenticate the request
const payload = await request.json();
const handle = await tasks.trigger<typeof helloWorldTask>("hello-world", payload);
return new Response(JSON.stringify(handle), {
headers: { "Content-Type": "application/json" },
});
}
```
</Step>
<Step title="Update the Vercel configuration">
Create or update the `vercel.json` file with the following:
```json vercel.json
{
"buildCommand": "npm run vercel-build",
"devCommand": "npm run dev",
"framework": "remix",
"installCommand": "npm install",
"outputDirectory": "build/client"
}
```
</Step>
<Step title="Update package.json scripts">
Update your `package.json` to include the following scripts:
```json package.json
"scripts": {
"build": "remix vite:build",
"dev": "remix vite:dev",
"lint": "eslint --ignore-path .gitignore --cache --cache-location ./node_modules/.cache/eslint .",
"start": "remix-serve ./build/server/index.js",
"typecheck": "tsc",
"vercel-build": "remix vite:build && cp -r ./public ./build/client"
},
```
</Step>
<Step title="Deploy to Vercel">
Push your code to a Git repository and create a new project in the Vercel dashboard. Select your repository and follow the prompts to complete the deployment.
</Step>
<Step title="Add your Vercel environment variables">
In the Vercel project settings, add your Trigger.dev secret key:
```bash
TRIGGER_SECRET_KEY=your-secret-key
```
You can find this key in the Trigger.dev dashboard under API Keys and select the environment key you want to use.
![How to find your secret key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/api-keys.png)
</Step>
<Step title="Deploy your project">
Once you've added the environment variable, deploy your project to Vercel.
<Note>
Ensure you have also deployed your Trigger.dev task. See [deploy your task
step](/guides/frameworks/remix#deploying-your-task-to-trigger-dev).
</Note>
</Step>
<Step title="Test your task in production">
After deployment, you can test your task in production by running this curl command:
```bash
curl -X POST https://your-app.vercel.app/api/hello-world \
-H "Content-Type: application/json" \
-d '{"name": "James"}'
```
This sends a POST request to your API endpoint with a JSON payload.
</Step>
</Steps>
### Additional notes
The `vercel-build` script in `package.json` is specific to Remix projects on Vercel, ensuring that static assets are correctly copied to the build output.
The `runtime: "edge"` configuration in the API route allows for better performance on Vercel's Edge Network.
## Additional resources for Remix
<Card title="Remix - triggering tasks using webhooks" icon="R" href="/guides/frameworks/remix-webhooks">
How to create a webhook handler in a Remix app, and trigger a task from it.
</Card>
## Useful next steps
<CardGroup cols={2}>
<Card title="Tasks overview" icon="diagram-subtask" href="/tasks/overview">
Learn what tasks are and their options
</Card>
<Card title="Writing tasks" icon="pen-nib" href="/writing-tasks-introduction">
Learn how to write your own tasks
</Card>
<Card title="Deploy using the CLI" icon="terminal" href="/cli-deploy">
Learn how to deploy your task manually using the CLI
</Card>
<Card title="Deploy using GitHub actions" icon="github" href="/github-actions">
Learn how to deploy your task using GitHub actions
</Card>
</CardGroup>
# Triggering tasks with webhooks in Remix
Source: https://trigger.dev/docs/guides/frameworks/remix-webhooks
Learn how to trigger a task from a webhook in a Remix app.
## Prerequisites
* [A Remix project, set up with Trigger.dev](/guides/frameworks/remix)
* [cURL](https://curl.se/) installed on your local machine. This will be used to send a POST request to your webhook handler.
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/remix-webhooks">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Adding the webhook handler
The webhook handler in this guide will be an API route. Create a new file `app/routes/api.webhook-handler.ts` or `app/routes/api.webhook-handler.js`.
In your new file, add the following code:
```ts /api/webhook-handler.ts
import type { ActionFunctionArgs } from "@remix-run/node";
import { tasks } from "@trigger.dev/sdk/v3";
import { helloWorldTask } from "src/trigger/example";
export async function action({ request }: ActionFunctionArgs) {
const payload = await request.json();
// Trigger the helloWorldTask with the webhook data as the payload
await tasks.trigger<typeof helloWorldTask>("hello-world", payload);
return new Response("OK", { status: 200 });
}
```
This code will handle the webhook payload and trigger the 'Hello World' task.
## Triggering the task locally
Now that you have a webhook handler set up, you can trigger the 'Hello World' task from it. We will do this locally using cURL.
<Steps>
<Step title="Run your Remix app and the Trigger.dev dev server">
First, run your Remix app.
<CodeGroup>
```bash npm
npm run dev
```
```bash pnpm
pnpm run dev
```
```bash yarn
yarn dev
```
</CodeGroup>
Then, open up a second terminal window and start the Trigger.dev dev server:
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Trigger the webhook with some dummy data">
To send a POST request to your webhook handler, open up a terminal window on your local machine and run the following command:
<Tip>
If `http://localhost:5173` isn't the URL of your locally running Remix app, replace the URL in the
below command with that URL instead.
</Tip>
```bash
curl -X POST -H "Content-Type: application/json" -d '{"Name": "John Doe", "Age": "87"}' http://localhost:5173/api/webhook-handler
```
This will send a POST request to your webhook handler, with a JSON payload.
</Step>
<Step title="Check the task ran successfully">
After running the command, you should see a successful dev run and a 200 response in your terminals.
If you now go to your [Trigger.dev dashboard](https://cloud.trigger.dev), you should also see a successful run for the 'Hello World' task, with the payload you sent, in this case; `{"name": "John Doe", "age": "87"}`.
</Step>
</Steps>
# Sequin database triggers
Source: https://trigger.dev/docs/guides/frameworks/sequin
This guide will show you how to trigger tasks from database changes using Sequin
[Sequin](https://sequinstream.com) allows you to trigger tasks from database changes. Sequin captures every insert, update, and delete on a table and then ensures a task is triggered for each change.
Often, task runs coincide with database changes. For instance, you might want to use a Trigger.dev task to generate an embedding for each post in your database:
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/sequin-intro.png" alt="Sequin and Trigger.dev Overview" />
In this guide, you'll learn how to use Sequin to trigger Trigger.dev tasks from database changes.
## Prerequisites
You are about to create a [regular Trigger.dev task](/tasks-regular) that you will execute when ever a post is inserted or updated in your database. Sequin will detect all the changes on the `posts` table and then send the payload of the post to an API endpoint that will call `tasks.trigger()` to create the embedding and update the database.
As long as you create an HTTP endpoint that Sequin can deliver webhooks to, you can use any web framework or edge function (e.g. Supabase Edge Functions, Vercel Functions, Cloudflare Workers, etc.) to invoke your Trigger.dev task. In this guide, we'll show you how to setup Trigger.dev tasks using Next.js API Routes.
You'll need the following to follow this guide:
* A Next.js project with [Trigger.dev](https://trigger.dev) installed
<Info>
If you don't have one already, follow [Trigger.dev's Next.js setup
guide](/guides/frameworks/nextjs) to setup your project. You can return to this guide when
you're ready to write your first Trigger.dev task.
</Info>
* A [Sequin](https://console.sequinstream.com/register) account
* A Postgres database (Sequin works with any Postgres database version 12 and up) with a `posts` table.
## Create a Trigger.dev task
Start by creating a new Trigger.dev task that takes in a Sequin change event as a payload, creates an embedding, and then inserts the embedding into the database:
<Steps titleSize="h3">
<Step title="Create a `create-embedding-for-post` task">
In your `src/trigger/tasks` directory, create a new file called `create-embedding-for-post.ts` and add the following code:
<CodeGroup>
```ts trigger/create-embedding-for-post.ts
import { task } from "@trigger.dev/sdk/v3";
import { OpenAI } from "openai";
import { upsertEmbedding } from "../util";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
export const createEmbeddingForPost = task({
id: "create-embedding-for-post",
run: async (payload: {
record: {
id: number;
title: string;
body: string;
author: string;
createdAt: string;
embedding: string | null;
},
metadata: {
table_schema: string,
table_name: string,
consumer: {
id: string;
name: string;
};
};
}) => {
// Create an embedding using the title and body of payload.record
const content = `${payload.record.title}\n\n${payload.record.body}`;
const embedding = (await openai.embeddings.create({
model: "text-embedding-ada-002",
input: content,
})).data[0].embedding;
// Upsert the embedding in the database. See utils.ts for the implementation -> ->
await upsertEmbedding(embedding, payload.record.id);
// Return the updated record
return {
...payload.record,
embedding: JSON.stringify(embedding),
};
}
});
```
```ts utils.ts
import pg from "pg";
export async function upsertEmbedding(embedding: number[], id: number) {
const client = new pg.Client({
connectionString: process.env.DATABASE_URL,
});
await client.connect();
try {
const query = `
INSERT INTO post_embeddings (id, embedding)
VALUES ($2, $1)
ON CONFLICT (id)
DO UPDATE SET embedding = $1
`;
const values = [JSON.stringify(embedding), id];
const result = await client.query(query, values);
console.log(`Updated record in database. Rows affected: ${result.rowCount}`);
return result.rowCount;
} catch (error) {
console.error("Error updating record in database:", error);
throw error;
} finally {
await client.end();
}
}
```
</CodeGroup>
This task takes in a Sequin record event, creates an embedding, and then upserts the embedding into a `post_embeddings` table.
</Step>
<Step title="Add the task to your Trigger.dev project">
Register the `create-embedding-for-post` task to your Trigger.dev cloud project by running the following command:
```bash
npx trigger.dev@latest dev
```
In the Trigger.dev dashboard, you should now see the `create-embedding-for-post` task:
<Frame>
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/sequin-register-task.png" alt="Task added" />
</Frame>
</Step>
</Steps>
<Check>
You've successfully created a Trigger.dev task that will create an embedding for each post in your
database. In the next step, you'll create an API endpoint that Sequin can deliver records to.
</Check>
## Setup API route
You'll now create an API endpoint that will receive posts from Sequin and then trigger the `create-embedding-for-post` task.
<Info>
This guide covers how to setup an API endpoint using the Next.js App Router. You can find examples
for Next.js Server Actions and Pages Router in the [Trigger.dev
documentation](https://trigger.dev/docs/guides/frameworks/nextjs).
</Info>
<Steps titleSize="h3">
<Step title="Create a route handler">
Add a route handler by creating a new `route.ts` file in a `/app/api/create-embedding-for-post` directory:
```ts app/api/create-embedding-for-post/route.ts
import type { createEmbeddingForPost } from "@/trigger/create-embedding-for-post";
import { tasks } from "@trigger.dev/sdk/v3";
import { NextResponse } from "next/server";
export async function POST(req: Request) {
const authHeader = req.headers.get("authorization");
if (!authHeader || authHeader !== `Bearer ${process.env.SEQUIN_WEBHOOK_SECRET}`) {
return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
}
const payload = await req.json();
const handle = await tasks.trigger<typeof createEmbeddingForPost>(
"create-embedding-for-post",
payload
);
return NextResponse.json(handle);
}
```
This route handler will receive records from Sequin, parse them, and then trigger the `create-embedding-for-post` task.
</Step>
<Step title="Set secret keys">
You'll need to set four secret keys in a `.env.local` file:
```bash
SEQUIN_WEBHOOK_SECRET=your-secret-key
TRIGGER_SECRET_KEY=secret-from-trigger-dev
OPENAI_API_KEY=sk-proj-asdfasdfasdf
DATABASE_URL=postgresql://
```
The `SEQUIN_WEBHOOK_SECRET` ensures that only Sequin can access your API endpoint.
The `TRIGGER_SECRET_KEY` is used to authenticate requests to Trigger.dev and can be found in the **API keys** tab of the Trigger.dev dashboard.
The `OPENAI_API_KEY` and `DATABASE_URL` are used to create an embedding using OpenAI and connect to your database. Be sure to add these as [environment variables](https://trigger.dev/docs/deploy-environment-variables) in Trigger.dev as well.
</Step>
</Steps>
<Check>
You've successfully created an API endpoint that can receive record payloads from Sequin and
trigger a Trigger.dev task. In the next step, you'll setup Sequin to trigger the endpoint.
</Check>
## Create Sequin consumer
You'll now configure Sequin to send every row in your `posts` table to your Trigger.dev task.
<Steps titleSize="h3">
<Step title="Connect Sequin to your database">
1. Login to your Sequin account and click the **Add New Database** button.
2. Enter the connection details for your Postgres database.
<Info>
If you need to connect to a local dev database, flip the **use localhost** switch and follow the instructions to create a tunnel using the [Sequin CLI](https://sequinstream.com/docs/cli).
</Info>
3. Follow the instructions to create a publication and a replication slot by running two SQL commands in your database:
```sql
create publication sequin_pub for all tables;
select pg_create_logical_replication_slot('sequin_slot', 'pgoutput');
```
4. Name your database and click the **Connect Database** button.
Sequin will connect to your database and ensure that it's configured properly.
<Note>
If you need step-by-step connection instructions to connect Sequin to your database, check out our [quickstart guide](https://sequinstream.com/docs/quickstart).
</Note>
</Step>
<Step title="Tunnel to your local endpoint">
Now, create a tunnel to your local endpoint so Sequin can deliver change payloads to your local API:
1. In the Sequin console, open the **HTTP Endpoint** tab and click the **Create HTTP Endpoint** button.
2. Enter a name for your endpoint (i.e. `local_endpoint`) and flip the **Use localhost** switch. Follow the instructions in the Sequin console to [install the Sequin CLI](https://sequinstream.com/docs/cli), then run:
```bash
sequin tunnel --ports=3001:local_endpoint
```
3. Now, click **Add encryption header** and set the key to `Authorization` and the value to `Bearer SEQUIN_WEBHOOK_SECRET`.
4. Click **Create HTTP Endpoint**.
</Step>
<Step title="Create a Push Consumer">
Create a push consumer that will capture posts from your database and deliver them to your local endpoint:
1. Navigate to the **Consumers** tab and click the **Create Consumer** button.
2. Select your `posts` table (i.e `public.posts`).
3. You want to ensure that every post receives an embedding - and that embeddings are updated as posts are updated. To do this, select to process **Rows** and click **Continue**.
<Note>
You can also use **changes** for this particular use case, but **rows** comes with some nice replay and backfill features.
</Note>
4. You'll now set the sort and filter for the consumer. For this guide, we'll sort by `updated_at` and start at the beginning of the table. We won't apply any filters:
<Frame>
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/sequin-sort-and-filter.png" alt="Consumer Sort and Filter" />
</Frame>
5. On the next screen, select **Push** to have Sequin send the events to your webhook URL. Click **Continue**.
6. Now, give your consumer a name (i.e. `posts_push_consumer`) and in the **HTTP Endpoint** section select the `local_endpoint` you created above. Add the exact API route you created in the previous step (i.e. `/api/create-embedding-for-post`):
<Frame>
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/sequin-consumer-config.png" alt="Consumer Endpoint" />
</Frame>
7. Click the **Create Consumer** button.
</Step>
</Steps>
<Check>Your Sequin consumer is now created and ready to send events to your API endpoint.</Check>
## Test end-to-end
<Steps titleSize="h3">
<Step title="Spin up you dev environment">
1. The Next.js app is running: `npm run dev`
2. The Trigger.dev dev server is running `npx trigger.dev@latest dev`
3. The Sequin tunnel is running: `sequin tunnel --ports=3001:local_endpoint`
</Step>
<Step title="Create a new post in your database">
```sql
insert into
posts (title, body, author)
values
(
'The Future of AI',
'An insightful look into how artificial intelligence is shaping the future of technology and society.',
'Alice H Johnson'
);
```
</Step>
<Step title="Trace the change in the Sequin dashboard">
In the Sequin console, navigate to the [**Trace**](https://console.sequinstream.com/trace) tab and confirm that Sequin delivered the event to your local endpoint:
<Frame>
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/sequin-trace.png" alt="Trace Event" />
</Frame>
</Step>
<Step title="Confirm the event was received by your endpoint">
In your local terminal, you should see a `200` response in your Next.js app:
```bash
POST /api/create-embedding-for-post 200 in 262ms
```
</Step>
<Step title="Observe the task run in the Trigger.dev dashboard">
Finally, in the [**Trigger.dev dashboard**](https://cloud.trigger.dev/), navigate to the Runs page and confirm that the task run completed successfully:
<Frame>
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/sequin-final-run.png" alt="Task run" />
</Frame>
</Step>
</Steps>
<Check>
Every time a post is created or updated, Sequin will deliver the row payload to your API endpoint
and Trigger.dev will run the `create-embedding-for-post` task.
</Check>
## Next steps
With Sequin and Trigger.dev, every post in your database will now have an embedding. This is a simple example of how you can trigger long-running tasks on database changes.
From here, add error handling and deploy to production:
* Add [retries](/errors-retrying) to your Trigger.dev task to ensure that any errors are captured and logged.
* Deploy to [production](/guides/frameworks/nextjs#deploying-your-task-to-trigger-dev) and update your Sequin consumer to point to your production database and endpoint.
# Triggering tasks from Supabase edge functions
Source: https://trigger.dev/docs/guides/frameworks/supabase-edge-functions-basic
This guide will show you how to trigger a task from a Supabase edge function, and then view the run in our dashboard.
## Overview
Supabase edge functions allow you to trigger tasks either when an event is sent from a third party (e.g. when a new Stripe payment is processed, when a new user signs up to a service, etc), or when there are any changes or updates to your Supabase database.
This guide shows you how to set up and deploy a simple Supabase edge function example that triggers a task when an edge function URL is accessed.
## Prerequisites
* Ensure you have the [Supabase CLI](https://supabase.com/docs/guides/cli/getting-started) installed
* Since Supabase CLI version 1.123.4, you must have [Docker Desktop installed](https://supabase.com/docs/guides/functions/deploy#deploy-your-edge-functions) to deploy Edge Functions
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/supabase-edge-functions">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Initial setup
<Steps>
<Step title="Optional step 1: create a new Supabase project">
<Info> If you already have a Supabase project on your local machine you can skip this step.</Info>
You can create a new project by running the following command in your terminal using the Supabase CLI:
```bash
supabase init
```
<Note>
If you are using VS Code, ensure to answer 'y' when asked to generate VS Code settings for Deno,
and install any recommended extensions.
</Note>
</Step>
<Step title="Optional step 2: create a package.json file">
If your project does not already have `package.json` file (e.g. if you are using Deno), create it manually in your project's root folder.
<Info> If your project has a `package.json` file you can skip this step.</Info>
This is required for the Trigger.dev SDK to work correctly.
```ts package.json
{
"devDependencies": {
"typescript": "^5.6.2"
}
}
```
<Note> Update your Typescript version to the latest version available. </Note>
</Step>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Create a new Supabase edge function and deploy it
<Steps>
<Step title="Create a new Supabase edge function">
We'll call this example `edge-function-trigger`.
In your project, run the following command in the terminal using the Supabase CLI:
```bash
supabase functions new edge-function-trigger
```
</Step>
<Step title="Update the edge function code">
Replace the placeholder code in your `edge-function-trigger/index.ts` file with the following:
```ts functions/edge-function-trigger/index.ts
// Setup type definitions for built-in Supabase Runtime APIs
import "jsr:@supabase/functions-js/edge-runtime.d.ts";
// Import the Trigger.dev SDK - replace "<your-sdk-version>" with the version of the SDK you are using, e.g. "3.0.0". You can find this in your package.json file.
import { tasks } from "npm:@trigger.dev/sdk@3.0.0/v3";
// Import your task type from your /trigger folder
import type { helloWorldTask } from "../../../src/trigger/example.ts";
//     ðŸ‘† **type-only** import
Deno.serve(async () => {
await tasks.trigger<typeof helloWorldTask>(
// Your task id
"hello-world",
// Your task payload
"Hello from a Supabase Edge Function!"
);
return new Response("OK");
});
```
<Note>You can only import the `type` from the task.</Note>
<Note>
Tasks in the `trigger` folder use Node, so they must stay in there or they will not run,
especially if you are using a different runtime like Deno. Also do not add "`npm:`" to imports
inside your task files, for the same reason.
</Note>
</Step>
<Step title="Deploy your edge function using the Supabase CLI">
You can now deploy your edge function with the following command in your terminal:
```bash
supabase functions deploy edge-function-trigger --no-verify-jwt
```
<Note>
`--no-verify-jwt` removes the JSON Web Tokens requirement from the authorization header. By
default this should be on, but it is not required for this example. Learn more about JWTs
[here](https://supabase.com/docs/guides/auth/jwts).
</Note>
Follow the CLI instructions and once complete you should now see your new edge function deployment in your Supabase edge functions dashboard.
There will be a link to the dashboard in your terminal output, or you can find it at this URL:
`https://supabase.com/dashboard/project/<your-project-id>/functions`
<Note>Replace `your-project-id` with your actual project ID.</Note>
</Step>
</Steps>
## Set your Trigger.dev prod secret key in the Supabase dashboard
To trigger a task from your edge function, you need to set your Trigger.dev secret key in the Supabase dashboard.
To do this, first go to your Trigger.dev [project dashboard](https://cloud.trigger.dev) and copy the `prod` secret key from the API keys page.
![How to find your prod secret key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/api-key-prod.png)
Then, in [Supabase](https://supabase.com/dashboard/projects), select your project, navigate to 'Project settings' <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />, click 'Edge functions' <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" /> in the configurations menu, and then click the 'Add new secret' <Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" /> button.
Add `TRIGGER_SECRET_KEY` <Icon icon="circle-4" iconType="solid" size={20} color="A8FF53" /> with the pasted value of your Trigger.dev `prod` secret key.
![Add secret key in Supabase](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-keys-1.png)
## Deploy your task and trigger it from your edge function
<Steps>
<Step title="Deploy your 'Hello World' task">
Next, deploy your `hello-world` task to [Trigger.dev cloud](https://cloud.trigger.dev).
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
</Step>
<Step title="Trigger a prod run from your deployed edge function">
To do this all you need to do is simply open the `edge-function-trigger` URL.
`https://supabase.com/dashboard/project/<your-project-id>/functions`
<Note>Replace `your-project-id` with your actual project ID.</Note>
In your Supabase project, go to your Edge function dashboard, find `edge-function-trigger`, copy the URL, and paste it into a new window in your browser.
Once loaded you should see â€˜OKâ€™ on the new screen.
![Edge function URL](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-function-url.png)
The task will be triggered when your edge function URL is accessed.
Check your [cloud.trigger.dev](http://cloud.trigger.dev) dashboard and you should see a succesful `hello-world` task.
**Congratulations, you have run a simple Hello World task from a Supabase edge function!**
</Step>
</Steps>
## Learn more about Supabase and Trigger.dev
### Full walkthrough guides from development to deployment
<CardGroup cols={2}>
<Card title="Edge function hello world guide" icon="book" href="/guides/frameworks/supabase-edge-functions-basic">
Learn how to trigger a task from a Supabase edge function when a URL is visited.
</Card>
<Card title="Database webhooks guide" icon="book" href="/guides/frameworks/supabase-edge-functions-database-webhooks">
Learn how to trigger a task from a Supabase edge function when an event occurs in your database.
</Card>
</CardGroup>
### Task examples with code you can copy and paste
<CardGroup cols={2}>
<Card title="Supabase database operations" icon="bolt" href="/guides/examples/supabase-database-operations">
Run basic CRUD operations on a table in a Supabase database using Trigger.dev.
</Card>
<Card title="Supabase Storage upload" icon="bolt" href="/guides/examples/supabase-storage-upload">
Download a video from a URL and upload it to Supabase Storage using S3.
</Card>
</CardGroup>
# Triggering tasks from Supabase Database Webhooks
Source: https://trigger.dev/docs/guides/frameworks/supabase-edge-functions-database-webhooks
This guide shows you how to trigger a transcribing task when a row is added to a table in a Supabase database, using a Database Webhook and Edge Function.
## Overview
Supabase and Trigger.dev can be used together to create powerful workflows triggered by real-time changes in your database tables:
* A Supabase Database Webhook triggers an Edge Function when a row including a video URL is inserted into a table
* The Edge Function triggers a Trigger.dev task, passing the `video_url` column data from the new table row as the payload
* The Trigger.dev task then:
* Uses [FFmpeg](https://www.ffmpeg.org/) to extract the audio track from a video URL
* Uses [Deepgram](https://deepgram.com) to transcribe the extracted audio
* Updates the original table row using the `record.id` in Supabase with the new transcription using `update`
## Prerequisites
* Ensure you have the [Supabase CLI](https://supabase.com/docs/guides/cli/getting-started) installed
* Since Supabase CLI version 1.123.4, you must have [Docker Desktop installed](https://supabase.com/docs/guides/functions/deploy#deploy-your-edge-functions) to deploy Edge Functions
* Ensure TypeScript is installed
* [Create a Trigger.dev account](https://cloud.trigger.dev)
* Create a new Trigger.dev project
* [Create a new Deepgram account](https://deepgram.com/) and get your API key from the dashboard
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/supabase-edge-functions">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## Initial setup
<Steps>
<Step title="Optional step 1: create a new Supabase project">
<Info> If you already have a Supabase project on your local machine you can skip this step.</Info>
You can create a new project by running the following command in your terminal using the Supabase CLI:
```bash
supabase init
```
<Note>
If you are using VS Code, ensure to answer 'y' when asked to generate VS Code settings for Deno,
and install any recommended extensions.
</Note>
</Step>
<Step title="Optional step 2: create a package.json file">
If your project does not already have `package.json` file (e.g. if you are using Deno), create it manually in your project's root folder.
<Info> If your project has a `package.json` file you can skip this step.</Info>
This is required for the Trigger.dev SDK to work correctly.
```ts package.json
{
"devDependencies": {
"typescript": "^5.6.2"
}
}
```
<Note> Update your Typescript version to the latest version available. </Note>
</Step>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Choose "None" when prompted to install an example task. We will create a new task for this guide.
</Step>
</Steps>
## Create a new table in your Supabase database
First, in the Supabase project dashboard, you'll need to create a new table to store the video URL and transcription.
To do this, click on 'Table Editor' <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" /> in the left-hand menu and create a new table. <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" />
![How to create a new Supabase table](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-new-table-1.png)
Call your table `video_transcriptions`. <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />
Add two new columns, one called `video_url` with the type `text` <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" />, and another called `transcription`, also with the type `text` <Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" />.
![How to create a new Supabase table 2](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-new-table-2.png)
## Create and deploy the Trigger.dev task
### Generate the Database type definitions
To allow you to use TypeScript to interact with your table, you need to [generate the type definitions](https://supabase.com/docs/guides/api/rest/generating-types) for your Supabase table using the Supabase CLI.
```bash
supabase gen types --lang=typescript --project-id <project-ref> --schema public > database.types.ts
```
<Note> Replace `<project-ref>` with your Supabase project reference ID. This can be found in your Supabase project settings under 'General'. </Note>
### Create the transcription task
Create a new task file in your `/trigger` folder. Call it `videoProcessAndUpdate.ts`.
This task takes a video from a public video url, extracts the audio using FFmpeg and transcribes the audio using Deepgram. The transcription summary will then be updated back to the original row in the `video_transcriptions` table in Supabase.
You will need to install some additional dependencies for this task:
<CodeGroup>
```bash npm
npm install @deepgram/sdk @supabase/supabase-js fluent-ffmpeg
```
```bash pnpm
pnpm install @deepgram/sdk @supabase/supabase-js fluent-ffmpeg
```
```bash yarn
yarn install @deepgram/sdk @supabase/supabase-js fluent-ffmpeg
```
</CodeGroup>
These dependencies will allow you to interact with the Deepgram and Supabase APIs and extract audio from a video using FFmpeg.
```ts /trigger/videoProcessAndUpdate.ts
// Install any missing dependencies below
import { createClient as createDeepgramClient } from "@deepgram/sdk";
import { createClient as createSupabaseClient } from "@supabase/supabase-js";
import { logger, task } from "@trigger.dev/sdk/v3";
import ffmpeg from "fluent-ffmpeg";
import fs from "fs";
import { Readable } from "node:stream";
import os from "os";
import path from "path";
import { Database } from "../../database.types";
// Create a single Supabase client for interacting with your database
// 'Database' supplies the type definitions to supabase-js
const supabase = createSupabaseClient<Database>(
// These details can be found in your Supabase project settings under `API`
process.env.SUPABASE_PROJECT_URL as string, // e.g. https://abc123.supabase.co - replace 'abc123' with your project ID
process.env.SUPABASE_SERVICE_ROLE_KEY as string // Your service role secret key
);
// Your DEEPGRAM_SECRET_KEY can be found in your Deepgram dashboard
const deepgram = createDeepgramClient(process.env.DEEPGRAM_SECRET_KEY);
export const videoProcessAndUpdate = task({
id: "video-process-and-update",
run: async (payload: { videoUrl: string; id: number }) => {
const { videoUrl, id } = payload;
logger.log(`Processing video at URL: ${videoUrl}`);
// Generate temporary file names
const tempDirectory = os.tmpdir();
const outputPath = path.join(tempDirectory, `audio_${Date.now()}.wav`);
const response = await fetch(videoUrl);
// Extract the audio using FFmpeg
await new Promise((resolve, reject) => {
if (!response.body) {
return reject(new Error("Failed to fetch video"));
}
ffmpeg(Readable.from(response.body))
.outputOptions([
"-vn", // Disable video output
"-acodec pcm_s16le", // Use PCM 16-bit little-endian encoding
"-ar 44100", // Set audio sample rate to 44.1 kHz
"-ac 2", // Set audio channels to stereo
])
.output(outputPath)
.on("end", resolve)
.on("error", reject)
.run();
});
logger.log(`Audio extracted from video`, { outputPath });
// Transcribe the audio using Deepgram
const { result, error } = await deepgram.listen.prerecorded.transcribeFile(
fs.readFileSync(outputPath),
{
model: "nova-2", // Use the Nova 2 model
smart_format: true, // Automatically format the transcription
diarize: true, // Enable speaker diarization
}
);
if (error) {
throw error;
}
const transcription = result.results.channels[0].alternatives[0].paragraphs?.transcript;
logger.log(`Transcription: ${transcription}`);
// Delete the temporary audio file
fs.unlinkSync(outputPath);
logger.log(`Temporary audio file deleted`, { outputPath });
const { error: updateError } = await supabase
.from("video_transcriptions")
// Update the transcription column
.update({ transcription: transcription })
// Find the row by its ID
.eq("id", id);
if (updateError) {
throw new Error(`Failed to update transcription: ${updateError.message}`);
}
return {
message: `Summary of the audio: ${transcription}`,
result,
};
},
});
```
<Warning>
When updating your tables from a Trigger.dev task which has been triggered by a database change,
be extremely careful to not cause an infinite loop. Ensure you have the correct conditions in
place to prevent this.
</Warning>
### Adding the FFmpeg build extension
Before you can deploy the task, you'll need to add the FFmpeg build extension to your `trigger.config.ts` file.
```ts trigger.config.ts
// Add this import
import { ffmpeg } from "@trigger.dev/build/extensions/core";
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>", // Replace with your project ref
// Your other config settings...
build: {
// Add the FFmpeg build extension
extensions: [ffmpeg()],
},
});
```
<Note>
[Build extensions](/config/extensions/overview) allow you to hook into the build system and
customize the build process or the resulting bundle and container image (in the case of
deploying). You can use pre-built extensions or create your own.
</Note>
<Note>
You'll also need to add `@trigger.dev/build` to your `package.json` file under `devDependencies`
if you don't already have it there.
</Note>
If you are modifying this example and using popular FFmpeg libraries like `fluent-ffmpeg` you'll also need to add them to [`external`](/config/config-file#external) in your `trigger.config.ts` file.
### Add your Deepgram and Supabase environment variables to your Trigger.dev project
You will need to add your `DEEPGRAM_SECRET_KEY`, `SUPABASE_PROJECT_URL` and `SUPABASE_SERVICE_ROLE_KEY` as environment variables in your Trigger.dev project. This can be done in the 'Environment Variables' page in your project dashboard.
![Adding environment variables](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/environment-variables-page.jpg)
### Deploying your task
Now you can now deploy your task using the following command:
<CodeGroup>
```bash npm
npx trigger.dev@latest deploy
```
```bash pnpm
pnpm dlx trigger.dev@latest deploy
```
```bash yarn
yarn dlx trigger.dev@latest deploy
```
</CodeGroup>
## Create and deploy the Supabase Edge Function
### Add your Trigger.dev prod secret key to the Supabase dashboard
Go to your Trigger.dev [project dashboard](https://cloud.trigger.dev) and copy the `prod` secret key from the API keys page.
![How to find your prod secret key](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/api-key-prod.png)
Then, in [Supabase](https://supabase.com/dashboard/projects), select the project you want to use, navigate to 'Project settings' <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />, click 'Edge Functions' <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" /> in the configurations menu, and then click the 'Add new secret' <Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" /> button.
Add `TRIGGER_SECRET_KEY` <Icon icon="circle-4" iconType="solid" size={20} color="A8FF53" /> with the pasted value of your Trigger.dev `prod` secret key.
![Add secret key in Supabase](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-keys-1.png)
### Create a new Edge Function using the Supabase CLI
Now create an Edge Function using the Supabase CLI. Call it `video-processing-handler`. This function will be triggered by the Database Webhook.
```bash
supabase functions new video-processing-handler
```
```ts functions/video-processing-handler/index.ts
// Setup type definitions for built-in Supabase Runtime APIs
import "jsr:@supabase/functions-js/edge-runtime.d.ts";
import { tasks } from "npm:@trigger.dev/sdk@latest/v3";
// Import the videoProcessAndUpdate task from the trigger folder
import type { videoProcessAndUpdate } from "../../../src/trigger/videoProcessAndUpdate.ts";
//     ðŸ‘† type only import
// Sets up a Deno server that listens for incoming JSON requests
Deno.serve(async (req) => {
const payload = await req.json();
// This payload will contain the video url and id from the new row in the table
const videoUrl = payload.record.video_url;
const id = payload.record.id;
// Trigger the videoProcessAndUpdate task with the videoUrl payload
await tasks.trigger<typeof videoProcessAndUpdate>("video-process-and-update", { videoUrl, id });
console.log(payload ?? "No name provided");
return new Response("ok");
});
```
<Note>
Tasks in the `trigger` folder use Node, so they must stay in there or they will not run,
especially if you are using a different runtime like Deno. Also do not add "`npm:`" to imports
inside your task files, for the same reason.
</Note>
### Deploy the Edge Function
Now deploy your new Edge Function with the following command:
```bash
supabase functions deploy video-processing-handler
```
Follow the CLI instructions, selecting the same project you added your `prod` secret key to, and once complete you should see your new Edge Function deployment in your Supabase Edge Functions dashboard.
There will be a link to the dashboard in your terminal output.
## Create the Database Webhook
In your Supabase project dashboard, click 'Project settings' <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />, then the 'API' tab <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" />, and copy the `anon` `public` API key from the table <Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" />.
![How to find your Supabase API keys](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-api-key.png)
Then, go to 'Database' <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" /> click on 'Webhooks' <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" />, and then click 'Create a new hook' <Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" />.
![How to create a new webhook](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-create-webhook-1.png)
<Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" /> Call the hook `edge-function-hook`.
<Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" /> Select the new table you have created:
`public` `video_transcriptions`.
<Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" /> Choose the `insert` event.
![How to create a new webhook 2](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-create-webhook-2.png)
<Icon icon="circle-4" iconType="solid" size={20} color="A8FF53" /> Under 'Webhook configuration', select
'Supabase Edge Functions'{" "}
<Icon icon="circle-5" iconType="solid" size={20} color="A8FF53" /> Under 'Edge Function', choose `POST`
and select the Edge Function you have created: `video-processing-handler`.{" "}
<Icon icon="circle-6" iconType="solid" size={20} color="A8FF53" /> Under 'HTTP Headers', add a new header with the key `Authorization` and the value `Bearer <your-api-key>` (replace `<your-api-key>` with the `anon` `public` API key you copied earlier).
<Info>
Supabase Edge Functions require a JSON Web Token [JWT](https://supabase.com/docs/guides/auth/jwts)
in the authorization header. This is to ensure that only authorized users can access your edge
functions.
</Info>
<Icon icon="circle-7" iconType="solid" size={20} color="A8FF53" /> Click 'Create webhook'.{" "}
![How to create a new webhook 3](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-create-webhook-3.png)
Your Database Webhook is now ready to use.
## Triggering the entire workflow
Your `video-processing-handler` Edge Function is now set up to trigger the `videoProcessAndUpdate` task every time a new row is inserted into your `video_transcriptions` table.
To do this, go back to your Supabase project dashboard, click on 'Table Editor' <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" /> in the left-hand menu, click on the `video_transcriptions` table <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" /> , and then click 'Insert', 'Insert Row' <Icon icon="circle-3" iconType="solid" size={20} color="A8FF53" />.
![How to insert a new row 1](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-new-table-3.png)
Add a new item under `video_url`, with a public video url. <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" />.
You can use the following public video URL for testing: `https://content.trigger.dev/Supabase%20Edge%20Functions%20Quickstart.mp4`.
![How to insert a new row 2](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-new-table-4.png)
Once the new table row has been inserted, check your [cloud.trigger.dev](http://cloud.trigger.dev) project 'Runs' list <Icon icon="circle-1" iconType="solid" size={20} color="A8FF53" /> and you should see a processing `videoProcessAndUpdate` task <Icon icon="circle-2" iconType="solid" size={20} color="A8FF53" /> which has been triggered when you added a new row with the video url to your `video_transcriptions` table.
![Supabase successful run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-run-result.png)
Once the run has completed successfully, go back to your Supabase `video_transcriptions` table, and you should see that in the row containing the original video URL, the transcription has now been added to the `transcription` column.
![Supabase successful table update](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/supabase-table-result.png)
**Congratulations! You have completed the full workflow from Supabase to Trigger.dev and back again.**
## Learn more about Supabase and Trigger.dev
### Full walkthrough guides from development to deployment
<CardGroup cols={2}>
<Card title="Edge function hello world guide" icon="book" href="/guides/frameworks/supabase-edge-functions-basic">
Learn how to trigger a task from a Supabase edge function when a URL is visited.
</Card>
<Card title="Database webhooks guide" icon="book" href="/guides/frameworks/supabase-edge-functions-database-webhooks">
Learn how to trigger a task from a Supabase edge function when an event occurs in your database.
</Card>
</CardGroup>
### Task examples with code you can copy and paste
<CardGroup cols={2}>
<Card title="Supabase database operations" icon="bolt" href="/guides/examples/supabase-database-operations">
Run basic CRUD operations on a table in a Supabase database using Trigger.dev.
</Card>
<Card title="Supabase Storage upload" icon="bolt" href="/guides/examples/supabase-storage-upload">
Download a video from a URL and upload it to Supabase Storage using S3.
</Card>
</CardGroup>
# Supabase overview
Source: https://trigger.dev/docs/guides/frameworks/supabase-guides-overview
Guides and examples for using Supabase with Trigger.dev.
## Learn more about Supabase and Trigger.dev
### Full walkthrough guides from development to deployment
<CardGroup cols={2}>
<Card title="Edge function hello world guide" icon="book" href="/guides/frameworks/supabase-edge-functions-basic">
Learn how to trigger a task from a Supabase edge function when a URL is visited.
</Card>
<Card title="Database webhooks guide" icon="book" href="/guides/frameworks/supabase-edge-functions-database-webhooks">
Learn how to trigger a task from a Supabase edge function when an event occurs in your database.
</Card>
</CardGroup>
### Task examples with code you can copy and paste
<CardGroup cols={2}>
<Card title="Supabase database operations" icon="bolt" href="/guides/examples/supabase-database-operations">
Run basic CRUD operations on a table in a Supabase database using Trigger.dev.
</Card>
<Card title="Supabase Storage upload" icon="bolt" href="/guides/examples/supabase-storage-upload">
Download a video from a URL and upload it to Supabase Storage using S3.
</Card>
</CardGroup>
# Using webhooks with Trigger.dev
Source: https://trigger.dev/docs/guides/frameworks/webhooks-guides-overview
Guides for using webhooks with Trigger.dev.
## Overview
Webhooks are a way to send and receive events from external services. Triggering tasks using webhooks allow you to add real-time, event driven functionality to your app.
A webhook handler is code that executes in response to an event. They can be endpoints in your framework's routing which can be triggered by an external service.
## Webhook guides
<CardGroup cols={2}>
<Card title="Next.js - triggering tasks using webhooks" icon="N" href="/guides/frameworks/nextjs-webhooks">
How to create a webhook handler in a Next.js app, and trigger a task from it.
</Card>
<Card title="Remix - triggering tasks using webhooks" icon="R" href="/guides/frameworks/remix-webhooks">
How to create a webhook handler in a Remix app, and trigger a task from it.
</Card>
<Card title="Stripe webhooks" icon="webhook" href="/guides/examples/stripe-webhook">
How to create a Stripe webhook handler and trigger a task when a 'checkout session completed'
event is received.
</Card>
<Card title="Supabase database webhooks guide" icon="webhook" href="/guides/frameworks/supabase-edge-functions-database-webhooks">
Learn how to trigger a task from a Supabase edge function when an event occurs in your database.
</Card>
</CardGroup>
# Frameworks, guides and examples
Source: https://trigger.dev/docs/guides/introduction
A growing list of guides and examples to get the most out of Trigger.dev.
## Frameworks
<CardGroup cols={3}>
<Card title="Bun" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/logo-bun.png" href="/guides/frameworks/bun" />
<Card title="Next.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/logo-nextjs.png" href="/guides/frameworks/nextjs" />
<Card title="Node.js" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/logo-nodejs.png" href="/guides/frameworks/nodejs" />
<Card title="Remix" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/logo-remix.png" href="/guides/frameworks/remix" />
<Card title="SvelteKit" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/logo-svelte.png" href="/guides/community/sveltekit" />
</CardGroup>
## Guides
Get set up fast using our detailed walk-through guides.
| Guide                                                                                      | Description                                                          |
| :----------------------------------------------------------------------------------------- | :------------------------------------------------------------------- |
| [AI Agent: Generate and translate copy](/guides/ai-agents/generate-translate-copy)         | Chain prompts to generate and translate content                      |
| [AI Agent: Route questions](/guides/ai-agents/route-question)                              | Route questions to different models based on complexity              |
| [AI Agent: Content moderation](/guides/ai-agents/respond-and-check-content)                | Parallel check content while responding to customers                 |
| [AI Agent: News verification](/guides/ai-agents/verify-news-article)                       | Orchestrate fact checking of news articles                           |
| [AI Agent: Translation refinement](/guides/ai-agents/translate-and-refine)                 | Evaluate and refine translations with feedback                       |
| [Prisma](/guides/frameworks/prisma)                                                        | How to setup Prisma with Trigger.dev                                 |
| [Python image processing](/guides/python/python-image-processing)                          | Use Python and Pillow to process images                              |
| [Python web crawler](/guides/python/python-crawl4ai)                                       | Use Python, Crawl4AI and Playwright to create a headless web crawler |
| [Python PDF form extractor](/guides/python/python-pdf-form-extractor)                      | Use Python, PyMuPDF and Trigger.dev to extract data from a PDF form  |
| [Sequin database triggers](/guides/frameworks/sequin)                                      | Trigger tasks from database changes using Sequin                     |
| [Supabase edge function hello world](/guides/frameworks/supabase-edge-functions-basic)     | Trigger tasks from Supabase edge function                            |
| [Supabase database webhooks](/guides/frameworks/supabase-edge-functions-database-webhooks) | Trigger tasks using Supabase database webhooks                       |
| [Using webhooks in Next.js](/guides/frameworks/nextjs-webhooks)                            | Trigger tasks from a webhook in Next.js                              |
| [Using webhooks in Remix](/guides/frameworks/remix-webhooks)                               | Trigger tasks from a webhook in Remix                                |
| [Stripe webhooks](/guides/examples/stripe-webhook)                                         | Trigger tasks from incoming Stripe webhook events                    |
## Example projects
Example projects are full projects with example repos you can fork and use. These are a great way of learning how to encorporate Trigger.dev into your project.
| Example project                                                                         | Description                                                                                                    | Framework | GitHub                                                                                                        |
| :-------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------- | :-------- | :------------------------------------------------------------------------------------------------------------ |
| [Batch LLM Evaluator](/guides/example-projects/batch-llm-evaluator)                     | Evaluate multiple LLM models and stream the results to the frontend.                                           | Next.js   | [View the repo](https://github.com/triggerdotdev/examples/tree/main/batch-llm-evaluator)                      |
| [Claude thinking chatbot](/guides/example-projects/claude-thinking-chatbot)             | Use Vercel's AI SDK and Anthropic's Claude 3.7 model to create a thinking chatbot.                             | Next.js   | [View the repo](https://github.com/triggerdotdev/examples/tree/main/claude-thinking-chatbot)                  |
| [Turborepo monorepo with Prisma](/guides/example-projects/turborepo-monorepo-prisma)    | Use Prisma in a Turborepo monorepo with Trigger.dev.                                                           | Next.js   | [View the repo](https://github.com/triggerdotdev/examples/tree/main/monorepos/turborepo-prisma-tasks-package) |
| [Realtime Fal.ai image generation](/guides/example-projects/realtime-fal-ai)            | Generate an image from a prompt using Fal.ai and show the progress of the task on the frontend using Realtime. | Next.js   | [View the repo](https://github.com/triggerdotdev/examples/tree/main/realtime-fal-ai-image-generation)         |
| [Realtime CSV Importer](/guides/example-projects/realtime-csv-importer)                 | Upload a CSV file and see the progress of the task streamed to the frontend.                                   | Next.js   | [View the repo](https://github.com/triggerdotdev/examples/tree/main/realtime-csv-importer)                    |
| [Vercel AI SDK image generator](/guides/example-projects/vercel-ai-sdk-image-generator) | Use the Vercel AI SDK to generate images from a prompt.                                                        | Next.js   | [View the repo](https://github.com/triggerdotdev/examples/tree/main/vercel-ai-sdk-image-generator)            |
| [Python web crawler](/guides/python/python-crawl4ai)                                    | Use Python, Crawl4AI and Playwright to create a headless web crawler with Trigger.dev.                         | â€”         | [View the repo](https://github.com/triggerdotdev/examples/tree/main/python-crawl4ai)                          |
## Example tasks
Task code you can copy and paste to use in your project. They can all be extended and customized to fit your needs.
| Example task                                                                  | Description                                                                                                                                          |
| :---------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------- |
| [DALLÂ·E 3 image generation](/guides/examples/dall-e3-generate-image)          | Use OpenAI's GPT-4o and DALLÂ·E 3 to generate an image and text.                                                                                      |
| [Deepgram audio transcription](/guides/examples/deepgram-transcribe-audio)    | Transcribe audio using Deepgram's speech recognition API.                                                                                            |
| [Fal.ai image to cartoon](/guides/examples/fal-ai-image-to-cartoon)           | Convert an image to a cartoon using Fal.ai, and upload the result to Cloudflare R2.                                                                  |
| [Fal.ai with Realtime](/guides/examples/fal-ai-realtime)                      | Generate an image from a prompt using Fal.ai and show the progress of the task on the frontend using Realtime.                                       |
| [FFmpeg video processing](/guides/examples/ffmpeg-video-processing)           | Use FFmpeg to process a video in various ways and save it to Cloudflare R2.                                                                          |
| [Firecrawl URL crawl](/guides/examples/firecrawl-url-crawl)                   | Learn how to use Firecrawl to crawl a URL and return LLM-ready markdown.                                                                             |
| [LibreOffice PDF conversion](/guides/examples/libreoffice-pdf-conversion)     | Convert a document to PDF using LibreOffice.                                                                                                         |
| [OpenAI with retrying](/guides/examples/open-ai-with-retrying)                | Create a reusable OpenAI task with custom retry options.                                                                                             |
| [PDF to image](/guides/examples/pdf-to-image)                                 | Use `MuPDF` to turn a PDF into images and save them to Cloudflare R2.                                                                                |
| [React to PDF](/guides/examples/react-pdf)                                    | Use `react-pdf` to generate a PDF and save it to Cloudflare R2.                                                                                      |
| [Puppeteer](/guides/examples/puppeteer)                                       | Use Puppeteer to generate a PDF or scrape a webpage.                                                                                                 |
| [Resend email sequence](/guides/examples/resend-email-sequence)               | Send a sequence of emails over several days using Resend with Trigger.dev.                                                                           |
| [Scrape Hacker News](/guides/examples/scrape-hacker-news)                     | Scrape Hacker News using BrowserBase and Puppeteer, summarize the articles with ChatGPT and send an email of the summary every weekday using Resend. |
| [Sentry error tracking](/guides/examples/sentry-error-tracking)               | Automatically send errors to Sentry from your tasks.                                                                                                 |
| [Sharp image processing](/guides/examples/sharp-image-processing)             | Use Sharp to process an image and save it to Cloudflare R2.                                                                                          |
| [Supabase database operations](/guides/examples/supabase-database-operations) | Run basic CRUD operations on a table in a Supabase database using Trigger.dev.                                                                       |
| [Supabase Storage upload](/guides/examples/supabase-storage-upload)           | Download a video from a URL and upload it to Supabase Storage using S3.                                                                              |
| [Vercel AI SDK](/guides/examples/vercel-ai-sdk)                               | Use Vercel AI SDK to generate text using OpenAI.                                                                                                     |
| [Vercel sync environment variables](/guides/examples/vercel-sync-env-vars)    | Automatically sync environment variables from your Vercel projects to Trigger.dev.                                                                   |
<Note>
If you would like to see a guide for your framework, or an example task for your use case, please
request it in our [Discord server](https://trigger.dev/discord) and we'll add it to the list.
</Note>
# Python headless browser web crawler example
Source: https://trigger.dev/docs/guides/python/python-crawl4ai
Learn how to use Python, Crawl4AI and Playwright to create a headless browser web crawler with Trigger.dev.
## Overview
This demo showcases how to use Trigger.dev with Python to build a web crawler that uses a headless browser to navigate websites and extract content.
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [Python](https://www.python.org/) installed on your local machine
## Features
* [Trigger.dev](https://trigger.dev) for background task orchestration
* Our [Python build extension](/config/extensions/pythonExtension) to install the dependencies and run the Python script
* [Crawl4AI](https://github.com/unclecode/crawl4ai), an open source LLM friendly web crawler
* A custom [Playwright extension](https://playwright.dev/) to create a headless chromium browser
* Proxy support
## Using Proxies
<Warning>
**WEB SCRAPING:** When web scraping, you MUST use a proxy to comply with our terms of service. Direct scraping of third-party websites without the site owner's permission using Trigger.dev Cloud is prohibited and will result in account suspension. See [this example](/guides/examples/puppeteer#scrape-content-from-a-web-page) which uses a proxy.
</Warning>
Some popular proxy services are:
* [Smartproxy](https://smartproxy.com/)
* [Bright Data](https://brightdata.com/)
* [Browserbase](https://browserbase.com/)
* [Oxylabs](https://oxylabs.io/)
* [ScrapingBee](https://scrapingbee.com/)
Once you have a proxy service, set the following environment variables in your Trigger.dev .env file, and add them in the Trigger.dev dashboard:
* `PROXY_URL`: The URL of your proxy server (e.g., `http://proxy.example.com:8080`)
* `PROXY_USERNAME`: Username for authenticated proxies (optional)
* `PROXY_PASSWORD`: Password for authenticated proxies (optional)
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/python-crawl4ai">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## The code
### Build configuration
After you've initialized your project with Trigger.dev, add these build settings to your `trigger.config.ts` file:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { pythonExtension } from "@trigger.dev/python/extension";
import type { BuildContext, BuildExtension } from "@trigger.dev/core/v3/build";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
extensions: [
// This is required to use the Python extension
pythonExtension(),
// This is required to create a headless chromium browser with Playwright
installPlaywrightChromium(),
],
},
});
// This is a custom build extension to install Playwright and Chromium
export function installPlaywrightChromium(): BuildExtension {
return {
name: "InstallPlaywrightChromium",
onBuildComplete(context: BuildContext) {
const instructions = [
// Base and Chromium dependencies
`RUN apt-get update && apt-get install -y --no-install-recommends \
curl unzip npm libnspr4 libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 \
libasound2 libnss3 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 \
libgbm1 libxkbcommon0 \
&& apt-get clean && rm -rf /var/lib/apt/lists/*`,
// Install Playwright and Chromium
`RUN npm install -g playwright`,
`RUN mkdir -p /ms-playwright`,
`RUN PLAYWRIGHT_BROWSERS_PATH=/ms-playwright python -m playwright install --with-deps chromium`,
];
context.addLayer({
id: "playwright",
image: { instructions },
deploy: {
env: {
PLAYWRIGHT_BROWSERS_PATH: "/ms-playwright",
PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: "1",
PLAYWRIGHT_SKIP_BROWSER_VALIDATION: "1",
},
override: true,
},
});
},
};
}
```
<Info>
Learn more about executing scripts in your Trigger.dev project using our Python build extension
[here](/config/extensions/pythonExtension).
</Info>
### Task code
This task uses the `python.runScript` method to run the `crawl-url.py` script with the given URL as an argument. You can see the original task in our examples repository [here](https://github.com/triggerdotdev/examples/blob/main/python-crawl4ai/src/trigger/pythonTasks.ts).
```ts src/trigger/pythonTasks.ts
import { logger, schemaTask, task } from "@trigger.dev/sdk/v3";
import { python } from "@trigger.dev/python";
import { z } from "zod";
export const convertUrlToMarkdown = schemaTask({
id: "convert-url-to-markdown",
schema: z.object({
url: z.string().url(),
}),
run: async (payload) => {
// Pass through any proxy environment variables
const env = {
PROXY_URL: process.env.PROXY_URL,
PROXY_USERNAME: process.env.PROXY_USERNAME,
PROXY_PASSWORD: process.env.PROXY_PASSWORD,
};
const result = await python.runScript("./src/python/crawl-url.py", [payload.url], { env });
logger.debug("convert-url-to-markdown", {
url: payload.url,
result,
});
return result.stdout;
},
});
```
### Add a requirements.txt file
Add the following to your `requirements.txt` file. This is required in Python projects to install the dependencies.
```txt requirements.txt
crawl4ai
playwright
urllib3<2.0.0
```
### The Python script
The Python script is a simple script using Crawl4AI that takes a URL and returns the markdown content of the page. You can see the original script in our examples repository [here](https://github.com/triggerdotdev/examples/blob/main/python-crawl4ai/src/python/crawl-url.py).
```python src/python/crawl-url.py
import asyncio
import sys
import os
from crawl4ai import *
from crawl4ai.async_configs import BrowserConfig
async def main(url: str):
# Get proxy configuration from environment variables
proxy_url = os.environ.get("PROXY_URL")
proxy_username = os.environ.get("PROXY_USERNAME")
proxy_password = os.environ.get("PROXY_PASSWORD")
# Configure the proxy
browser_config = None
if proxy_url:
if proxy_username and proxy_password:
# Use authenticated proxy
proxy_config = {
"server": proxy_url,
"username": proxy_username,
"password": proxy_password
}
browser_config = BrowserConfig(proxy_config=proxy_config)
else:
# Use simple proxy
browser_config = BrowserConfig(proxy=proxy_url)
else:
browser_config = BrowserConfig()
async with AsyncWebCrawler(config=browser_config) as crawler:
result = await crawler.arun(
url=url,
)
print(result.markdown)
if __name__ == "__main__":
if len(sys.argv) < 2:
print("Usage: python crawl-url.py <url>")
sys.exit(1)
url = sys.argv[1]
asyncio.run(main(url))
```
## Testing your task
1. Create a virtual environment `python -m venv venv`
2. Activate the virtual environment, depending on your OS: On Mac/Linux: `source venv/bin/activate`, on Windows: `venv\Scripts\activate`
3. Install the Python dependencies `pip install -r requirements.txt`
4. If you haven't already, copy your project ref from your [Trigger.dev dashboard](https://cloud.trigger.dev) and add it to the `trigger.config.ts` file.
5. Run the Trigger.dev CLI `dev` command (it may ask you to authorize the CLI if you haven't already).
6. Test the task in the dashboard, using a URL of your choice.
<Warning>
**WEB SCRAPING:** When web scraping, you MUST use a proxy to comply with our terms of service. Direct scraping of third-party websites without the site owner's permission using Trigger.dev Cloud is prohibited and will result in account suspension. See [this example](/guides/examples/puppeteer#scrape-content-from-a-web-page) which uses a proxy.
</Warning>
## Deploying your task
Deploy the task to production using the Trigger.dev CLI `deploy` command.
## Learn more about using Python with Trigger.dev
<Card title="Python build extension" icon="code" href="/config/extensions/pythonExtension">
Learn how to use our built-in Python build extension to install dependencies and run your Python
code.
</Card>
# Python image processing example
Source: https://trigger.dev/docs/guides/python/python-image-processing
Learn how to use Trigger.dev with Python to process images from URLs and upload them to S3.
## Overview
This demo showcases how to use Trigger.dev with Python to process an image using Pillow (PIL) from a URL and upload it to S3-compatible storage bucket.
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [Python](https://www.python.org/) installed on your local machine
## Features
* A [Trigger.dev](https://trigger.dev) task to trigger the image processing Python script, and then upload the processed image to S3-compatible storage
* The [Trigger.dev Python build extension](https://trigger.dev/docs/config/extensions/pythonExtension) to install dependencies and run Python scripts
* [Pillow (PIL)](https://pillow.readthedocs.io/) for powerful image processing capabilities
* [AWS SDK v3](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/client/s3/) for S3 uploads
* S3-compatible storage support (AWS S3, Cloudflare R2, etc.)
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/tree/main/python-image-processing">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## The code
### Build configuration
After you've initialized your project with Trigger.dev, add these build settings to your `trigger.config.ts` file:
```ts trigger.config.ts
import { pythonExtension } from "@trigger.dev/python/extension";
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
runtime: "node",
project: "<your-project-ref>",
// Your other config settings...
build: {
extensions: [
pythonExtension({
// The path to your requirements.txt file
requirementsFile: "./requirements.txt",
// The path to your Python binary
devPythonBinaryPath: `venv/bin/python`,
// The paths to your Python scripts to run
scripts: ["src/python/**/*.py"],
}),
],
},
});
```
<Info>
Learn more about executing scripts in your Trigger.dev project using our Python build extension
[here](/config/extensions/pythonExtension).
</Info>
### Task code
This task uses the `python.runScript` method to run the `image-processing.py` script with the given image URL as an argument. You can adjust the image processing parameters in the payload, with options such as height, width, quality, output format, etc.
```ts src/trigger/processImage.ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { z } from "zod";
import { python } from "@trigger.dev/python";
import { promises as fs } from "fs";
import { S3Client } from "@aws-sdk/client-s3";
import { Upload } from "@aws-sdk/lib-storage";
// Initialize S3 client
const s3Client = new S3Client({
region: "auto",
endpoint: process.env.S3_ENDPOINT,
credentials: {
accessKeyId: process.env.S3_ACCESS_KEY_ID ?? "",
secretAccessKey: process.env.S3_SECRET_ACCESS_KEY ?? "",
},
});
// Define the input schema with Zod
const imageProcessingSchema = z.object({
imageUrl: z.string().url(),
height: z.number().positive().optional().default(800),
width: z.number().positive().optional().default(600),
quality: z.number().min(1).max(100).optional().default(85),
maintainAspectRatio: z.boolean().optional().default(true),
outputFormat: z.enum(["jpeg", "png", "webp", "gif", "avif"]).optional().default("jpeg"),
brightness: z.number().optional(),
contrast: z.number().optional(),
sharpness: z.number().optional(),
grayscale: z.boolean().optional().default(false),
});
// Define the output schema
const outputSchema = z.object({
url: z.string().url(),
key: z.string(),
format: z.string(),
originalSize: z.object({
width: z.number(),
height: z.number(),
}),
newSize: z.object({
width: z.number(),
height: z.number(),
}),
fileSizeBytes: z.number(),
exitCode: z.number(),
});
export const processImage = schemaTask({
id: "process-image",
schema: imageProcessingSchema,
run: async (payload, io) => {
const {
imageUrl,
height,
width,
quality,
maintainAspectRatio,
outputFormat,
brightness,
contrast,
sharpness,
grayscale,
} = payload;
try {
// Run the Python script
const result = await python.runScript("./src/python/image-processing.py", [
imageUrl,
height.toString(),
width.toString(),
quality.toString(),
maintainAspectRatio.toString(),
outputFormat,
brightness?.toString() || "null",
contrast?.toString() || "null",
sharpness?.toString() || "null",
grayscale.toString(),
]);
const { outputPath, format, originalSize, newSize, fileSizeBytes } = JSON.parse(
result.stdout
);
// Read file once
const fileContent = await fs.readFile(outputPath);
try {
// Upload to S3
const key = `processed-images/${Date.now()}-${outputPath.split("/").pop()}`;
await new Upload({
client: s3Client,
params: {
Bucket: process.env.S3_BUCKET!,
Key: key,
Body: fileContent,
ContentType: `image/${format}`,
},
}).done();
return {
url: `${process.env.S3_PUBLIC_URL}/${key}`,
key,
format,
originalSize,
newSize,
fileSizeBytes,
exitCode: result.exitCode,
};
} finally {
// Always clean up the temp file
await fs.unlink(outputPath).catch(console.error);
}
} catch (error) {
throw new Error(
`Processing failed: ${error instanceof Error ? error.message : "Unknown error"}`
);
}
},
});
```
### Add a requirements.txt file
Add the following to your `requirements.txt` file. This is required in Python projects to install the dependencies.
```txt requirements.txt
# Core dependencies
Pillow==10.2.0            # Image processing library
python-dotenv==1.0.0      # Environment variable management
requests==2.31.0          # HTTP requests
numpy==1.26.3             # Numerical operations (for advanced processing)
# Optional enhancements
opencv-python==4.8.1.78   # For more advanced image processing
```
### The Python script
The Python script uses Pillow (PIL) to process an image. You can see the original script in our examples repository [here](https://github.com/triggerdotdev/examples/blob/main/python-image-processing/src/python/image-processing.py).
```python src/python/image-processing.py
from PIL import Image, ImageOps, ImageEnhance
import io
from io import BytesIO
import os
from typing import Tuple, List, Dict, Optional, Union
import logging
import sys
import json
import requests
# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
class ImageProcessor:
"""Image processing utility for resizing, optimizing, and converting images."""
# Supported formats for conversion
SUPPORTED_FORMATS = ['JPEG', 'PNG', 'WEBP', 'GIF', 'AVIF']
@staticmethod
def open_image(image_data: Union[bytes, str]) -> Image.Image:
"""Open an image from bytes or file path."""
try:
if isinstance(image_data, bytes):
return Image.open(io.BytesIO(image_data))
else:
return Image.open(image_data)
except Exception as e:
logger.error(f"Failed to open image: {e}")
raise ValueError(f"Could not open image: {e}")
@staticmethod
def resize_image(
img: Image.Image,
width: Optional[int] = None,
height: Optional[int] = None,
maintain_aspect_ratio: bool = True
) -> Image.Image:
"""
Resize an image to specified dimensions.
Args:
img: PIL Image object
width: Target width (None to auto-calculate from height)
height: Target height (None to auto-calculate from width)
maintain_aspect_ratio: Whether to maintain the original aspect ratio
Returns:
Resized PIL Image
"""
if width is None and height is None:
return img  # No resize needed
original_width, original_height = img.size
if maintain_aspect_ratio:
if width and height:
# Calculate the best fit while maintaining aspect ratio
ratio = min(width / original_width, height / original_height)
new_width = int(original_width * ratio)
new_height = int(original_height * ratio)
elif width:
# Calculate height based on width
ratio = width / original_width
new_width = width
new_height = int(original_height * ratio)
else:
# Calculate width based on height
ratio = height / original_height
new_width = int(original_width * ratio)
new_height = height
else:
# Force exact dimensions
new_width = width if width else original_width
new_height = height if height else original_height
return img.resize((new_width, new_height), Image.LANCZOS)
@staticmethod
def optimize_image(
img: Image.Image,
quality: int = 85,
format: Optional[str] = None
) -> Tuple[bytes, str]:
"""
Optimize an image for web delivery.
Args:
img: PIL Image object
quality: JPEG/WebP quality (0-100)
format: Output format (JPEG, PNG, WEBP, etc.)
Returns:
Tuple of (image_bytes, format)
"""
if format is None:
format = img.format or 'JPEG'
format = format.upper()
if format not in ImageProcessor.SUPPORTED_FORMATS:
format = 'JPEG'  # Default to JPEG if unsupported format
# Convert mode if needed
if format == 'JPEG' and img.mode in ('RGBA', 'P'):
img = img.convert('RGB')
# Save to bytes
buffer = io.BytesIO()
if format == 'JPEG':
img.save(buffer, format=format, quality=quality, optimize=True)
elif format == 'PNG':
img.save(buffer, format=format, optimize=True)
elif format == 'WEBP':
img.save(buffer, format=format, quality=quality)
elif format == 'AVIF':
img.save(buffer, format=format, quality=quality)
else:
img.save(buffer, format=format)
buffer.seek(0)
return buffer.getvalue(), format.lower()
@staticmethod
def apply_filters(
img: Image.Image,
brightness: Optional[float] = None,
contrast: Optional[float] = None,
sharpness: Optional[float] = None,
grayscale: bool = False
) -> Image.Image:
"""
Apply various filters and enhancements to an image.
Args:
img: PIL Image object
brightness: Brightness factor (0.0-2.0, 1.0 is original)
contrast: Contrast factor (0.0-2.0, 1.0 is original)
sharpness: Sharpness factor (0.0-2.0, 1.0 is original)
grayscale: Convert to grayscale if True
Returns:
Processed PIL Image
"""
# Apply grayscale first if requested
if grayscale:
img = ImageOps.grayscale(img)
# Convert back to RGB if other filters will be applied
if any(x is not None for x in [brightness, contrast, sharpness]):
img = img.convert('RGB')
# Apply enhancements
if brightness is not None:
img = ImageEnhance.Brightness(img).enhance(brightness)
if contrast is not None:
img = ImageEnhance.Contrast(img).enhance(contrast)
if sharpness is not None:
img = ImageEnhance.Sharpness(img).enhance(sharpness)
return img
@staticmethod
def process_image(
image_data: Union[bytes, str],
width: Optional[int] = None,
height: Optional[int] = None,
maintain_aspect_ratio: bool = True,
quality: int = 85,
output_format: Optional[str] = None,
brightness: Optional[float] = None,
contrast: Optional[float] = None,
sharpness: Optional[float] = None,
grayscale: bool = False
) -> Dict:
"""
Process an image with all available options.
Args:
image_data: Image bytes or file path
width: Target width
height: Target height
maintain_aspect_ratio: Whether to maintain aspect ratio
quality: Output quality
output_format: Output format
brightness: Brightness adjustment
contrast: Contrast adjustment
sharpness: Sharpness adjustment
grayscale: Convert to grayscale
Returns:
Dict with processed image data and metadata
"""
# Open the image
img = ImageProcessor.open_image(image_data)
original_format = img.format
original_size = img.size
# Apply filters
img = ImageProcessor.apply_filters(
img,
brightness=brightness,
contrast=contrast,
sharpness=sharpness,
grayscale=grayscale
)
# Resize if needed
if width or height:
img = ImageProcessor.resize_image(
img,
width=width,
height=height,
maintain_aspect_ratio=maintain_aspect_ratio
)
# Optimize and get bytes
processed_bytes, actual_format = ImageProcessor.optimize_image(
img,
quality=quality,
format=output_format
)
# Return result with metadata
return {
"processed_image": processed_bytes,
"format": actual_format,
"original_format": original_format,
"original_size": original_size,
"new_size": img.size,
"file_size_bytes": len(processed_bytes)
}
def process_image(url, height, width, quality):
# Download image from URL
response = requests.get(url)
img = Image.open(BytesIO(response.content))
# Resize
img = img.resize((int(width), int(height)), Image.Resampling.LANCZOS)
# Save with quality setting
output_path = f"/tmp/processed_{width}x{height}.jpg"
img.save(output_path, "JPEG", quality=int(quality))
return output_path
if __name__ == "__main__":
url = sys.argv[1]
height = int(sys.argv[2])
width = int(sys.argv[3])
quality = int(sys.argv[4])
maintain_aspect_ratio = sys.argv[5].lower() == 'true'
output_format = sys.argv[6]
brightness = float(sys.argv[7]) if sys.argv[7] != 'null' else None
contrast = float(sys.argv[8]) if sys.argv[8] != 'null' else None
sharpness = float(sys.argv[9]) if sys.argv[9] != 'null' else None
grayscale = sys.argv[10].lower() == 'true'
processor = ImageProcessor()
result = processor.process_image(
requests.get(url).content,
width=width,
height=height,
maintain_aspect_ratio=maintain_aspect_ratio,
quality=quality,
output_format=output_format,
brightness=brightness,
contrast=contrast,
sharpness=sharpness,
grayscale=grayscale
)
output_path = f"/tmp/processed_{width}x{height}.{result['format']}"
with open(output_path, 'wb') as f:
f.write(result['processed_image'])
print(json.dumps({
"outputPath": output_path,
"format": result['format'],
"originalSize": result['original_size'],
"newSize": result['new_size'],
"fileSizeBytes": result['file_size_bytes']
}))
```
## Testing your task
1. Create a virtual environment `python -m venv venv`
2. Activate the virtual environment, depending on your OS: On Mac/Linux: `source venv/bin/activate`, on Windows: `venv\Scripts\activate`
3. Install the Python dependencies `pip install -r requirements.txt`
4. Set up your S3-compatible storage credentials in your environment variables, in .env for local development, or in the Trigger.dev dashboard for production:
```
S3_ENDPOINT=https://your-endpoint.com
S3_ACCESS_KEY_ID=your-access-key
S3_SECRET_ACCESS_KEY=your-secret-key
S3_BUCKET=your-bucket-name
S3_PUBLIC_URL=https://your-public-url.com
```
5. Copy the project ref from your [Trigger.dev dashboard](https://cloud.trigger.dev) and add it to the `trigger.config.ts` file.
6. Run the Trigger.dev CLI `dev` command (it may ask you to authorize the CLI if you haven't already).
7. Test the task in the dashboard by providing a valid image URL and processing options.
8. Deploy the task to production using the Trigger.dev CLI `deploy` command.
## Example Payload
These are all optional parameters that can be passed to the `image-processing.py` Python script from the `processImage.ts` task.
```json
{
"imageUrl": "<your-image-url>",
"height": 1200,
"width": 900,
"quality": 90,
"maintainAspectRatio": true,
"outputFormat": "webp",
"brightness": 1.2,
"contrast": 1.1,
"sharpness": 1.3,
"grayscale": false
}
```
## Deploying your task
Deploy the task to production using the CLI command `npx trigger.dev@latest deploy`
## Learn more about using Python with Trigger.dev
<Card title="Python build extension" icon="code" href="/config/extensions/pythonExtension">
Learn how to use our built-in Python build extension to install dependencies and run your Python
code.
</Card>
# Python PDF form extractor example
Source: https://trigger.dev/docs/guides/python/python-pdf-form-extractor
Learn how to use Trigger.dev with Python to extract form data from PDF files.
## Overview
This demo showcases how to use Trigger.dev with Python to extract structured form data from a PDF file available at a URL.
## Prerequisites
* A project with [Trigger.dev initialized](/quick-start)
* [Python](https://www.python.org/) installed on your local machine
## Features
* A [Trigger.dev](https://trigger.dev) task to trigger the Python script
* [Trigger.dev Python build extension](https://trigger.dev/docs/config/extensions/pythonExtension) to install the dependencies and run the Python script
* [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) to extract form data from PDF files
* [Requests](https://docs.python-requests.org/en/master/) to download PDF files from URLs
## GitHub repo
<Card title="View the project on GitHub" icon="GitHub" href="https://github.com/triggerdotdev/examples/edit/main/python-pdf-form-extractor/">
Click here to view the full code for this project in our examples repository on GitHub. You can
fork it and use it as a starting point for your own project.
</Card>
## The code
### Build configuration
After you've initialized your project with Trigger.dev, add these build settings to your `trigger.config.ts` file:
```ts trigger.config.ts
import { pythonExtension } from "@trigger.dev/python/extension";
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
runtime: "node",
project: "<your-project-ref>",
// Your other config settings...
build: {
extensions: [
pythonExtension({
// The path to your requirements.txt file
requirementsFile: "./requirements.txt",
// The path to your Python binary
devPythonBinaryPath: `venv/bin/python`,
// The paths to your Python scripts to run
scripts: ["src/python/**/*.py"],
}),
],
},
});
```
<Info>
Learn more about executing scripts in your Trigger.dev project using our Python build extension
[here](/config/extensions/pythonExtension).
</Info>
### Task code
This task uses the `python.runScript` method to run the `image-processing.py` script with the given image URL as an argument. You can adjust the image processing parameters in the payload, with options such as height, width, quality, output format, etc.
```ts src/trigger/pythonPdfTask.ts
import { task } from "@trigger.dev/sdk/v3";
import { python } from "@trigger.dev/python";
export const processPdfForm = task({
id: "process-pdf-form",
run: async (payload: { pdfUrl: string }, io: any) => {
const { pdfUrl } = payload;
const args = [pdfUrl];
const result = await python.runScript("./src/python/extract-pdf-form.py", args);
// Parse the JSON output from the script
let formData;
try {
formData = JSON.parse(result.stdout);
} catch (error) {
throw new Error(`Failed to parse JSON output: ${result.stdout}`);
}
return {
formData,
stderr: result.stderr,
exitCode: result.exitCode,
};
},
});
```
### Add a requirements.txt file
Add the following to your `requirements.txt` file. This is required in Python projects to install the dependencies.
```txt requirements.txt
PyMuPDF==1.23.8
requests==2.31.0
```
### The Python script
The Python script uses PyMuPDF to extract form data from a PDF file. You can see the original script in our examples repository [here](https://github.com/triggerdotdev/examples/blob/main/python-pdf-form-extractor/src/python/extract-pdf-form.py).
```python src/python/extract-pdf-form.py
import fitz  # PyMuPDF
import requests
import os
import json
import sys
from urllib.parse import urlparse
def download_pdf(url):
"""Download PDF from URL to a temporary file"""
response = requests.get(url)
response.raise_for_status()
# Get filename from URL or use default
filename = os.path.basename(urlparse(url).path) or "downloaded.pdf"
filepath = os.path.join("/tmp", filename)
with open(filepath, 'wb') as f:
f.write(response.content)
return filepath
def extract_form_data(pdf_path):
"""Extract form data from a PDF file."""
doc = fitz.open(pdf_path)
form_data = {}
for page_num, page in enumerate(doc):
fields = page.widgets()
for field in fields:
field_name = field.field_name or f"unnamed_field_{page_num}_{len(form_data)}"
field_type = field.field_type_string
field_value = field.field_value
# For checkboxes, convert to boolean
if field_type == "CheckBox":
field_value = field_value == "Yes"
form_data[field_name] = {
"type": field_type,
"value": field_value,
"page": page_num + 1
}
return form_data
def main():
if len(sys.argv) < 2:
print(json.dumps({"error": "PDF URL is required as an argument"}), file=sys.stderr)
return 1
url = sys.argv[1]
try:
pdf_path = download_pdf(url)
form_data = extract_form_data(pdf_path)
# Convert to JSON for structured output
structured_output = json.dumps(form_data, indent=2)
print(structured_output)
return 0
except Exception as e:
print(json.dumps({"error": str(e)}), file=sys.stderr)
return 1
if __name__ == "__main__":
sys.exit(main())
```
## Testing your task
1. Create a virtual environment `python -m venv venv`
2. Activate the virtual environment, depending on your OS: On Mac/Linux: `source venv/bin/activate`, on Windows: `venv\Scripts\activate`
3. Install the Python dependencies `pip install -r requirements.txt`
4. Copy the project ref from your [Trigger.dev dashboard](https://cloud.trigger.dev) and add it to the `trigger.config.ts` file.
5. Run the Trigger.dev CLI `dev` command (it may ask you to authorize the CLI if you haven't already).
6. Test the task in the dashboard by providing a valid PDF URL.
7. Deploy the task to production using the Trigger.dev CLI `deploy` command.
## Learn more about using Python with Trigger.dev
<Card title="Python build extension" icon="code" href="/config/extensions/pythonExtension">
Learn how to use our built-in Python build extension to install dependencies and run your Python
code.
</Card>
# Email us
Source: https://trigger.dev/docs/help-email
You can [email us](https://trigger.dev/contact) by filling out this form.
# Slack support
Source: https://trigger.dev/docs/help-slack
If you have a paid Trigger.dev account, you can request a private Slack Connect channel.
To do this:
1. Login to the [Trigger.dev web app](https://cloud.trigger.dev).
2. Subscribe to a paid plan if you haven't already.
3. In the bottom-left corner click "Join our Slack".
# How it works
Source: https://trigger.dev/docs/how-it-works
Understand how Trigger.dev works and how it can help you.
## Introduction
Trigger.dev v3 allows you to integrate long-running async tasks into your application and run them in the background. This allows you to offload tasks that take a long time to complete, such as sending multi-day email campaigns, processing videos, or running long chains of AI tasks.
For example, the below task processes a video with `ffmpeg` and sends the results to an s3 bucket, then updates a database with the results and sends an email to the user.
```ts /trigger/video.ts
import { logger, task } from "@trigger.dev/sdk/v3";
import { updateVideoUrl } from "../db.js";
import ffmpeg from "fluent-ffmpeg";
import { Readable } from "node:stream";
import type { ReadableStream } from "node:stream/web";
import * as fs from "node:fs/promises";
import * as path from "node:path";
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";
import { sendEmail } from "../email.js";
import { getVideo } from "../db.js";
// Initialize S3 client
const s3Client = new S3Client({
region: process.env.AWS_REGION,
});
export const convertVideo = task({
id: "convert-video",
retry: {
maxAttempts: 5,
minTimeoutInMs: 1000,
maxTimeoutInMs: 10000,
factor: 2,
},
run: async ({ videoId }: { videoId: string }) => {
const { url, userId } = await getVideo(videoId);
const outputPath = path.join("/tmp", `output_${videoId}.mp4`);
const response = await fetch(url);
await new Promise((resolve, reject) => {
ffmpeg(Readable.fromWeb(response.body as ReadableStream))
.videoFilters("scale=iw/2:ih/2")
.output(outputPath)
.on("end", resolve)
.on("error", reject)
.run();
});
const processedContent = await fs.readFile(outputPath);
// Upload to S3
const s3Key = `processed-videos/output_${videoId}.mp4`;
const uploadParams = {
Bucket: process.env.S3_BUCKET,
Key: s3Key,
Body: processedContent,
};
await s3Client.send(new PutObjectCommand(uploadParams));
const s3Url = `https://${process.env.S3_BUCKET}.s3.amazonaws.com/${s3Key}`;
logger.info("Video converted", { videoId, s3Url });
// Update database
await updateVideoUrl(videoId, s3Url);
await sendEmail(
userId,
"Video Processing Complete",
`Your video has been processed and is available at: ${s3Url}`
);
return { success: true, s3Url };
},
});
```
Now in your application, you can trigger this task by calling:
```ts
import { NextResponse } from "next/server";
import { tasks } from "@trigger.dev/sdk/v3";
import type { convertVideo } from "./trigger/video";
//     ðŸ‘† **type-only** import
export async function POST(request: Request) {
const body = await request.json();
// Trigger the task, this will return before the task is completed
const handle = await tasks.trigger<typeof convertVideo>("convert-video", body);
return NextResponse.json(handle);
}
```
This will schedule the task to run in the background and return a handle that you can use to check the status of the task. This allows your backend application to respond quickly to the user and offload the long-running task to Trigger.dev.
## The CLI
Trigger.dev comes with a CLI that allows you to initialize Trigger.dev into your project, deploy your tasks, and run your tasks locally. You can run it via `npx` like so:
```sh
npx trigger.dev@latest login # Log in to your Trigger.dev account
npx trigger.dev@latest init # Initialize Trigger.dev in your project
npx trigger.dev@latest dev # Run your tasks locally
npx trigger.dev@latest deploy # Deploy your tasks to the Trigger.dev instance
```
All these commands work with the Trigger.dev cloud and/or your self-hosted instance. It supports multiple profiles so you can easily switch between different accounts or instances.
```sh
npx trigger.dev@latest login --profile <profile> -a https://trigger.example.com # Log in to a specific profile into a self-hosted instance
npx trigger.dev@latest dev --profile <profile> # Initialize Trigger.dev in your project
npx trigger.dev@latest deploy --profile <profile> # Deploy your tasks to the Trigger.dev instance
```
## Trigger.dev architecture
Trigger.dev implements a serverless architecture (without timeouts!) that allows you to run your tasks in a scalable and reliable way. When you run `npx trigger.dev@latest deploy`, we build and deploy your task code to your Trigger.dev instance. Then, when you trigger a task from your application, it's run in a secure, isolated environment with the resources you need to complete the task. A simplified diagram for a task execution looks like this:
```mermaid
sequenceDiagram
participant App
participant Trigger.dev
participant Task Worker
App->>Trigger.dev: Trigger task
Trigger.dev-->>App: Task handle
Trigger.dev->>Task Worker: Run task
Task Worker-->>Trigger.dev: Task completed
```
In reality there are many more components involved, such as the task queue, the task scheduler, and the task worker pool, logging (etc.), but this diagram gives you a high-level overview of how Trigger.dev works.
## The Checkpoint-Resume System
Trigger.dev implements a powerful Checkpoint-Resume System that enables efficient execution of long-running background tasks in a serverless-like environment. This system allows tasks to pause, checkpoint their state, and resume seamlessly, optimizing resource usage and enabling complex workflows.
Here's how the Checkpoint-Resume System works:
1. **Task Execution**: When a task is triggered, it runs in an isolated environment with all necessary resources.
2. **Subtask Handling**: If a task needs to trigger a subtask, it can do so and wait for its completion using `triggerAndWait`
3. **State Checkpointing**: While waiting for a subtask or during a programmed pause (e.g., `wait.for({ seconds: 30 })`), the system uses CRIU (Checkpoint/Restore In Userspace) to create a checkpoint of the task's entire state, including memory, CPU registers, and open file descriptors.
4. **Resource Release**: After checkpointing, the parent task's resources are released, freeing up the execution environment.
5. **Efficient Storage**: The checkpoint is efficiently compressed and stored on disk, ready to be restored when needed.
6. **Event-Driven Resumption**: When a subtask completes or a wait period ends, Trigger.dev's event system triggers the restoration process.
7. **State Restoration**: The checkpoint is loaded back into a new execution environment, restoring the task to its exact state before suspension.
8. **Seamless Continuation**: The task resumes execution from where it left off, with any subtask results or updated state seamlessly integrated.
This approach allows Trigger.dev to manage resources efficiently, handle complex task dependencies, and provide a virtually limitless execution time for your tasks, all while maintaining the simplicity and scalability of a serverless architecture.
Example of a parent and child task using the Checkpoint-Resume System:
```ts
import { task, wait } from "@trigger.dev/sdk/v3";
export const parentTask = task({
id: "parent-task",
run: async () => {
console.log("Starting parent task");
// This will cause the parent task to be checkpointed and suspended
const result = await childTask.triggerAndWait({ data: "some data" });
console.log("Child task result:", result);
// This will also cause the task to be checkpointed and suspended
await wait.for({ seconds: 30 });
console.log("Resumed after 30 seconds");
return "Parent task completed";
},
});
export const childTask = task({
id: "child-task",
run: async (payload: { data: string }) => {
console.log("Starting child task with data:", payload.data);
// Simulate some work
await sleep(5);
return "Child task result";
},
});
```
The diagram below illustrates the flow of the parent and child tasks using the Checkpoint-Resume System:
```mermaid
sequenceDiagram
participant App
participant Trigger.dev
participant Parent Task
participant Child Task
participant CR System
participant Storage
App->>Trigger.dev: Trigger parent task
Trigger.dev->>Parent Task: Start execution
Parent Task->>Child Task: Trigger child task
Parent Task->>CR System: Request snapshot
CR System->>Storage: Store snapshot
CR System-->>Parent Task: Confirm snapshot stored
Parent Task->>Trigger.dev: Release resources
Child Task->>Trigger.dev: Complete execution
Trigger.dev->>CR System: Request parent task restoration
CR System->>Storage: Retrieve snapshot
CR System->>Parent Task: Restore state
Parent Task->>Trigger.dev: Resume execution
Parent Task->>Trigger.dev: Complete execution
```
<Note>
This is why, in the Trigger.dev Cloud, we don't charge for the time waiting for subtasks or the
time spent in a paused state.
</Note>
## Durable execution
Trigger.dev's Checkpoint-Resume System, combined with idempotency keys, enables durable execution of complex workflows. This approach allows for efficient retries and caching of results, ensuring that work is not unnecessarily repeated in case of failures.
### How it works
1. **Task breakdown**: Complex workflows are broken down into smaller, independent subtasks.
2. **Idempotency keys**: Each subtask is assigned a unique idempotency key.
3. **Result caching**: The output of each subtask is cached based on its idempotency key.
4. **Intelligent retries**: If a failure occurs, only the failed subtask and subsequent tasks are retried.
### Example: Video processing workflow
Let's rewrite the `convert-video` task above to be more durable:
<CodeGroup>
```ts /trigger/video.ts
import { idempotencyKeys, logger, task } from "@trigger.dev/sdk/v3";
import { processVideo, sendUserEmail, uploadToS3 } from "./tasks.js";
import { updateVideoUrl } from "../db.js";
export const convertVideo = task({
id: "convert-video",
retry: {
maxAttempts: 5,
minTimeoutInMs: 1000,
maxTimeoutInMs: 10000,
factor: 2,
},
run: async ({ videoId }: { videoId: string }) => {
// Automatically scope the idempotency key to this run, across retries
const idempotencyKey = await idempotencyKeys.create(videoId);
// Process video
const { processedContent } = await processVideo
.triggerAndWait({ videoId }, { idempotencyKey })
.unwrap(); // Calling unwrap will return the output of the subtask, or throw an error if the subtask failed
// Upload to S3
const { s3Url } = await uploadToS3
.triggerAndWait({ processedContent, videoId }, { idempotencyKey })
.unwrap();
// Update database
await updateVideoUrl(videoId, s3Url);
// Send email, we don't need to wait for this to finish
await sendUserEmail.trigger({ videoId, s3Url }, { idempotencyKey });
return { success: true, s3Url };
},
});
```
```ts /trigger/tasks.ts
import { task, logger } from "@trigger.dev/sdk/v3";
import ffmpeg from "fluent-ffmpeg";
import { Readable } from "node:stream";
import type { ReadableStream } from "node:stream/web";
import * as fs from "node:fs/promises";
import * as path from "node:path";
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";
import { sendEmail } from "../email.js";
import { getVideo } from "../db.js";
// Initialize S3 client
const s3Client = new S3Client({
region: process.env.AWS_REGION,
});
export const processVideo = task({
id: "process-video",
run: async ({ videoId }: { videoId: string }) => {
const { url } = await getVideo(videoId);
const outputPath = path.join("/tmp", `output_${videoId}.mp4`);
const response = await fetch(url);
await logger.trace("ffmpeg", async (span) => {
await new Promise((resolve, reject) => {
ffmpeg(Readable.fromWeb(response.body as ReadableStream))
.videoFilters("scale=iw/2:ih/2")
.output(outputPath)
.on("end", resolve)
.on("error", reject)
.run();
});
});
const processedContent = await fs.readFile(outputPath);
await fs.unlink(outputPath);
return { processedContent: processedContent.toString("base64") };
},
});
export const uploadToS3 = task({
id: "upload-to-s3",
run: async (payload: { processedContent: string; videoId: string }) => {
const { processedContent, videoId } = payload;
const s3Key = `processed-videos/output_${videoId}.mp4`;
const uploadParams = {
Bucket: process.env.S3_BUCKET,
Key: s3Key,
Body: Buffer.from(processedContent, "base64"),
};
await s3Client.send(new PutObjectCommand(uploadParams));
const s3Url = `https://${process.env.S3_BUCKET}.s3.amazonaws.com/${s3Key}`;
return { s3Url };
},
});
export const sendUserEmail = task({
id: "send-user-email",
run: async ({ videoId, s3Url }: { videoId: string; s3Url: string }) => {
const { userId } = await getVideo(videoId);
return await sendEmail(
userId,
"Video Processing Complete",
`Your video has been processed and is available at: ${s3Url}`
);
},
});
```
</CodeGroup>
### How retries work
Let's say the email sending fails in our video processing workflow. Here's how the retry process works:
1. The main task throws an error and is scheduled for retry.
2. When retried, it starts from the beginning, but leverages cached results for completed subtasks.
Here's a sequence diagram illustrating this process:
```mermaid
sequenceDiagram
participant Main as Main Task
participant Process as Process Video
participant Upload as Upload to S3
participant DB as Update Database
participant Email as Send Email
Main->>Process: triggerAndWait (1st attempt)
Process-->>Main: Return result
Main->>Upload: triggerAndWait (1st attempt)
Upload-->>Main: Return result
Main->>DB: Update
Main->>Email: triggerAndWait (1st attempt)
Email--xMain: Fail
Main-->>Main: Schedule retry
Main->>Process: triggerAndWait (2nd attempt)
Process-->>Main: Return cached result
Main->>Upload: triggerAndWait (2nd attempt)
Upload-->>Main: Return cached result
Main->>DB: Update (idempotent)
Main->>Email: triggerAndWait (2nd attempt)
Email-->>Main: Success
```
## The build system
When you run `npx trigger.dev@latest deploy` or `npx trigger.dev@latest dev`, we build your task code using our build system, which is powered by [esbuild](https://esbuild.github.io/). When deploying, the code is packaged up into a Docker image and deployed to your Trigger.dev instance. When running in dev mode, the code is built and run locally on your machine. Some features of our build system include:
* **Bundled by default**: Code + dependencies are bundled and tree-shaked by default.
* **Build extensions**: Use and write custom build extensions to transform your code or the resulting docker image.
* **ESM ouput**: We output to ESM, which allows tree-shaking and better performance.
You can review the build output by running deploy with the `--dry-run` flag, which will output the Containerfile and the build output.
Learn more about working with our build system in the [configuration docs](/config/config-file).
## Dev mode
When you run `npx trigger.dev@latest dev`, we run your task code locally on your machine. All scheduling is still done in the Trigger.dev server instance, but the task code is run locally. This allows you to develop and test your tasks locally before deploying them to the cloud, and is especially useful for debugging and testing.
* The same build system is used in dev mode, so you can be sure that your code will run the same locally as it does in the cloud.
* Changes are automatically detected and a new version is spun up when you save your code.
* Add debuggers and breakpoints to your code and debug it locally.
* Each task is run in a separate process, so you can run multiple tasks in parallel.
* Auto-cancels tasks when you stop the dev server.
<Note>
Trigger.dev currently does not support "offline" dev mode, where you can run tasks without an
internet connection. [Please let us know](https://feedback.trigger.dev/) if this is a feature you
want/need.
</Note>
## Staging and production environments
Trigger.dev supports deploying to multiple "deployed" environments, such as staging and production. This allows you to test your tasks in a staging environment before deploying them to production. You can deploy to a new environment by running `npx trigger.dev@latest deploy --env <env>`, where `<env>` is the name of the environment you want to deploy to. Each environment has its own API Key, which you can use to trigger tasks in that environment.
## OpenTelemetry
The Trigger.dev logging and task dashboard is powered by OpenTelemetry traces and logs, which allows you to trace your tasks and auto-instrument your code. We also auto-correlate logs from subtasks and parent tasks, making it easy view the entire trace of a task execution. A single run of the video processing task above looks like this in the dashboard:
![OpenTelemetry trace](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/opentelemetry-trace.png)
Because we use standard OpenTelemetry, you can instrument your code and OpenTelemetry compatible libraries to get detailed traces and logs of your tasks. The above trace instruments both Prisma and the AWS SDK:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { PrismaInstrumentation } from "@prisma/instrumentation";
import { AwsInstrumentation } from "@opentelemetry/instrumentation-aws-sdk";
export default defineConfig({
project: "<your-project-ref>",
instrumentations: [new PrismaInstrumentation(), new AwsInstrumentation()],
});
```
# Idempotency
Source: https://trigger.dev/docs/idempotency
An API call or operation is â€œidempotentâ€ if it has the same result when called more than once.
We currently support idempotency at the task level, meaning that if you trigger a task with the same `idempotencyKey` twice, the second request will not create a new task run.
<Warning>
In version 3.3.0 and later, the `idempotencyKey` option is not available when using
`triggerAndWait` or `batchTriggerAndWait`, due to a bug that would sometimes cause the parent task
to become stuck. We are working on a fix for this issue.
</Warning>
## `idempotencyKey` option
You can provide an `idempotencyKey` to ensure that a task is only triggered once with the same key. This is useful if you are triggering a task within another task that might be retried:
```ts
import { idempotencyKeys, task } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
retry: {
maxAttempts: 4,
},
run: async (payload: any) => {
// This idempotency key will be unique to this task run, meaning the childTask will only be triggered once across all retries
const idempotencyKey = await idempotencyKeys.create("my-task-key");
// childTask will only be triggered once with the same idempotency key
await childTask.trigger({ foo: "bar" }, { idempotencyKey });
// Do something else, that may throw an error and cause the task to be retried
throw new Error("Something went wrong");
},
});
```
You can use the `idempotencyKeys.create` SDK function to create an idempotency key before passing it to the `options` object.
We automatically inject the run ID when generating the idempotency key when running inside a task by default. You can turn it off by passing the `scope` option to `idempotencyKeys.create`:
```ts
import { idempotencyKeys, task } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
retry: {
maxAttempts: 4,
},
run: async (payload: any) => {
// This idempotency key will be globally unique, meaning only a single task run will be triggered with this key
const idempotencyKey = await idempotencyKeys.create("my-task-key", { scope: "global" });
// childTask will only be triggered once with the same idempotency key
await childTask.trigger({ foo: "bar" }, { idempotencyKey });
},
});
```
If you are triggering a task from your backend code, you can use the `idempotencyKeys.create` SDK function to create an idempotency key.
```ts
import { idempotencyKeys, tasks } from "@trigger.dev/sdk/v3";
// You can also pass an array of strings to create a idempotency key
const idempotencyKey = await idempotencyKeys.create([myUser.id, "my-task"]);
await tasks.trigger("my-task", { some: "data" }, { idempotencyKey });
```
You can also pass a string to the `idempotencyKey` option, without first creating it with `idempotencyKeys.create`.
```ts
import { myTask } from "./trigger/myTasks";
// You can also pass an array of strings to create a idempotency key
await myTask.trigger({ some: "data" }, { idempotencyKey: myUser.id });
```
<Note>Make sure you provide sufficiently unique keys to avoid collisions.</Note>
You can pass the `idempotencyKey` when calling `batchTrigger` as well:
```ts
import { tasks } from "@trigger.dev/sdk/v3";
await tasks.batchTrigger("my-task", [
{
payload: { some: "data" },
options: { idempotencyKey: await idempotencyKeys.create(myUser.id) },
},
]);
```
## `idempotencyKeyTTL` option
The `idempotencyKeyTTL` option defines a time window during which a task with the same idempotency key will only run once. Here's how it works:
1. When you trigger a task with an idempotency key and set `idempotencyKeyTTL: "5m"`, it creates a 5-minute window.
2. During this window, any subsequent triggers with the same idempotency key will return the original task run instead of creating a new one.
3. Once the TTL window expires, the next trigger with that idempotency key will create a new task run and start a new time window.
![idempotency-key-ttl](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/idempotency-key-ttl.png)
By default idempotency keys are stored for 30 days. You can change this by passing the `idempotencyKeyTTL` option when triggering a task:
```ts
import { idempotencyKeys, task, wait } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
retry: {
maxAttempts: 4,
},
run: async (payload: any) => {
const idempotencyKey = await idempotencyKeys.create("my-task-key");
// The idempotency key will expire after 60 seconds
await childTask.trigger({ foo: "bar" }, { idempotencyKey, idempotencyKeyTTL: "60s" });
await wait.for({ seconds: 61 });
// The idempotency key will have expired, so the childTask will be triggered again
await childTask.trigger({ foo: "bar" }, { idempotencyKey });
// Do something else, that may throw an error and cause the task to be retried
throw new Error("Something went wrong");
},
});
```
You can use the following units for the `idempotencyKeyTTL` option:
* `s` for seconds (e.g. `60s`)
* `m` for minutes (e.g. `5m`)
* `h` for hours (e.g. `2h`)
* `d` for days (e.g. `3d`)
## Payload-based idempotency
We don't currently support payload-based idempotency, but you can implement it yourself by hashing the payload and using the hash as the idempotency key.
```ts
import { idempotencyKeys, task } from "@trigger.dev/sdk/v3";
import { createHash } from "node:crypto";
// Somewhere in your code
const idempotencyKey = await idempotencyKeys.create(hash(childPayload));
// childTask will only be triggered once with the same idempotency key
await tasks.trigger("child-task", { some: "payload" }, { idempotencyKey });
// Create a hash of the payload using Node.js crypto
// Ideally, you'd do a stable serialization of the payload before hashing, to ensure the same payload always results in the same hash
function hash(payload: any): string {
const hash = createHash("sha256");
hash.update(JSON.stringify(payload));
return hash.digest("hex");
}
```
## Important notes
Idempotency keys, even the ones scoped globally, are actually scoped to the task and the environment. This means that you cannot collide with keys from other environments (e.g. dev will never collide with prod), or to other projects and orgs.
If you use the same idempotency key for triggering different tasks, the tasks will not be idempotent, and both tasks will be triggered. There's currently no way to make multiple tasks idempotent with the same key.
# Welcome to the Trigger.dev docs
Source: https://trigger.dev/docs/introduction
Find all the resources and guides you need to get started
<CardGroup cols={2}>
<Card title="Quick start" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-quickstart.jpg" href="/quick-start">
Get started with Trigger.dev in 3 minutes
</Card>
<Card title="Examples" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-examples.jpg" href="/guides/introduction#example-tasks">
Explore dozens of examples tasks to use in your own projects
</Card>
<Card title="Frameworks" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-frameworks.jpg" href="/guides/introduction#frameworks">
Learn how to use Trigger.dev with your favorite frameworks
</Card>
<Card title="Video walkthrough" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-video.jpg" href="/video-walkthrough">
Watch an end-to-end demo of Trigger.dev in 10 minutes
</Card>
</CardGroup>
## What is Trigger.dev?
Trigger.dev is an open source background jobs framework that lets you write reliable workflows in plain async code. Run long-running AI tasks, handle complex background jobs, and build AI agents with built-in queuing, automatic retries, and real-time monitoring. No timeouts, elastic scaling, and zero infrastructure management required.
We provide everything you need to build and manage background tasks: a CLI and SDK for writing tasks in your existing codebase, support for both [regular](/tasks/overview) and [scheduled](/tasks/scheduled) tasks, full observability through our dashboard, and a [Realtime API](/realtime) with [React hooks](/frontend/react-hooks#realtime-hooks) for showing task status in your frontend. You can use [Trigger.dev Cloud](https://cloud.trigger.dev) or [self-host](/open-source-self-hosting) on your own infrastructure.
## Learn the concepts
<CardGroup cols={2}>
<Card title="Writing tasks" icon="wand-magic-sparkles" href="/tasks/overview" color="#3B82F6">
Tasks are the core of Trigger.dev. Learn what they are and how to write them.
</Card>
<Card title="Triggering tasks" icon="bullseye-pointer" href="/triggering" color="#fbbf24">
Learn how to trigger tasks from your codebase.
</Card>
<Card title="Runs" icon="person-running" href="/runs" color="#EA189E">
Runs are the instances of tasks that are executed. Learn how they work.
</Card>
<Card title="API keys" icon="key" href="/apikeys" color="#EAEA08">
API keys are used to authenticate requests to the Trigger.dev API. Learn how to create and use them.
</Card>
</CardGroup>
## Explore by feature
<CardGroup>
<Card title="Scheduled tasks (cron)" icon="clock" href="/tasks/scheduled" color="#EAEA08">
Scheduled tasks are a type of task that is scheduled to run at a specific time.
</Card>
<Card title="Realtime API" icon="loader" href="/realtime" color="#22C55E">
The Realtime API allows you to trigger tasks and get the status of runs.
</Card>
<Card title="React hooks" icon="react" href="/frontend/react-hooks" color="#3B82F6">
React hooks are a way to show task status in your frontend.
</Card>
<Card title="Waits" icon="calendar-clock" href="/wait" color="#F59E0B">
Waits are a way to wait for a task to finish before continuing.
</Card>
<Card title="Errors and retries" icon="message-exclamation" href="/errors-retrying" color="#F43F5E">
Learn how to handle errors and retries.
</Card>
<Card title="Concurrency & Queues" icon="line-height" href="/queue-concurrency" color="#D946EF">
Configure what you want to happen when there is more than one run at a time.
</Card>
</CardGroup>
## Explore by example
<CardGroup cols={3}>
<Card title="FFmpeg" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-ffmpeg.jpg" href="/guides/examples/ffmpeg-video-processing" />
<Card title="Fal.ai" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-fal.jpg" href="/guides/examples/fal-ai-image-to-cartoon" />
<Card title="Puppeteer" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-puppeteer.jpg" href="/guides/examples/puppeteer" />
<Card title="LibreOffice" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-libreoffice.jpg" href="/guides/examples/libreoffice-pdf-conversion" />
<Card title="OpenAI" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-openai.jpg" href="/guides/examples/open-ai-with-retrying" />
<Card title="Browserbase" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-browserbase.jpg" href="/guides/examples/scrape-hacker-news" />
<Card title="Sentry" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-sentry.jpg" href="/guides/examples/sentry-error-tracking" />
<Card title="Resend" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-resend.jpg" href="/guides/examples/resend-email-sequence" />
<Card title="Vercel AI SDK" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-vercel.jpg" href="/guides/examples/vercel-ai-sdk" />
<Card title="Sharp" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-sharp.jpg" href="/guides/examples/sharp-image-processing" />
<Card title="Deepgram" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-deepgram.jpg" href="/guides/examples/deepgram-transcribe-audio" />
<Card title="Supabase" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-supabase.jpg" href="/guides/examples/supabase-database-operations" />
<Card title="DALLâ€¢E" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-openai.jpg" href="/guides/examples/dall-e3-generate-image" />
<Card title="Firecrawl" img="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/intro-firecrawl.jpg" href="/guides/examples/firecrawl-url-crawl" />
</CardGroup>
## Explore by build extension
| Extension             | What it does                                                 | Docs                                                   |
| :-------------------- | :----------------------------------------------------------- | :----------------------------------------------------- |
| prismaExtension       | Use Prisma with Trigger.dev                                  | [Learn more](/config/extensions/prismaExtension)       |
| pythonExtension       | Execute Python scripts in Trigger.dev                        | [Learn more](/config/extensions/pythonExtension)       |
| puppeteer             | Use Puppeteer with Trigger.dev                               | [Learn more](/config/extensions/puppeteer)             |
| ffmpeg                | Use FFmpeg with Trigger.dev                                  | [Learn more](/config/extensions/ffmpeg)                |
| aptGet                | Install system packages with aptGet                          | [Learn more](/config/extensions/aptGet)                |
| additionalFiles       | Copy additional files to the build directory                 | [Learn more](/config/extensions/additionalFiles)       |
| additionalPackages    | Include additional packages in the build                     | [Learn more](/config/extensions/additionalPackages)    |
| syncEnvVars           | Automatically sync environment variables to Trigger.dev      | [Learn more](/config/extensions/syncEnvVars)           |
| esbuildPlugin         | Add existing or custom esbuild plugins to your build process | [Learn more](/config/extensions/esbuildPlugin)         |
| emitDecoratorMetadata | Support for the emitDecoratorMetadata TypeScript compiler    | [Learn more](/config/extensions/emitDecoratorMetadata) |
| audioWaveform         | Support for Audio Waveform in your project                   | [Learn more](/config/extensions/audioWaveform)         |
## Getting help
We'd love to hear from you or give you a hand getting started. Here are some ways to get in touch with us.
<CardGroup>
<Card title="Join our Discord server" icon="discord" href="https://discord.gg/kA47vcd8P6" color="#5865F2">
Our Discord is the best place to get help with any questions about Trigger.dev.
</Card>
<Card
title="Follow us on X (Twitter)"
icon={
<svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 0 512 512">
<path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z" />
</svg>
}
href="https://twitter.com/triggerdotdev"
color="#1DA1F2"
>
Follow us to get the latest updates and news.
</Card>
<Card title="Schedule a call" icon="phone" iconType="solid" color="#22C55E" href="https://cal.com/team/triggerdotdev/founders-call">
Arrange a call with one of the founders to get help with any questions.
</Card>
<Card title="Give us a star on GitHub" icon="star" iconType="solid" href="https://github.com/triggerdotdev/trigger.dev" color="#fbbf24">
Check us out our GitHub repo and give us a star if you like what we're doing.
</Card>
</CardGroup>
# Limits
Source: https://trigger.dev/docs/limits
There are some hard and soft limits that you might hit.
## Concurrency limits
| Pricing tier | Limit                |
| :----------- | :------------------- |
| Free         | 5 concurrent runs    |
| Hobby        | 25 concurrent runs   |
| Pro          | 100+ concurrent runs |
If you need more than 100 concurrent runs on the Pro tier, you can request more by contacting us via [email](https://trigger.dev/contact) or [Discord](https://trigger.dev/discord).
## Rate limits
Generally speaking each SDK call is an API call.
| Limit | Details                   |
| :---- | :------------------------ |
| API   | 1,500 requests per minute |
The most common cause of hitting the API rate limit is if youâ€™re calling `trigger()` on a task in a loop, instead of doing this use `batchTrigger()` which will trigger multiple tasks in a single API call. You can have up to 100 tasks in a single batch trigger call.
## Queued tasks
The number of queued tasks by environment.
| Limit   | Details            |
| :------ | :----------------- |
| Dev     | At most 500        |
| Staging | At most 10 million |
| Prod    | At most 10 million |
## Schedules
| Pricing tier | Limit              |
| :----------- | :----------------- |
| Free         | 10 per project     |
| Hobby        | 100 per project    |
| Pro          | 1,000+ per project |
When attaching schedules to tasks we strongly recommend you add them [in our dashboard](/tasks/scheduled#attaching-schedules-in-the-dashboard) if they're "static". That way you can control them easily per environment.
If you add them [dynamically using code](/management/schedules/create) make sure you add a `deduplicationKey` so you don't add the same schedule to a task multiple times. If you don't your task will get triggered multiple times, it will cost you more, and you will hit the limit.
If you're creating schedules for your user you will definitely need to request more schedules from us.
## Task payloads and outputs
| Limit                  | Details                                       |
| :--------------------- | :-------------------------------------------- |
| Single trigger payload | Must not exceed 3MB                           |
| Batch trigger payload  | The total of all payloads must not exceed 5MB |
| Task outputs           | Must not exceed 10MB                          |
Payloads and outputs that exceed 512KB will be offloaded to object storage and a presigned URL will be provided to download the data when calling `runs.retrieve`. You don't need to do anything to handle this in your tasks however, as we will transparently upload/download these during operation.
## Batch size
A single batch can have a maximum of 500 items.
<SoftLimit />
## Log retention
| Pricing tier | Limit   |
| :----------- | :------ |
| Free         | 1 day   |
| Hobby        | 7 days  |
| Pro          | 30 days |
## Log size
We limit the size of logs to prevent oversized data potentially causing issues.
<Expandable title="log limits">
#### Attribute Limits
* Span Attribute Count Limit: 256
* Log Attribute Count Limit: 256
* Span Attribute Value Length Limit: 1028 characters
* Log Attribute Value Length Limit: 1028 characters
#### Event and Link Limits
* Span Event Count Limit: 10
* Link Count Limit: 2
* Attributes per Link Limit: 10
* Attributes per Event Limit: 10
#### I/O Packet Length Limit
128 KB (131,072 bytes)
#### Attribute Clipping Behavior
* Attributes exceeding the value length limit (1028 characters) are discarded.
* If the total number of attributes exceeds 256, additional attributes are not included.
#### Attribute Value Size Calculation
* Strings: Actual length of the string
* Numbers: 8 bytes
* Booleans: 4 bytes
* Arrays: Sum of the sizes of all elements
* Undefined or null: 0 bytes
</Expandable>
## Alerts
An alert destination is a single email address, Slack channel, or webhook URL that you want to send alerts to. If you're on the Pro plan and need more than the plan limit, you can request more by contacting us via [email](https://trigger.dev/contact) or [Discord](https://trigger.dev/discord).
| Pricing tier | Limit                   |
| :----------- | :---------------------- |
| Free         | 1 alert destination     |
| Hobby        | 3 alert destinations    |
| Pro          | 100+ alert destinations |
## Machines
The default machine is `small-1x` which has 0.5 vCPU and 0.5 GB of RAM. You can optionally configure a higher spec machine which will increase the cost of running the task but can also improve the performance of the task if it is CPU or memory bound.
See the [machine configurations](/machines#machine-configurations) for more details.
## Team members
| Pricing tier | Limit            |
| :----------- | :--------------- |
| Free         | 5 team members   |
| Hobby        | 5 team members   |
| Pro          | 25+ team members |
# Logging and tracing
Source: https://trigger.dev/docs/logging
How to use the built-in logging and tracing system.
![The run log](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-log.png)
The run log shows you exactly what happened in every run of your tasks. It is comprised of logs, traces and spans.
## Logs
You can use `console.log()`, `console.error()`, etc as normal and they will be shown in your run log. This is the standard function so you can use it as you would in any other JavaScript or TypeScript code. Logs from any functions/packages will also be shown.
### logger
We recommend that you use our `logger` object which creates structured logs. Structured logs will make it easier for you to search the logs to quickly find runs.
```ts /trigger/logging.ts
import { task, logger } from "@trigger.dev/sdk/v3";
export const loggingExample = task({
id: "logging-example",
run: async (payload: { data: Record<string, string> }) => {
//the first parameter is the message, the second parameter must be a key-value object (Record<string, unknown>)
logger.debug("Debug message", payload.data);
logger.log("Log message", payload.data);
logger.info("Info message", payload.data);
logger.warn("You've been warned", payload.data);
logger.error("Error message", payload.data);
},
});
```
## Tracing and spans
Tracing is a way to follow the flow of your code. It's very useful for debugging and understanding how your code is working, especially with long-running or complex tasks.
Trigger.dev uses OpenTelemetry tracing under the hood. With automatic tracing for many things like task triggering, task attempts, HTTP requests, and more.
| Name          | Description                      |
| :------------ | :------------------------------- |
| Task triggers | Task triggers                    |
| Task attempts | Task attempts                    |
| HTTP requests | HTTP requests made by your code. |
### Adding instrumentations
![The run log](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/auto-instrumentation.png)
You can [add instrumentations](/config/config-file#instrumentations). The Prisma one above will automatically trace all Prisma queries.
### Add custom traces
If you want to add custom traces to your code, you can use the `logger.trace` function. It will create a new OTEL trace and you can set attributes on it.
```ts
import { logger, task } from "@trigger.dev/sdk/v3";
export const customTrace = task({
id: "custom-trace",
run: async (payload) => {
//you can wrap code in a trace, and set attributes
const user = await logger.trace("fetch-user", async (span) => {
span.setAttribute("user.id", "1");
//...do stuff
//you can return a value
return {
id: "1",
name: "John Doe",
fetchedAt: new Date(),
};
});
const usersName = user.name;
},
});
```
# Machines
Source: https://trigger.dev/docs/machines
Configure the number of vCPUs and GBs of RAM you want the task to use.
The `machine` configuration is optional. Using higher spec machines will increase the cost of running the task but can also improve the performance of the task if it is CPU or memory bound.
```ts /trigger/heavy-task.ts
import {  task } from "@trigger.dev/sdk/v3";
export const heavyTask = task({
id: "heavy-task",
machine: "large-1x",
run: async ({ payload, ctx }) => {
//...
},
});
```
The default machine is `small-1x` which has 0.5 vCPU and 0.5 GB of RAM. You can change the default machine in your `trigger.config.ts` file:
```ts trigger.config.ts
import type { TriggerConfig } from "@trigger.dev/sdk/v3";
export const config: TriggerConfig = {
machine: "small-2x",
// ... other config
};
```
## Machine configurations
| Preset             | vCPU | Memory | Disk space |
| :----------------- | :--- | :----- | :--------- |
| micro              | 0.25 | 0.25   | 10GB       |
| small-1x (default) | 0.5  | 0.5    | 10GB       |
| small-2x           | 1    | 1      | 10GB       |
| medium-1x          | 1    | 2      | 10GB       |
| medium-2x          | 2    | 4      | 10GB       |
| large-1x           | 4    | 8      | 10GB       |
| large-2x           | 8    | 16     | 10GB       |
You can view the Trigger.dev cloud pricing for these machines [here](https://trigger.dev/pricing#computePricing).
## Overriding the machine when triggering
You can also override the task machine when you [trigger](/triggering) it:
```ts
await tasks.trigger<typeof heavyTask>(
"heavy-task",
{ message: "hello world" },
{ machine: "large-2x" }
);
```
This is useful when you know that a certain payload will require more memory than the default machine. For example, you know it's a larger file or a customer that has a lot of data.
## Out Of Memory (OOM) errors
Sometimes you might see one of your runs fail with an "Out Of Memory" error.
> TASK\_PROCESS\_OOM\_KILLED. Your task ran out of memory. Try increasing the machine specs. If this doesn't fix it there might be a memory leak.
We automatically detect common Out Of Memory errors, including when ffmpeg throws an error because it ran out of memory.
You can explicitly throw an Out Of Memory error in your task. This can be useful if you use a native package that detects it's going to run out of memory and then stops before it runs out. If you can detect this, you can then throw this error.
```ts /trigger/heavy-task.ts
import {  task } from "@trigger.dev/sdk/v3";
import { OutOfMemoryError } from "@trigger.dev/sdk/v3";
export const yourTask = task({
id: "your-task",
machine: "medium-1x",
run: async (payload: any, { ctx }) => {
//...
throw new OutOfMemoryError();
},
});
```
If OOM errors happen regularly you need to either optimize the memory-efficiency of your code, or increase the machine.
### Retrying with a larger machine
If you are seeing rare OOM errors, it might make sense to add a setting to your task to retry with a large machine when an OOM happens:
```ts /trigger/heavy-task.ts
import {  task } from "@trigger.dev/sdk/v3";
export const yourTask = task({
id: "your-task",
machine: "medium-1x",
retry: {
outOfMemory: {
machine: "large-1x",
},
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
<Note>
This will only retry the task if you get an OOM error. It won't permanently change the machine that a new run starts on, so if you consistently see OOM errors you should change the machine in the `machine` property.
</Note>
# Advanced usage
Source: https://trigger.dev/docs/management/advanced-usage
Advanced usage of the Trigger.dev management API
### Accessing raw HTTP responses
All API methods return a `Promise` subclass `ApiPromise` that includes helpers for accessing the underlying HTTP response:
```ts
import { runs } from "@trigger.dev/sdk/v3";
async function main() {
const { data: run, response: raw } = await runs.retrieve("run_1234").withResponse();
console.log(raw.status);
console.log(raw.headers);
const response = await runs.retrieve("run_1234").asResponse(); // Returns a Response object
console.log(response.status);
console.log(response.headers);
}
```
# Authentication
Source: https://trigger.dev/docs/management/authentication
Authenticating with the Trigger.dev management API
There are two methods of authenticating with the management API: using a secret key associated with a specific environment in a project (`secretKey`), or using a personal access token (`personalAccessToken`). Both methods should only be used in a backend server, as they provide full access to the project.
<Note>
There is a separate authentication strategy when making requests from your frontend application.
See the [Frontend guide](/frontend/overview) for more information. This guide is for backend usage
only.
</Note>
Certain API functions work with both authentication methods, but require different arguments depending on the method used. For example, the `runs.list` function can be called using either a `secretKey` or a `personalAccessToken`, but the `projectRef` argument is required when using a `personalAccessToken`:
```ts
import { configure, runs } from "@trigger.dev/sdk/v3";
// Using secretKey authentication
configure({
secretKey: process.env["TRIGGER_SECRET_KEY"], // starts with tr_dev_ or tr_prod_
});
function secretKeyExample() {
return runs.list({
limit: 10,
status: ["COMPLETED"],
});
}
// Using personalAccessToken authentication
configure({
secretKey: process.env["TRIGGER_ACCESS_TOKEN"], // starts with tr_pat_
});
function personalAccessTokenExample() {
// Notice the projectRef argument is required when using a personalAccessToken
return runs.list("prof_1234", {
limit: 10,
status: ["COMPLETED"],
projectRef: "tr_proj_1234567890",
});
}
```
<Accordion title="View endpoint support">
Consult the following table to see which endpoints support each authentication method.
| Endpoint               | Secret key | Personal Access Token |
| ---------------------- | ---------- | --------------------- |
| `task.trigger`         | âœ…          |                       |
| `task.batchTrigger`    | âœ…          |                       |
| `runs.list`            | âœ…          | âœ…                     |
| `runs.retrieve`        | âœ…          |                       |
| `runs.cancel`          | âœ…          |                       |
| `runs.replay`          | âœ…          |                       |
| `envvars.list`         | âœ…          | âœ…                     |
| `envvars.retrieve`     | âœ…          | âœ…                     |
| `envvars.upload`       | âœ…          | âœ…                     |
| `envvars.create`       | âœ…          | âœ…                     |
| `envvars.update`       | âœ…          | âœ…                     |
| `envvars.del`          | âœ…          | âœ…                     |
| `schedules.list`       | âœ…          |                       |
| `schedules.create`     | âœ…          |                       |
| `schedules.retrieve`   | âœ…          |                       |
| `schedules.update`     | âœ…          |                       |
| `schedules.activate`   | âœ…          |                       |
| `schedules.deactivate` | âœ…          |                       |
| `schedules.del`        | âœ…          |                       |
</Accordion>
### Secret key
Secret key authentication scopes the API access to a specific environment in a project, and works with certain endpoints. You can read our [API Keys guide](/apikeys) for more information.
### Personal Access Token (PAT)
A PAT is a token associated with a specific user, and gives access to all the orgs, projects, and environments that the user has access to. You can identify a PAT by the `tr_pat_` prefix. Because a PAT does not scope access to a specific environment, you must provide the `projectRef` argument when using a PAT (and sometimes the environment as well).
For example, when uploading environment variables using a PAT, you must provide the `projectRef` and `environment` arguments:
```ts
import { configure, envvars } from "@trigger.dev/sdk/v3";
configure({
secretKey: process.env["TRIGGER_ACCESS_TOKEN"], // starts with tr_pat_
});
await envvars.upload("proj_1234", "dev", {
variables: {
MY_ENV_VAR: "MY_ENV_VAR_VALUE",
},
override: true,
});
```
# Auto-pagination
Source: https://trigger.dev/docs/management/auto-pagination
Using auto-pagination with the Trigger.dev management API
All list endpoints in the management API support auto-pagination.
You can use `for await â€¦ of` syntax to iterate through items across all pages:
```ts
import { runs } from "@trigger.dev/sdk/v3";
async function fetchAllRuns() {
const allRuns = [];
for await (const run of runs.list({ limit: 10 })) {
allRuns.push(run);
}
return allRuns;
}
```
You can also use helpers on the return value from any `list` method to get the next/previous page of results:
```ts
import { runs } from "@trigger.dev/sdk/v3";
async function main() {
let page = await runs.list({ limit: 10 });
for (const run of page.data) {
console.log(run);
}
while (page.hasNextPage()) {
page = await page.getNextPage();
// ... do something with the next page
}
}
```
# Create Env Var
Source: https://trigger.dev/docs/management/envvars/create
v3-openapi POST /api/v1/projects/{projectRef}/envvars/{env}
Create a new environment variable for a specific project and environment.
# Delete Env Var
Source: https://trigger.dev/docs/management/envvars/delete
v3-openapi DELETE /api/v1/projects/{projectRef}/envvars/{env}/{name}
Delete a specific environment variable for a specific project and environment.
# Import Env Vars
Source: https://trigger.dev/docs/management/envvars/import
v3-openapi POST /api/v1/projects/{projectRef}/envvars/{env}/import
Upload mulitple environment variables for a specific project and environment.
# List Env Vars
Source: https://trigger.dev/docs/management/envvars/list
v3-openapi GET /api/v1/projects/{projectRef}/envvars/{env}
List all environment variables for a specific project and environment.
# Retrieve Env Var
Source: https://trigger.dev/docs/management/envvars/retrieve
v3-openapi GET /api/v1/projects/{projectRef}/envvars/{env}/{name}
Retrieve a specific environment variable for a specific project and environment.
# Update Env Var
Source: https://trigger.dev/docs/management/envvars/update
v3-openapi PUT /api/v1/projects/{projectRef}/envvars/{env}/{name}
Update a specific environment variable for a specific project and environment.
# Errors and retries
Source: https://trigger.dev/docs/management/errors-and-retries
Handling errors and retries with the Trigger.dev management API
## Handling errors
When the SDK method is unable to connect to the API server, or the API server returns a non-successful response, the SDK will throw an `ApiError` that you can catch and handle:
```ts
import { runs, APIError } from "@trigger.dev/sdk/v3";
async function main() {
try {
const run = await runs.retrieve("run_1234");
} catch (error) {
if (error instanceof ApiError) {
console.error(`API error: ${error.status}, ${error.headers}, ${error.body}`);
} else {
console.error(`Unknown error: ${error.message}`);
}
}
}
```
## Retries
The SDK will automatically retry requests that fail due to network errors or server errors. By default, the SDK will retry requests up to 3 times, with an exponential backoff delay between retries.
You can customize the retry behavior by passing a `requestOptions` option to the `configure` function:
```ts
import { configure } from "@trigger.dev/sdk/v3";
configure({
requestOptions: {
retry: {
maxAttempts: 5,
minTimeoutInMs: 1000,
maxTimeoutInMs: 5000,
factor: 1.8,
randomize: true,
},
},
});
```
All SDK functions also take a `requestOptions` parameter as the last argument, which can be used to customize the request options. You can use this to disable retries for a specific request:
```ts
import { runs } from "@trigger.dev/sdk/v3";
async function main() {
const run = await runs.retrieve("run_1234", {
retry: {
maxAttempts: 1, // Disable retries
},
});
}
```
<Note>
When running inside a task, the SDK ignores customized retry options for certain functions (e.g.,
`task.trigger`, `task.batchTrigger`), and uses retry settings optimized for task execution.
</Note>
# Overview
Source: https://trigger.dev/docs/management/overview
Using the Trigger.dev management API
## Installation
The management API is available through the same `@trigger.dev/sdk` package used in defining and triggering tasks. If you have already installed the package in your project, you can skip this step.
<CodeGroup>
```bash npm
npm i @trigger.dev/sdk@latest
```
```bash pnpm
pnpm add @trigger.dev/sdk@latest
```
```bash yarn
yarn add @trigger.dev/sdk@latest
```
</CodeGroup>
## Usage
All `v3` functionality is provided through the `@trigger.dev/sdk/v3` module. You can import the entire module or individual resources as needed.
```ts
import { configure, runs } from "@trigger.dev/sdk/v3";
configure({
// this is the default and if the `TRIGGER_SECRET_KEY` environment variable is set, can omit calling configure
secretKey: process.env["TRIGGER_SECRET_KEY"],
});
async function main() {
const runs = await runs.list({
limit: 10,
status: ["COMPLETED"],
});
}
main().catch(console.error);
```
# Cancel run
Source: https://trigger.dev/docs/management/runs/cancel
v3-openapi POST /api/v2/runs/{runId}/cancel
Cancels an in-progress run. If the run is already completed, this will have no effect.
# List runs
Source: https://trigger.dev/docs/management/runs/list
v3-openapi GET /api/v1/runs
List runs in a specific environment. You can filter the runs by status, created at, task identifier, version, and more.
# Replay run
Source: https://trigger.dev/docs/management/runs/replay
v3-openapi POST /api/v1/runs/{runId}/replay
Creates a new run with the same payload and options as the original run.
# Reschedule run
Source: https://trigger.dev/docs/management/runs/reschedule
v3-openapi POST /api/v1/runs/{runId}/reschedule
Updates a delayed run with a new delay. Only valid when the run is in the DELAYED state.
# Retrieve run
Source: https://trigger.dev/docs/management/runs/retrieve
v3-openapi GET /api/v3/runs/{runId}
Retrieve information about a run, including its status, payload, output, and attempts. If you authenticate with a Public API key, we will omit the payload and output fields for security reasons.
# Update metadata
Source: https://trigger.dev/docs/management/runs/update-metadata
v3-openapi PUT /api/v1/runs/{runId}/metadata
Update the metadata of a run.
# Activate Schedule
Source: https://trigger.dev/docs/management/schedules/activate
v3-openapi POST /api/v1/schedules/{schedule_id}/activate
Activate a schedule by its ID. This will only work on `IMPERATIVE` schedules that were created in the dashboard or using the imperative SDK functions like `schedules.create()`.
# Create Schedule
Source: https://trigger.dev/docs/management/schedules/create
v3-openapi POST /api/v1/schedules
Create a new `IMPERATIVE` schedule based on the specified options.
# Deactivate Schedule
Source: https://trigger.dev/docs/management/schedules/deactivate
v3-openapi POST /api/v1/schedules/{schedule_id}/deactivate
Deactivate a schedule by its ID. This will only work on `IMPERATIVE` schedules that were created in the dashboard or using the imperative SDK functions like `schedules.create()`.
# Delete Schedule
Source: https://trigger.dev/docs/management/schedules/delete
v3-openapi DELETE /api/v1/schedules/{schedule_id}
Delete a schedule by its ID. This will only work on `IMPERATIVE` schedules that were created in the dashboard or using the imperative SDK functions like `schedules.create()`.
# List Schedules
Source: https://trigger.dev/docs/management/schedules/list
v3-openapi GET /api/v1/schedules
List all schedules. You can also paginate the results.
# Retrieve Schedule
Source: https://trigger.dev/docs/management/schedules/retrieve
v3-openapi GET /api/v1/schedules/{schedule_id}
Get a schedule by its ID.
# Get timezones
Source: https://trigger.dev/docs/management/schedules/timezones
v3-openapi GET /api/v1/timezones
Get all supported timezones that schedule tasks support.
# Update Schedule
Source: https://trigger.dev/docs/management/schedules/update
v3-openapi PUT /api/v1/schedules/{schedule_id}
Update a schedule by its ID. This will only work on `IMPERATIVE` schedules that were created in the dashboard or using the imperative SDK functions like `schedules.create()`.
# Batch trigger
Source: https://trigger.dev/docs/management/tasks/batch-trigger
v3-openapi POST /api/v1/tasks/batch
Batch trigger tasks with up to 500 payloads.
# Trigger
Source: https://trigger.dev/docs/management/tasks/trigger
v3-openapi POST /api/v1/tasks/{taskIdentifier}/trigger
Trigger a task by its identifier.
# Contributing
Source: https://trigger.dev/docs/open-source-contributing
You can contribute to Trigger.dev in many ways.
Go to our [GitHub repository](https://github.com/triggerdotdev/trigger.dev) and open an issue or a pull request. We are always looking for contributors to help us improve Trigger.dev. You can contribute in many ways, including:
* Reporting bugs
* Suggesting new features
* Writing documentation
* Writing code
* Reviewing code
* Translating the app
* Sharing the app with others
* Giving feedback
* And more!
# Self-hosting
Source: https://trigger.dev/docs/open-source-self-hosting
You can self-host Trigger.dev on your own infrastructure.
<Warning>Security, scaling, and reliability concerns are not fully addressed here. This guide is meant for evaluation purposes and won't result in a production-ready deployment.</Warning>
<Note>This guide is for Docker only. We don't currently provide documentation for Kubernetes.</Note>
## Overview
<Frame>
<img src="https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/self-hosting.png" alt="Self-hosting architecture" />
</Frame>
The self-hosting guide covers two alternative setups. The first option uses a simple setup where you run everything on one server. With the second option, the webapp and worker components are split on two separate machines.
You're going to need at least one Debian (or derivative) machine with Docker and Docker Compose installed. We'll also use Ngrok to expose the webapp to the internet.
## Support
It's dangerous to go alone! Join the self-hosting channel on our [Discord server](https://discord.gg/NQTxt5NA7s).
## Caveats
<Note>The v3 worker components don't have ARM support yet.</Note>
This guide outlines a quick way to start self-hosting Trigger.dev for evaluation purposes - it won't result in a production-ready deployment. Security, scaling, and reliability concerns are not fully addressed here.
As self-hosted deployments tend to have unique requirements and configurations, we don't provide specific advice for securing your deployment, scaling up, or improving reliability.
Should the burden ever get too much, we'd be happy to see you on [Trigger.dev cloud](https://trigger.dev/pricing) where we deal with these concerns for you.
<Accordion title="Please consider these additional warnings">
* The [docker checkpoint](https://docs.docker.com/reference/cli/docker/checkpoint/) command is an experimental feature which may not work as expected. It won't be enabled by default. Instead, the containers will stay up and their processes frozen. They won't consume CPU but they *will* consume RAM.
* The `docker-provider` does not currently enforce any resource limits. This means your tasks can consume up to the total machine CPU and RAM. Having no limits may be preferable when self-hosting, but can impact the performance of other services.
* The worker components (not the tasks!) have direct access to the Docker socket. This means they can run any Docker command. To restrict access, you may want to consider using [Docker Socket Proxy](https://github.com/Tecnativa/docker-socket-proxy).
* The task containers are running with host networking. This means there is no network isolation between them and the host machine. They will be able to access any networked service on the host.
* There is currently no support for adding multiple worker machines, but we're working on it.
</Accordion>
## Requirements
* 4 CPU
* 8 GB RAM
* Debian or derivative
* Optional: A separate machine for the worker components
You will also need a way to expose the webapp to the internet. This can be done with a reverse proxy, or with a service like Ngrok. We will be using the latter in this guide.
## Option 1: Single server
This is the simplest setup. You run everything on one server. It's a good option if you have spare capacity on an existing machine, and have no need to independently scale worker capacity.
### Server setup
Some very basic steps to get started:
1. [Install Docker](https://docs.docker.com/get-docker/)
2. [Install Docker Compose](https://docs.docker.com/compose/install/)
3. [Install Ngrok](https://ngrok.com/download)
<Accordion title="On a Debian server, you can run these commands">
```bash
# add ngrok repo
curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | \
sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && \
echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | \
sudo tee /etc/apt/sources.list.d/ngrok.list
# add docker repo
curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc && \
sudo chmod a+r /etc/apt/keyrings/docker.asc && \
echo \
"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \
$(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
# update and install
sudo apt-get update
sudo apt-get install -y \
docker.io \
docker-compose-plugin \
ngrok
```
</Accordion>
### Trigger.dev setup
1. Clone the [Trigger.dev docker repository](https://github.com/triggerdotdev/docker)
```bash
git clone https://github.com/triggerdotdev/docker
cd docker
```
2. Run the start script and follow the prompts
```bash
./start.sh # hint: you can append -d to run in detached mode
```
#### Manual
Alternatively, you can follow these manual steps after cloning the docker repo:
1. Create the `.env` file
```bash
cp .env.example .env
```
2. Generate the required secrets
```bash
echo MAGIC_LINK_SECRET=$(openssl rand -hex 16)
echo SESSION_SECRET=$(openssl rand -hex 16)
echo ENCRYPTION_KEY=$(openssl rand -hex 16)
echo PROVIDER_SECRET=$(openssl rand -hex 32)
echo COORDINATOR_SECRET=$(openssl rand -hex 32)
```
3. Replace the default secrets in the `.env` file with the generated ones
4. Run docker compose to start the services
```bash
. lib.sh # source the helper function
docker_compose -p=trigger up
```
### Tunnelling
You will need to expose the webapp to the internet. You can use Ngrok for this. If you already have a working reverse proxy setup and a domain, you can skip to the last step.
1. Start Ngrok. You may get prompted to sign up - it's free.
```bash
./tunnel.sh
```
2. Copy the domain from the output, for example: `1234-42-42-42-42.ngrok-free.app`
3. Uncomment the `TRIGGER_PROTOCOL` and `TRIGGER_DOMAIN` lines in the `.env` file. Set it to the domain you copied.
```bash
TRIGGER_PROTOCOL=https
TRIGGER_DOMAIN=1234-42-42-42-42.ngrok-free.app
```
4. Quit the start script and launch it again, or run this:
```bash
./stop.sh && ./start.sh
```
### Registry setup
If you want to deploy v3 projects, you will need access to a Docker registry. The [CLI deploy](/cli-deploy) command will push the images, and then the worker machine can pull them when needed. We will use Docker Hub as an example.
1. Sign up for a free account at [Docker Hub](https://hub.docker.com/)
2. Edit the `.env` file and add the registry details
```bash
DEPLOY_REGISTRY_HOST=docker.io
DEPLOY_REGISTRY_NAMESPACE=<your_dockerhub_username>
```
3. Log in to Docker Hub both locally and your server. For the split setup, this will be the worker machine. You may want to create an [access token](https://hub.docker.com/settings/security) for this.
```bash
docker login -u <your_dockerhub_username> docker.io
```
4. Required on some systems: Run the login command inside the `docker-provider` container so it can pull deployment images to run your tasks.
```bash
docker exec -ti \
trigger-docker-provider-1 \
docker login -u <your_dockerhub_username> docker.io
```
5. Restart the services
```bash
./stop.sh && ./start.sh
```
6. You can now deploy v3 projects using the CLI with these flags:
```
npx trigger.dev@latest deploy --self-hosted --push
```
## Option 2: Split services
With this setup, the webapp will run on a different machine than the worker components. This allows independent scaling of your workload capacity.
### Webapp setup
All steps are the same as for a single server, except for the following:
1. **Startup.** Run the start script with the `webapp` argument
```bash
./start.sh webapp
```
2. **Tunnelling.** This is now *required*. Please follow the [tunnelling](/open-source-self-hosting#tunnelling) section.
### Worker setup
1. **Environment variables.** Copy your `.env` file from the webapp to the worker machine:
```bash
# an example using scp
scp -3 root@<webapp_machine>:docker/.env root@<worker_machine>:docker/.env
```
2. **Startup.** Run the start script with the `worker` argument
```bash
./start.sh worker
```
3. **Tunnelling.** This is *not* required for the worker components.
4. **Registry setup.** Follow the [registry setup](/open-source-self-hosting#registry-setup) section but run the last command on the worker machine - note the container name is different:
```bash
docker exec -ti \
trigger-worker-docker-provider-1 \
docker login -u <your_dockerhub_username> docker.io
```
## Additional features
### Large payloads
By default, payloads over 512KB will be offloaded to S3-compatible storage. If you don't provide the required env vars, runs with payloads larger than this will fail.
For example, using Cloudflare R2:
```bash
OBJECT_STORE_BASE_URL="https://<bucket>.<account>.r2.cloudflarestorage.com"
OBJECT_STORE_ACCESS_KEY_ID="<r2 access key with read/write access to bucket>"
OBJECT_STORE_SECRET_ACCESS_KEY="<r2 secret key>"
```
Alternatively, you can increase the threshold:
```bash
# size in bytes, example with 5MB threshold
TASK_PAYLOAD_OFFLOAD_THRESHOLD=5242880
```
### Version locking
There are several reasons to lock the version of your Docker images:
* **Backwards compatibility.** We try our best to maintain compatibility with older CLI versions, but it's not always possible. If you don't want to update your CLI, you can lock your Docker images to that specific version.
* **Ensuring full feature support.** Sometimes, new CLI releases will also require new or updated platform features. Running unlocked images can make any issues difficult to debug. Using a specific tag can help here as well.
By default, the images will point at the latest versioned release via the `v3` tag. You can override this by specifying a different tag in your `.env` file. For example:
```bash
TRIGGER_IMAGE_TAG=v3.0.4
```
### Auth options
By default, magic link auth is the only login option. If the `EMAIL_TRANSPORT` env var is not set, the magic links will be logged by the webapp container and not sent via email.
Depending on your choice of mail provider/transport, you will want to configure a set of variables like one of the following:
##### Resend:
```bash
EMAIL_TRANSPORT=resend
FROM_EMAIL=
REPLY_TO_EMAIL=
RESEND_API_KEY=<your_resend_api_key>
```
##### SMTP
Note that setting `SMTP_SECURE=false` does *not* mean the email is sent insecurely.
This simply means that the connection is secured using the modern STARTTLS protocol command instead of implicit TLS.
You should only set this to true when the SMTP server host directs you to do so (generally when using port 465)
```bash
EMAIL_TRANSPORT=smtp
FROM_EMAIL=
REPLY_TO_EMAIL=
SMTP_HOST=<your_smtp_server>
SMTP_PORT=587
SMTP_SECURE=false
SMTP_USER=<your_smtp_username>
SMTP_PASSWORD=<your_smtp_password>
```
##### AWS Simple Email Service
Credentials are to be supplied as with any other program using the AWS SDK.
In this scenario, you would likely either supply the additional environment variables `AWS_REGION`, `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` or, when running on AWS, use credentials supplied by the EC2 IMDS.
```bash
EMAIL_TRANSPORT=aws-ses
FROM_EMAIL=
REPLY_TO_EMAIL=
```
All email addresses can sign up and log in this way. If you would like to restrict this, you can use the `WHITELISTED_EMAILS` env var. For example:
```bash
# every email that does not match this regex will be rejected
WHITELISTED_EMAILS="authorized@yahoo\.com|authorized@gmail\.com"
```
It's currently impossible to restrict GitHub OAuth logins by account name or email like above, so this method is *not recommended* for self-hosted instances. It's also very easy to lock yourself out of your own instance.
<Warning>Only enable GitHub auth if you understand the risks! We strongly advise you against this.</Warning>
Your GitHub OAuth app needs a callback URL `https://<your_domain>/auth/github/callback` and you will have to set the following env vars:
```bash
AUTH_GITHUB_CLIENT_ID=<your_client_id>
AUTH_GITHUB_CLIENT_SECRET=<your_client_secret>
```
### Checkpoint support
<Warning>
This requires an *experimental Docker feature*. Successfully checkpointing a task today, does not
mean you will be able to restore it tomorrow. Your data may be lost. You've been warned!
</Warning>
Checkpointing allows you to save the state of a running container to disk and restore it later. This can be useful for
long-running tasks that need to be paused and resumed without losing state. Think fan-out and fan-in, or long waits in email campaigns.
The checkpoints will be pushed to the same registry as the deployed images. Please see the [registry setup](#registry-setup) section for more information.
#### Requirements
* Debian, **NOT** a derivative like Ubuntu
* Additional storage space for the checkpointed containers
#### Setup
Underneath the hood this uses Checkpoint and Restore in Userspace, or [CRIU](https://github.com/checkpoint-restore/criu) in short. We'll have to do a few things to get this working:
1. Install CRIU
```bash
sudo apt-get update
sudo apt-get install criu
```
2. Tweak the config so we can successfully checkpoint our workloads
```bash
mkdir -p /etc/criu
cat << EOF >/etc/criu/runc.conf
tcp-close
EOF
```
3. Make sure everything works
```bash
sudo criu check
```
3. Enable Docker experimental features, by adding the following to `/etc/docker/daemon.json`
```json
{
"experimental": true
}
```
4. Restart the Docker daemon
```bash
sudo systemctl restart docker
```
5. Uncomment `FORCE_CHECKPOINT_SIMULATION=0` in your `.env` file. Alternatively, run this:
```bash
echo "FORCE_CHECKPOINT_SIMULATION=0" >> .env
```
6. Restart the services
```bash
# if you're running everything on the same machine
./stop.sh && ./start.sh
# if you're running the worker on a different machine
./stop.sh worker && ./start.sh worker
```
## Updating
Once you have everything set up, you will periodically want to update your Docker images. You can easily do this by running the update script and restarting your services:
```bash
./update.sh
./stop.sh && ./start.sh
```
Sometimes, we will make more extensive changes that require pulling updated compose files, scripts, etc from our docker repo:
```bash
git pull
./stop.sh && ./start.sh
```
Occasionally, you may also have to update your `.env` file, but we will try to keep these changes to a minimum. Check the `.env.example` file for new variables.
### From beta
If you're coming from the beta CLI package images, you will need to:
* **Stash you changes.** If you made any changes, stash them with `git stash`.
* **Switch branches.** We moved back to main. Run `git checkout main` in your docker repo.
* **Pull in updates.** We've added a new container for [Electric](https://github.com/electric-sql/electric) and made some other improvements. Run `git pull` to get the latest updates.
* **Apply your changes.** If you stashed your changes, apply them with `git stash pop`.
* **Update your images.** We've also published new images. Run `./update.sh` to pull them.
* **Restart all services.** Run `./stop.sh && ./start.sh` and you're good to go.
In summary, run this wherever you cloned the docker repo:
```bash
# if you made changes
git stash
# switch to the main branch and pull the latest changes
git checkout main
git pull
# if you stashed your changes
git stash pop
# update and restart your services
./update.sh
./stop.sh && ./start.sh
```
## Troubleshooting
* **Deployment fails at the push step.** The machine running `deploy` needs registry access:
```bash
docker login -u <username> <registry>
# this should now succeed
npx trigger.dev@latest deploy --self-hosted --push
```
* **Prod runs fail to start.** The `docker-provider` needs registry access:
```bash
# single server? run this:
docker exec -ti \
trigger-docker-provider-1 \
docker login -u <your_dockerhub_username> docker.io
# split webapp and worker? run this on the worker:
docker exec -ti \
trigger-worker-docker-provider-1 \
docker login -u <your_dockerhub_username> docker.io
```
## CLI usage
This section highlights some of the CLI commands and options that are useful when self-hosting. Please check the [CLI reference](/cli-introduction) for more in-depth documentation.
### Login
To avoid being redirected to the [Trigger.dev Cloud](https://cloud.trigger.dev) login page when using the CLI, you can specify the URL of your self-hosted instance with the `--api-url` or `-a` flag. For example:
```bash
npx trigger.dev@latest login -a http://trigger.example.com
```
Once you've logged in, the CLI will remember your login details and you won't need to specify the URL again with other commands.
#### Custom profiles
You can specify a custom profile when logging in. This allows you to easily use the CLI with our cloud product and your self-hosted instance at the same time. For example:
```
npx trigger.dev@latest login -a http://trigger.example.com --profile my-profile
```
You can then use this profile with other commands:
```
npx trigger.dev@latest dev --profile my-profile
```
To list all your profiles, use the `list-profiles` command:
```
npx trigger.dev@latest list-profiles
```
#### Verify login
It can be useful to check you have successfully logged in to the correct instance. You can do this with the `whoami` command, which will also show the API URL:
```bash
npx trigger.dev@latest whoami
# with a custom profile
npx trigger.dev@latest whoami --profile my-profile
```
### Deploy
On [Trigger.dev Cloud](https://cloud.trigger.dev), we build deployments remotely and push those images for you. When self-hosting you will have to do that locally yourself. This can be done with the `--self-hosted` and `--push` flags. For example:
```
npx trigger.dev@latest deploy --self-hosted --push
```
### CI / GitHub Actions
When running the CLI in a CI environment, your login profiles won't be available. Instead, you can use the `TRIGGER_API_URL` and `TRIGGER_ACCESS_TOKEN` environment
variables to point at your self-hosted instance and authenticate.
For more detailed instructions, see the [GitHub Actions guide](/github-actions).
## Telemetry
By default, the Trigger.dev webapp sends telemetry data to our servers. This data is used to improve the product and is not shared with third parties. If you would like to opt-out of this, you can set the `TRIGGER_TELEMETRY_DISABLED` environment variable in your `.env` file. The value doesn't matter, it just can't be empty. For example:
```bash
TRIGGER_TELEMETRY_DISABLED=1
```
# Concurrency & Queues
Source: https://trigger.dev/docs/queue-concurrency
Configure what you want to happen when there is more than one run at a time.
When you trigger a task, it isn't executed immediately. Instead, the task [run](/runs) is placed into a queue for execution. By default, each task gets its own queue with unbounded concurrencyâ€”meaning the task runs as soon as resources are available, subject only to the overall concurrency limits of your environment. If you need more control (for example, to limit concurrency or share limits across multiple tasks), you can define a custom queue as described later in this document.
Controlling concurrency is useful when you have a task that can't be run concurrently, or when you want to limit the number of runs to avoid overloading a resource.
It's important to note that only actively executing runs count towards concurrency limits. Runs that are delayed or waiting in a queue do not consume concurrency slots until they begin execution.
## Default concurrency
By default, all tasks have an unbounded concurrency limit, limited only by the overall concurrency limits of your environment. This means that each task could possibly "fill up" the entire
concurrency limit of your environment.
<Note>
Your environment has a maximum concurrency limit which depends on your plan. If you're a paying
customer you can request a higher limit by [contacting us](https://www.trigger.dev/contact).
</Note>
## Setting task concurrency
You can set the concurrency limit for a task by setting the `concurrencyLimit` property on the task's queue. This limits the number of runs that can be executing at any one time:
```ts /trigger/one-at-a-time.ts
// This task will only run one at a time
export const oneAtATime = task({
id: "one-at-a-time",
queue: {
concurrencyLimit: 1,
},
run: async (payload) => {
//...
},
});
```
This is useful if you need to control access to a shared resource, like a database or an API that has rate limits.
## Sharing concurrency between tasks
As well as putting queue settings directly on a task, you can define a queue and reuse it across multiple tasks. This allows you to share the same concurrency limit:
```ts /trigger/queue.ts
export const myQueue = queue({
name: "my-queue",
concurrencyLimit: 1,
});
export const task1 = task({
id: "task-1",
queue: myQueue,
run: async (payload: { message: string }) => {
// ...
},
});
export const task2 = task({
id: "task-2",
queue: myQueue,
run: async (payload: { message: string }) => {
// ...
},
});
```
In this example, `task1` and `task2` share the same queue, so only one of them can run at a time.
## Setting the concurrency when you trigger a run
When you trigger a task you can override the concurrency limit. This is really useful if you sometimes have high priority runs.
The task:
```ts /trigger/override-concurrency.ts
export const generatePullRequest = task({
id: "generate-pull-request",
queue: {
//normally when triggering this task it will be limited to 1 run at a time
concurrencyLimit: 1,
},
run: async (payload) => {
//todo generate a PR using OpenAI
},
});
```
Triggering from your backend and overriding the concurrency:
```ts app/api/push/route.ts
import { generatePullRequest } from "~/trigger/override-concurrency";
export async function POST(request: Request) {
const data = await request.json();
if (data.branch === "main") {
//trigger the task, with a different queue
const handle = await generatePullRequest.trigger(data, {
queue: {
//the "main-branch" queue will have a concurrency limit of 10
//this triggered run will use that queue
name: "main-branch", // Make sure to change the queue name or the task concurrency limit will be updated
concurrencyLimit: 10,
},
});
return Response.json(handle);
} else {
//triggered with the default (concurrency of 1)
const handle = await generatePullRequest.trigger(data);
return Response.json(handle);
}
}
```
## Concurrency keys and per-tenant queuing
If you're building an application where you want to run tasks for your users, you might want a separate queue for each of your users (or orgs, projects, etc.).
You can do this by using `concurrencyKey`. It creates a separate queue for each value of the key.
Your backend code:
```ts app/api/pr/route.ts
import { generatePullRequest } from "~/trigger/override-concurrency";
export async function POST(request: Request) {
const data = await request.json();
if (data.isFreeUser) {
//free users can only have 1 PR generated at a time
const handle = await generatePullRequest.trigger(data, {
queue: {
//every free user gets a queue with a concurrency limit of 1
name: "free-users",
concurrencyLimit: 1,
},
concurrencyKey: data.userId,
});
//return a success response with the handle
return Response.json(handle);
} else {
//trigger the task, with a different queue
const handle = await generatePullRequest.trigger(data, {
queue: {
//every paid user gets a queue with a concurrency limit of 10
name: "paid-users",
concurrencyLimit: 10,
},
concurrencyKey: data.userId,
});
//return a success response with the handle
return Response.json(handle);
}
}
```
## Concurrency and subtasks
When you trigger a task that has subtasks, the subtasks will not inherit the concurrency settings of the parent task. Unless otherwise specified, subtasks will run on their own queue
```ts /trigger/subtasks.ts
export const parentTask = task({
id: "parent-task",
run: async (payload) => {
//trigger a subtask
await subtask.triggerAndWait(payload);
},
});
// This subtask will run on its own queue
export const subtask = task({
id: "subtask",
run: async (payload) => {
//...
},
});
```
## Waits and concurrency
With our [task checkpoint system](/how-it-works#the-checkpoint-resume-system), a parent task can trigger and wait for a subtask to complete. The way this system interacts with the concurrency system is a little complicated but important to understand. There are two main scenarios that we handle slightly differently:
* When a parent task waits for a subtask on a different queue.
* When a parent task waits for a subtask on the same queue.
These scenarios are discussed in more detail below:
<Note>
We sometimes refer to the parent task as the "parent" and the subtask as the "child". Subtask and
child task are used interchangeably. We apologize for the confusion.
</Note>
### Waiting for a subtask on a different queue
During the time when a parent task is waiting on a subtask, the "concurrency" slot of the parent task is still considered occupied on the parent task queue, but is temporarily "released" to the environment. An example will help illustrate this:
```ts /trigger/waiting.ts
export const parentTask = task({
id: "parent-task",
queue: {
concurrencyLimit: 1,
},
run: async (payload) => {
//trigger a subtask
await subtask.triggerAndWait(payload);
},
});
export const subtask = task({
id: "subtask",
run: async (payload) => {
//...
},
});
```
For example purposes, let's say the environment concurrency limit is 1. When the parent task is triggered, it will occupy the only slot in the environment. When the parent task triggers the subtask, the subtask will be placed in the queue for the subtask. The parent task will then wait for the subtask to complete. During this time, the parent task slot is temporarily released to the environment, allowing another task to run. Once the subtask completes, the parent task slot is reoccupied.
This system prevents "stuck" tasks. If the parent task were to wait on the subtask and not release the slot, the environment would be stuck with only one task running.
And because only the environment slot is released, the parent task queue slot is still occupied. This means that if another task is triggered on the parent task queue, it will be placed in the queue and wait for the parent task to complete, respecting the concurrency limit.
### Waiting for a subtask on the same queue
Because tasks can trigger and wait recursively, or share the same queue, we've added special handling for when a parent task waits for a subtask on the same queue.
Recall above that when waiting for a subtask on a different queue, the parent task slot is temporarily released to the environment. When the parent task and the subtask share a queue, we also release the parent task slot to the queue. Again, an example will help illustrate this:
```ts /trigger/waiting-same-queue.ts
export const myQueue = queue({
name: "my-queue",
concurrencyLimit: 1,
});
export const parentTask = task({
id: "parent-task",
queue: myQueue,
run: async (payload) => {
//trigger a subtask
await subtask.triggerAndWait(payload);
},
});
export const subtask = task({
id: "subtask",
queue: myQueue,
run: async (payload) => {
//...
},
});
```
In this example, the parent task and the subtask share the same queue with a concurrency limit of 1. When the parent task triggers the subtask, the parent task slot is released to the queue, giving the subtask the opportunity to run. Once the subtask completes, the parent task slot is reoccupied.
It's very important to note that we only release at-most X slots to the queue, where X is the concurrency limit of the queue. This means that you can only trigger and wait for X subtasks on the same queue. If you try to trigger and wait for more than X subtasks, you will receive a `RECURSIVE_WAIT_DEADLOCK` error. The following example will result in a deadlock:
```ts /trigger/deadlock.ts
export const myQueue = queue({
name: "my-queue",
concurrencyLimit: 1,
});
export const parentTask = task({
id: "parent-task",
queue: myQueue,
run: async (payload) => {
//trigger a subtask
await subtask.triggerAndWait(payload);
},
});
export const subtask = task({
id: "subtask",
queue: myQueue,
run: async (payload) => {
//trigger a subtask
await subsubtask.triggerAndWait(payload);
},
});
export const subsubtask = task({
id: "subsubtask",
queue: myQueue,
run: async (payload) => {
//...
},
});
```
Now this will result in a `RECURSIVE_WAIT_DEADLOCK` error because the parent task is waiting for the subtask, and the subtask is waiting for the subsubtask, but there is no more concurrency available in the queue. It will look a bit like this in the logs:
![Recursive task deadlock](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/recursive-task-deadlock-min.png)
### Mitigating recursive wait deadlocks
If you are recursively triggering and waiting for tasks on the same queue, you can mitigate the risk of a deadlock by increasing the concurrency limit of the queue. This will allow you to trigger and wait for more subtasks.
You can also use different queues for the parent task and the subtask. This will allow you to trigger and wait for more subtasks without the risk of a deadlock.
# Quick start
Source: https://trigger.dev/docs/quick-start
How to get started in 3 minutes using the CLI and SDK.
In this guide we will:
1. Create a `trigger.config.ts` file and a `/trigger` directory with an example task.
2. Get you to run the task using the CLI.
3. Show you how to view the run logs for that task.
<Steps titleSize="h3">
<Step title="Create a Trigger.dev account">
You can either:
* Use the [Trigger.dev Cloud](https://cloud.trigger.dev).
* Or [self-host](/open-source-self-hosting) the service.
</Step>
<Step title="Create your first project">
Once you've created an account, follow the steps in the app to:
1. Complete your account details.
2. Create your first Organization and Project.
</Step>
<Step title="Run the CLI `init` command">
The easiest way to get started is to use the CLI. It will add Trigger.dev to your existing project, create a `/trigger` folder and give you an example task.
Run this command in the root of your project to get started:
<CodeGroup>
```bash npm
npx trigger.dev@latest init
```
```bash pnpm
pnpm dlx trigger.dev@latest init
```
```bash yarn
yarn dlx trigger.dev@latest init
```
</CodeGroup>
It will do a few things:
1. Log you into the CLI if you're not already logged in.
2. Create a `trigger.config.ts` file in the root of your project.
3. Ask where you'd like to create the `/trigger` directory.
4. Create the `/trigger` directory with an example task, `/trigger/example.[ts/js]`.
Install the "Hello World" example task when prompted. We'll use this task to test the setup.
</Step>
<Step title="Run the CLI `dev` command">
The CLI `dev` command runs a server for your tasks. It watches for changes in your `/trigger` directory and communicates with the Trigger.dev platform to register your tasks, perform runs, and send data back and forth.
It can also update your `@trigger.dev/*` packages to prevent version mismatches and failed deploys. You will always be prompted first.
<CodeGroup>
```bash npm
npx trigger.dev@latest dev
```
```bash pnpm
pnpm dlx trigger.dev@latest dev
```
```bash yarn
yarn dlx trigger.dev@latest dev
```
</CodeGroup>
</Step>
<Step title="Perform a test run using the dashboard">
The CLI `dev` command spits out various useful URLs. Right now we want to visit the Test page <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" />.
You should see our Example task in the list <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" />, select it. Most tasks have a "payload" which you enter in the JSON editor <Icon icon="circle-3" iconType="solid" size={20} color="F43F47" />, but our example task doesn't need any input.
Press the "Run test" button <Icon icon="circle-4" iconType="solid" size={20} color="F43F47" />.
![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-page.png)
</Step>
<Step title="View your run">
Congratulations, you should see the run page which will live reload showing you the current state of the run.
![Run page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-page.png)
If you go back to your terminal you'll see that the dev command also shows the task status and links to the run log.
![Terminal showing completed run](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/terminal-completed-run.png)
</Step>
</Steps>
## Next steps
<CardGroup>
<Card title="How to trigger your tasks" icon="bolt" href="/triggering">
Learn how to trigger tasks from your code.
</Card>
<Card title="Writing tasks" icon="wand-magic-sparkles" href="/tasks/overview">
Tasks are the core of Trigger.dev. Learn what they are and how to write them.
</Card>
</CardGroup>
# Realtime overview
Source: https://trigger.dev/docs/realtime/overview
Using the Trigger.dev v3 realtime API
Trigger.dev Realtime is a set of APIs that allow you to subscribe to runs and get real-time updates on the run status. This is useful for monitoring runs, updating UIs, and building realtime dashboards.
## How it works
The Realtime API is built on top of [Electric SQL](https://electric-sql.com/), an open-source PostgreSQL syncing engine. The Trigger.dev API wraps Electric SQL and provides a simple API to subscribe to [runs](/runs) and get real-time updates.
## Walkthrough
<iframe width="100%" height="315" src="https://www.youtube.com/embed/RhJAbSGkS88?si=4Z72SfygeklNI3As" title="YouTube video player" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen />
## Usage
After you trigger a task, you can subscribe to the run using the `runs.subscribeToRun` function. This function returns an async iterator that you can use to get updates on the run status.
```ts
import { runs, tasks } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
async function myBackend() {
const handle = await tasks.trigger("my-task", { some: "data" });
for await (const run of runs.subscribeToRun(handle.id)) {
// This will log the run every time it changes
console.log(run);
}
}
```
Every time the run changes, the async iterator will yield the updated run. You can use this to update your UI, log the run status, or take any other action.
Alternatively, you can subscribe to changes to any run that includes a specific tag (or tags) using the `runs.subscribeToRunsWithTag` function.
```ts
import { runs } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
for await (const run of runs.subscribeToRunsWithTag("user:1234")) {
// This will log the run every time it changes, for all runs with the tag "user:1234"
console.log(run);
}
```
If you've used `batchTrigger` to trigger multiple runs, you can also subscribe to changes to all the runs triggered in the batch using the `runs.subscribeToBatch` function.
```ts
import { runs } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
for await (const run of runs.subscribeToBatch("batch-id")) {
// This will log the run every time it changes, for all runs in the batch with the ID "batch-id"
console.log(run);
}
```
### React hooks
We also provide a set of React hooks that make it easy to use the Realtime API in your React components. See the [React hooks doc](/frontend/react-hooks) for more information.
## Run changes
You will receive updates whenever a run changes for the following reasons:
* The run moves to a new state. See our [run lifecycle docs](/runs#the-run-lifecycle) for more information.
* [Run tags](/tags) are added or removed.
* [Run metadata](/runs/metadata) is updated.
## Run object
The run object returned by the async iterator is NOT the same as the run object returned by the `runs.retrieve` function. This is because Electric SQL streams changes from a single PostgreSQL table, and the run object returned by `runs.retrieve` is a combination of multiple tables.
The run object returned by the async iterator has the following fields:
<ParamField path="id" type="string" required>
The run ID.
</ParamField>
<ParamField path="taskIdentifier" type="string" required>
The task identifier.
</ParamField>
<ParamField path="payload" type="object" required>
The input payload for the run.
</ParamField>
<ParamField path="output" type="object">
The output result of the run.
</ParamField>
<ParamField path="createdAt" type="Date" required>
Timestamp when the run was created.
</ParamField>
<ParamField path="updatedAt" type="Date" required>
Timestamp when the run was last updated.
</ParamField>
<ParamField path="number" type="number" required>
Sequential number assigned to the run.
</ParamField>
<ParamField path="status" type="RunStatus" required>
Current status of the run.
<Accordion title="RunStatus enum">
| Status               | Description                                                                                               |
| -------------------- | --------------------------------------------------------------------------------------------------------- |
| `WAITING_FOR_DEPLOY` | Task hasn't been deployed yet but is waiting to be executed                                               |
| `QUEUED`             | Run is waiting to be executed by a worker                                                                 |
| `EXECUTING`          | Run is currently being executed by a worker                                                               |
| `REATTEMPTING`       | Run has failed and is waiting to be retried                                                               |
| `FROZEN`             | Run has been paused by the system, and will be resumed by the system                                      |
| `COMPLETED`          | Run has been completed successfully                                                                       |
| `CANCELED`           | Run has been canceled by the user                                                                         |
| `FAILED`             | Run has been completed with errors                                                                        |
| `CRASHED`            | Run has crashed and won't be retried, most likely the worker ran out of resources, e.g. memory or storage |
| `INTERRUPTED`        | Run was interrupted during execution, mostly this happens in development environments                     |
| `SYSTEM_FAILURE`     | Run has failed to complete, due to an error in the system                                                 |
| `DELAYED`            | Run has been scheduled to run at a specific time                                                          |
| `EXPIRED`            | Run has expired and won't be executed                                                                     |
| `TIMED_OUT`          | Run has reached it's maxDuration and has been stopped                                                     |
</Accordion>
</ParamField>
<ParamField path="durationMs" type="number" required>
Duration of the run in milliseconds.
</ParamField>
<ParamField path="costInCents" type="number" required>
Total cost of the run in cents.
</ParamField>
<ParamField path="baseCostInCents" type="number" required>
Base cost of the run in cents before any additional charges.
</ParamField>
<ParamField path="tags" type="string[]" required>
Array of tags associated with the run.
</ParamField>
<ParamField path="idempotencyKey" type="string">
Key used to ensure idempotent execution.
</ParamField>
<ParamField path="expiredAt" type="Date">
Timestamp when the run expired.
</ParamField>
<ParamField path="ttl" type="string">
Time-to-live duration for the run.
</ParamField>
<ParamField path="finishedAt" type="Date">
Timestamp when the run finished.
</ParamField>
<ParamField path="startedAt" type="Date">
Timestamp when the run started.
</ParamField>
<ParamField path="delayedUntil" type="Date">
Timestamp until which the run is delayed.
</ParamField>
<ParamField path="queuedAt" type="Date">
Timestamp when the run was queued.
</ParamField>
<ParamField path="metadata" type="Record<string, DeserializedJson>">
Additional metadata associated with the run.
</ParamField>
<ParamField path="error" type="SerializedError">
Error information if the run failed.
</ParamField>
<ParamField path="isTest" type="boolean" required>
Indicates whether this is a test run.
</ParamField>
## Type-safety
You can infer the types of the run's payload and output by passing the type of the task to the `subscribeToRun` function. This will give you type-safe access to the run's payload and output.
```ts
import { runs, tasks } from "@trigger.dev/sdk/v3";
import type { myTask } from "./trigger/my-task";
// Somewhere in your backend code
async function myBackend() {
const handle = await tasks.trigger("my-task", { some: "data" });
for await (const run of runs.subscribeToRun<typeof myTask>(handle.id)) {
// This will log the run every time it changes
console.log(run.payload.some);
if (run.output) {
// This will log the output if it exists
console.log(run.output.some);
}
}
}
```
When using `subscribeToRunsWithTag`, you can pass a union of task types for all the possible tasks that can have the tag.
```ts
import { runs } from "@trigger.dev/sdk/v3";
import type { myTask, myOtherTask } from "./trigger/my-task";
// Somewhere in your backend code
for await (const run of runs.subscribeToRunsWithTag<typeof myTask | typeof myOtherTask>("my-tag")) {
// You can narrow down the type based on the taskIdentifier
switch (run.taskIdentifier) {
case "my-task": {
console.log("Run output:", run.output.foo); // This will be type-safe
break;
}
case "my-other-task": {
console.log("Run output:", run.output.bar); // This will be type-safe
break;
}
}
}
```
## Run metadata
The run metadata API gives you the ability to add or update custom metadata on a run, which will cause the run to be updated. This allows you to extend the realtime API with custom data attached to a run that can be used for various purposes. Some common use cases include:
* Adding a link to a related resource
* Adding a reference to a user or organization
* Adding a custom status with progress information
See our [run metadata docs](/runs/metadata) for more on how to use this feature.
### Using w/Realtime & React hooks
We suggest combining run metadata with the realtime API and our [React hooks](/frontend/react-hooks) to bridge the gap between your trigger.dev tasks and your UI. This allows you to update your UI in real-time based on changes to the run metadata. As a simple example, you could add a custom status to a run with a progress value, and update your UI based on that progress.
We have a full demo app repo available [here](https://github.com/triggerdotdev/nextjs-realtime-simple-demo)
## Realtime streams
See our dedicated [Realtime streams](/realtime/streams) documentation for more information on how to use the Realtime streams API.
## Limits
The Realtime API in the Trigger.dev Cloud limits the number of concurrent subscriptions, depending on your plan. If you exceed the limit, you will receive an error when trying to subscribe to a run. For more information, see our [pricing page](https://trigger.dev/pricing).
## Known issues
There is currently a known issue where the realtime API does not work if subscribing to a run that has a large payload or large output and are stored in object store instead of the database. We are working on a fix for this issue: [https://github.com/triggerdotdev/trigger.dev/issues/1451](https://github.com/triggerdotdev/trigger.dev/issues/1451). As a workaround you'll need to keep payloads and outputs below 128KB when using the realtime API.
# Realtime React hooks
Source: https://trigger.dev/docs/realtime/react-hooks
Subscribes to all changes to a run in a React component.
See our [React hooks](/frontend/react-hooks) for more information about how to use the Realtime API from your frontend application.
# Realtime streams
Source: https://trigger.dev/docs/realtime/streams
Stream data in realtime from inside your tasks
The world is going realtime, and so should your tasks. With the Streams API, you can stream data from your tasks to the outside world in realtime. This is useful for a variety of use cases, including AI.
## How it works
The Streams API is a simple API that allows you to send data from your tasks to the outside world in realtime using the [metadata](/runs/metadata) system. You can send any kind of data that is streamed in realtime, but the most common use case is to send streaming output from streaming LLM providers, like OpenAI.
## Usage
To use the Streams API, you need to register a stream with a specific key using `metadata.stream`. The following example uses the OpenAI SDK with `stream: true` to stream the output of the LLM model in realtime:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
import OpenAI from "openai";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
export type STREAMS = {
openai: OpenAI.ChatCompletionChunk; // The type of the chunk is determined by the provider
};
export const myTask = task({
id: "my-task",
run: async (payload: { prompt: string }) => {
const completion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
stream: true,
});
// Register the stream with the key "openai"
// This will "tee" the stream and send it to the metadata system
const stream = await metadata.stream("openai", completion);
let text = "";
// You can read the returned stream as an async iterator
for await (const chunk of stream) {
logger.log("Received chunk", { chunk });
// The type of the chunk is determined by the provider
text += chunk.choices.map((choice) => choice.delta?.content).join("");
}
return { text };
},
});
```
You can then subscribe to the stream using the `runs.subscribeToRun` method:
<Note>
`runs.subscribeToRun` should be used from your backend or another task. To subscribe to a run from
your frontend, you can use our [React hooks](/frontend/react-hooks).
</Note>
```ts
import { runs } from "@trigger.dev/sdk/v3";
import type { myTask, STREAMS } from "./trigger/my-task";
// Somewhere in your backend
async function subscribeToStream(runId: string) {
// Use a for-await loop to subscribe to the stream
for await (const part of runs.subscribeToRun<typeof myTask>(runId).withStreams<STREAMS>()) {
switch (part.type) {
case "run": {
console.log("Received run", part.run);
break;
}
case "openai": {
// part.chunk is of type OpenAI.ChatCompletionChunk
console.log("Received OpenAI chunk", part.chunk);
break;
}
}
}
}
```
You can register and subscribe to multiple streams in the same task. Let's add a stream from the response body of a fetch request:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
import OpenAI from "openai";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
export type STREAMS = {
openai: OpenAI.ChatCompletionChunk; // The type of the chunk is determined by the provider
fetch: string; // The response body will be an array of strings
};
export const myTask = task({
id: "my-task",
run: async (payload: { prompt: string }) => {
const completion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
stream: true,
});
// Register the stream with the key "openai"
await metadata.stream("openai", completion);
const response = await fetch("https://jsonplaceholder.typicode.com/posts");
if (!response.body) {
return;
}
// Register the stream with the key "fetch"
// Pipe the response.body through a TextDecoderStream to convert it to a string
await metadata.stream("fetch", response.body.pipeThrough(new TextDecoderStream()));
},
});
```
<Note>
You may notice above that we aren't consuming either of the streams in the task. In the
background, we'll wait until all streams are consumed before the task is considered complete (with
a max timeout of 60 seconds). If you have a longer running stream, make sure to consume it in the
task.
</Note>
And then subscribing to the streams:
```ts
import { runs } from "@trigger.dev/sdk/v3";
import type { myTask, STREAMS } from "./trigger/my-task";
// Somewhere in your backend
async function subscribeToStream(runId: string) {
// Use a for-await loop to subscribe to the stream
for await (const part of runs.subscribeToRun<typeof myTask>(runId).withStreams<STREAMS>()) {
switch (part.type) {
case "run": {
console.log("Received run", part.run);
break;
}
case "openai": {
// part.chunk is of type OpenAI.ChatCompletionChunk
console.log("Received OpenAI chunk", part.chunk);
break;
}
case "fetch": {
// part.chunk is a string
console.log("Received fetch chunk", part.chunk);
break;
}
}
}
}
```
## React hooks
If you're building a frontend application, you can use our React hooks to subscribe to streams. Here's an example of how you can use the `useRealtimeRunWithStreams` hook to subscribe to a stream:
```tsx
import { useRealtimeRunWithStreams } from "@trigger.dev/react-hooks";
import type { myTask, STREAMS } from "./trigger/my-task";
// Somewhere in your React component
function MyComponent({ runId, publicAccessToken }: { runId: string; publicAccessToken: string }) {
const { run, streams } = useRealtimeRunWithStreams<typeof myTask, STREAMS>(runId, {
accessToken: publicAccessToken,
});
if (!run) {
return <div>Loading...</div>;
}
return (
<div>
<h1>Run ID: {run.id}</h1>
<h2>Streams:</h2>
<ul>
{Object.entries(streams).map(([key, value]) => (
<li key={key}>
<strong>{key}</strong>: {JSON.stringify(value)}
</li>
))}
</ul>
</div>
);
}
```
Read more about using the React hooks in the [React hooks](/frontend/react-hooks) documentation.
## Usage with the `ai` SDK
The [ai SDK](https://sdk.vercel.ai/docs/introduction) provides a higher-level API for working with AI models. You can use the `ai` SDK with the Streams API by using the `streamText` method:
```ts
import { openai } from "@ai-sdk/openai";
import { logger, metadata, runs, schemaTask } from "@trigger.dev/sdk/v3";
import { streamText } from "ai";
import { z } from "zod";
export type STREAMS = {
openai: string;
};
export const aiStreaming = schemaTask({
id: "ai-streaming",
description: "Stream data from the AI sdk",
schema: z.object({
model: z.string().default("o1-preview"),
prompt: z.string().default("Hello, how are you?"),
}),
run: async ({ model, prompt }) => {
logger.info("Running OpenAI model", { model, prompt });
const result = streamText({
model: openai(model),
prompt,
});
// pass the textStream to the metadata system
const stream = await metadata.stream("openai", result.textStream);
let text = "";
for await (const chunk of stream) {
logger.log("Received chunk", { chunk });
text += chunk; // chunk is a string
}
return { text };
},
});
```
And then render the stream in your frontend:
```tsx
import { useRealtimeRunWithStreams } from "@trigger.dev/react-hooks";
import type { aiStreaming, STREAMS } from "./trigger/ai-streaming";
function MyComponent({ runId, publicAccessToken }: { runId: string; publicAccessToken: string }) {
const { streams } = useRealtimeRunWithStreams<typeof aiStreaming, STREAMS>(runId, {
accessToken: publicAccessToken,
});
if (!streams.openai) {
return <div>Loading...</div>;
}
const text = streams.openai.join(""); // `streams.openai` is an array of strings
return (
<div>
<h2>OpenAI response:</h2>
<p>{text}</p>
</div>
);
}
```
### Using tools and `fullStream`
When calling `streamText`, you can provide a `tools` object that allows the LLM to use additional tools. You can then access the tool call and results using the `fullStream` method:
```ts
import { openai } from "@ai-sdk/openai";
import { logger, metadata, runs, schemaTask } from "@trigger.dev/sdk/v3";
import { streamText, tool, type TextStreamPart } from "ai";
import { z } from "zod";
const tools = {
getWeather: tool({
description: "Get the weather in a location",
parameters: z.object({
location: z.string().describe("The location to get the weather for"),
}),
execute: async ({ location }) => ({
location,
temperature: 72 + Math.floor(Math.random() * 21) - 10,
}),
}),
};
export type STREAMS = {
// Give the stream a type of TextStreamPart along with the tools
openai: TextStreamPart<{ getWeather: typeof tools.getWeather }>;
};
export const aiStreamingWithTools = schemaTask({
id: "ai-streaming-with-tools",
description: "Stream data from the AI SDK and use tools",
schema: z.object({
model: z.string().default("gpt-4o-mini"),
prompt: z
.string()
.default(
"Based on the temperature, will I need to wear extra clothes today in San Fransico? Please be detailed."
),
}),
run: async ({ model, prompt }) => {
logger.info("Running OpenAI model", { model, prompt });
const result = streamText({
model: openai(model),
prompt,
tools, // Pass in the tools to use
maxSteps: 5, // Allow streamText to repeatedly call the model
});
// pass the fullStream to the metadata system
const stream = await metadata.stream("openai", result.fullStream);
let text = "";
for await (const chunk of stream) {
logger.log("Received chunk", { chunk });
// chunk is a TextStreamPart
if (chunk.type === "text-delta") {
text += chunk.textDelta;
}
}
return { text };
},
});
```
Now you can get access to the tool call and results in your frontend:
```tsx
import { useRealtimeRunWithStreams } from "@trigger.dev/react-hooks";
import type { aiStreamingWithTools, STREAMS } from "./trigger/ai-streaming";
function MyComponent({ runId, publicAccessToken }: { runId: string; publicAccessToken: string }) {
const { streams } = useRealtimeRunWithStreams<typeof aiStreamingWithTools, STREAMS>(runId, {
accessToken: publicAccessToken,
});
if (!streams.openai) {
return <div>Loading...</div>;
}
// streams.openai is an array of TextStreamPart
const toolCall = streams.openai.find(
(stream) => stream.type === "tool-call" && stream.toolName === "getWeather"
);
const toolResult = streams.openai.find((stream) => stream.type === "tool-result");
const textDeltas = streams.openai.filter((stream) => stream.type === "text-delta");
const text = textDeltas.map((delta) => delta.textDelta).join("");
const weatherLocation = toolCall ? toolCall.args.location : undefined;
const weather = toolResult ? toolResult.result.temperature : undefined;
return (
<div>
<h2>OpenAI response:</h2>
<p>{text}</p>
<h2>Weather:</h2>
<p>
{weatherLocation
? `The weather in ${weatherLocation} is ${weather} degrees.`
: "No weather data"}
</p>
</div>
);
}
```
### Using `toolTask`
As you can see above, we defined a tool which will be used in the `aiStreamingWithTools` task. You can also define a Trigger.dev task that can be used as a tool, and will automatically be invoked with `triggerAndWait` when the tool is called. This is done using the `toolTask` function:
```ts
import { openai } from "@ai-sdk/openai";
import { logger, metadata, runs, schemaTask, toolTask } from "@trigger.dev/sdk/v3";
import { streamText, tool, type TextStreamPart } from "ai";
import { z } from "zod";
export const getWeather = toolTask({
id: "get-weather",
description: "Get the weather for a location",
// Define the parameters for the tool, which becomes the task payload
parameters: z.object({
location: z.string(),
}),
run: async ({ location }) => {
// return mock data
return {
location,
temperature: 72 + Math.floor(Math.random() * 21) - 10,
};
},
});
export type STREAMS = {
// Give the stream a type of TextStreamPart along with the tools
openai: TextStreamPart<{ getWeather: typeof getWeather.tool }>;
};
export const aiStreamingWithTools = schemaTask({
id: "ai-streaming-with-tools",
description: "Stream data from the AI SDK and use tools",
schema: z.object({
model: z.string().default("gpt-4o-mini"),
prompt: z
.string()
.default(
"Based on the temperature, will I need to wear extra clothes today in San Fransico? Please be detailed."
),
}),
run: async ({ model, prompt }) => {
logger.info("Running OpenAI model", { model, prompt });
const result = streamText({
model: openai(model),
prompt,
tools: {
getWeather: getWeather.tool, // pass weatherTask.tool as a tool
},
maxSteps: 5, // Allow streamText to repeatedly call the model
});
// pass the fullStream to the metadata system
const stream = await metadata.stream("openai", result.fullStream);
let text = "";
for await (const chunk of stream) {
logger.log("Received chunk", { chunk });
// chunk is a TextStreamPart
if (chunk.type === "text-delta") {
text += chunk.textDelta;
}
}
return { text };
},
});
```
# runs.subscribeToBatch
Source: https://trigger.dev/docs/realtime/subscribe-to-batch
Subscribes to all changes for runs in a batch.
<RequestExample>
```ts Example
import { runs } from "@trigger.dev/sdk/v3";
for await (const run of runs.subscribeToBatch("batch_1234")) {
console.log(run);
}
```
</RequestExample>
This function subscribes to all changes for runs in a batch. It returns an async iterator that yields the a run object whenever a run in the batch is updated. The iterator does not complete on it's own, you must manually `break` the loop when you want to stop listening for updates.
### Authentication
This function supports both server-side and client-side authentication. For server-side authentication, use your API key. For client-side authentication, you must generate a public access token with one of the following scopes:
* `read:batch:<batchId>`
* `read:runs` will provide access to all runs (not recommended for production use)
To generate a public access token, use the `auth.createPublicToken` function:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const publicToken = await auth.createPublicToken({
scopes: {
read: {
batch: ["batch_1234"],
},
},
});
```
### Response
The AsyncIterator yields an object with the following properties:
<ParamField path="id" type="string" required>
The run ID.
</ParamField>
<ParamField path="taskIdentifier" type="string" required>
The task identifier.
</ParamField>
<ParamField path="payload" type="object" required>
The input payload for the run.
</ParamField>
<ParamField path="output" type="object">
The output result of the run.
</ParamField>
<ParamField path="createdAt" type="Date" required>
Timestamp when the run was created.
</ParamField>
<ParamField path="updatedAt" type="Date" required>
Timestamp when the run was last updated.
</ParamField>
<ParamField path="number" type="number" required>
Sequential number assigned to the run.
</ParamField>
<ParamField path="status" type="RunStatus" required>
Current status of the run.
<Accordion title="RunStatus enum">
| Status               | Description                                                                                               |
| -------------------- | --------------------------------------------------------------------------------------------------------- |
| `WAITING_FOR_DEPLOY` | Task hasn't been deployed yet but is waiting to be executed                                               |
| `QUEUED`             | Run is waiting to be executed by a worker                                                                 |
| `EXECUTING`          | Run is currently being executed by a worker                                                               |
| `REATTEMPTING`       | Run has failed and is waiting to be retried                                                               |
| `FROZEN`             | Run has been paused by the system, and will be resumed by the system                                      |
| `COMPLETED`          | Run has been completed successfully                                                                       |
| `CANCELED`           | Run has been canceled by the user                                                                         |
| `FAILED`             | Run has been completed with errors                                                                        |
| `CRASHED`            | Run has crashed and won't be retried, most likely the worker ran out of resources, e.g. memory or storage |
| `INTERRUPTED`        | Run was interrupted during execution, mostly this happens in development environments                     |
| `SYSTEM_FAILURE`     | Run has failed to complete, due to an error in the system                                                 |
| `DELAYED`            | Run has been scheduled to run at a specific time                                                          |
| `EXPIRED`            | Run has expired and won't be executed                                                                     |
| `TIMED_OUT`          | Run has reached it's maxDuration and has been stopped                                                     |
</Accordion>
</ParamField>
<ParamField path="durationMs" type="number" required>
Duration of the run in milliseconds.
</ParamField>
<ParamField path="costInCents" type="number" required>
Total cost of the run in cents.
</ParamField>
<ParamField path="baseCostInCents" type="number" required>
Base cost of the run in cents before any additional charges.
</ParamField>
<ParamField path="tags" type="string[]" required>
Array of tags associated with the run.
</ParamField>
<ParamField path="idempotencyKey" type="string">
Key used to ensure idempotent execution.
</ParamField>
<ParamField path="expiredAt" type="Date">
Timestamp when the run expired.
</ParamField>
<ParamField path="ttl" type="string">
Time-to-live duration for the run.
</ParamField>
<ParamField path="finishedAt" type="Date">
Timestamp when the run finished.
</ParamField>
<ParamField path="startedAt" type="Date">
Timestamp when the run started.
</ParamField>
<ParamField path="delayedUntil" type="Date">
Timestamp until which the run is delayed.
</ParamField>
<ParamField path="queuedAt" type="Date">
Timestamp when the run was queued.
</ParamField>
<ParamField path="metadata" type="Record<string, DeserializedJson>">
Additional metadata associated with the run.
</ParamField>
<ParamField path="error" type="SerializedError">
Error information if the run failed.
</ParamField>
<ParamField path="isTest" type="boolean" required>
Indicates whether this is a test run.
</ParamField>
# runs.subscribeToRun
Source: https://trigger.dev/docs/realtime/subscribe-to-run
Subscribes to all changes to a run.
<RequestExample>
```ts Example
import { runs } from "@trigger.dev/sdk/v3";
for await (const run of runs.subscribeToRun("run_1234")) {
console.log(run);
}
```
</RequestExample>
This function subscribes to all changes to a run. It returns an async iterator that yields the run object whenever the run is updated. The iterator will complete when the run is finished.
### Authentication
This function supports both server-side and client-side authentication. For server-side authentication, use your API key. For client-side authentication, you must generate a public access token with one of the following scopes:
* `read:runs`
* `read:runs:<runId>`
To generate a public access token, use the `auth.createPublicToken` function:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const publicToken = await auth.createPublicToken({
scopes: {
read: {
runs: ["run_1234"],
},
},
});
```
### Response
The AsyncIterator yields an object with the following properties:
<ParamField path="id" type="string" required>
The run ID.
</ParamField>
<ParamField path="taskIdentifier" type="string" required>
The task identifier.
</ParamField>
<ParamField path="payload" type="object" required>
The input payload for the run.
</ParamField>
<ParamField path="output" type="object">
The output result of the run.
</ParamField>
<ParamField path="createdAt" type="Date" required>
Timestamp when the run was created.
</ParamField>
<ParamField path="updatedAt" type="Date" required>
Timestamp when the run was last updated.
</ParamField>
<ParamField path="number" type="number" required>
Sequential number assigned to the run.
</ParamField>
<ParamField path="status" type="RunStatus" required>
Current status of the run.
<Accordion title="RunStatus enum">
| Status               | Description                                                                                               |
| -------------------- | --------------------------------------------------------------------------------------------------------- |
| `WAITING_FOR_DEPLOY` | Task hasn't been deployed yet but is waiting to be executed                                               |
| `QUEUED`             | Run is waiting to be executed by a worker                                                                 |
| `EXECUTING`          | Run is currently being executed by a worker                                                               |
| `REATTEMPTING`       | Run has failed and is waiting to be retried                                                               |
| `FROZEN`             | Run has been paused by the system, and will be resumed by the system                                      |
| `COMPLETED`          | Run has been completed successfully                                                                       |
| `CANCELED`           | Run has been canceled by the user                                                                         |
| `FAILED`             | Run has been completed with errors                                                                        |
| `CRASHED`            | Run has crashed and won't be retried, most likely the worker ran out of resources, e.g. memory or storage |
| `INTERRUPTED`        | Run was interrupted during execution, mostly this happens in development environments                     |
| `SYSTEM_FAILURE`     | Run has failed to complete, due to an error in the system                                                 |
| `DELAYED`            | Run has been scheduled to run at a specific time                                                          |
| `EXPIRED`            | Run has expired and won't be executed                                                                     |
| `TIMED_OUT`          | Run has reached it's maxDuration and has been stopped                                                     |
</Accordion>
</ParamField>
<ParamField path="durationMs" type="number" required>
Duration of the run in milliseconds.
</ParamField>
<ParamField path="costInCents" type="number" required>
Total cost of the run in cents.
</ParamField>
<ParamField path="baseCostInCents" type="number" required>
Base cost of the run in cents before any additional charges.
</ParamField>
<ParamField path="tags" type="string[]" required>
Array of tags associated with the run.
</ParamField>
<ParamField path="idempotencyKey" type="string">
Key used to ensure idempotent execution.
</ParamField>
<ParamField path="expiredAt" type="Date">
Timestamp when the run expired.
</ParamField>
<ParamField path="ttl" type="string">
Time-to-live duration for the run.
</ParamField>
<ParamField path="finishedAt" type="Date">
Timestamp when the run finished.
</ParamField>
<ParamField path="startedAt" type="Date">
Timestamp when the run started.
</ParamField>
<ParamField path="delayedUntil" type="Date">
Timestamp until which the run is delayed.
</ParamField>
<ParamField path="queuedAt" type="Date">
Timestamp when the run was queued.
</ParamField>
<ParamField path="metadata" type="Record<string, DeserializedJson>">
Additional metadata associated with the run.
</ParamField>
<ParamField path="error" type="SerializedError">
Error information if the run failed.
</ParamField>
<ParamField path="isTest" type="boolean" required>
Indicates whether this is a test run.
</ParamField>
# runs.subscribeToRunsWithTag
Source: https://trigger.dev/docs/realtime/subscribe-to-runs-with-tag
Subscribes to all changes to runs with a specific tag.
<RequestExample>
```ts Example
import { runs } from "@trigger.dev/sdk/v3";
for await (const run of runs.subscribeToRunsWithTag("user:1234")) {
console.log(run);
}
```
</RequestExample>
This function subscribes to all changes to runs with a specific tag. It returns an async iterator that yields the run object whenever a run with the specified tag is updated. This iterator will never complete, so you must manually break out of the loop when you no longer want to receive updates.
### Authentication
This function supports both server-side and client-side authentication. For server-side authentication, use your API key. For client-side authentication, you must generate a public access token with one of the following scopes:
* `read:runs`
* `read:tags:<tagName>`
To generate a public access token, use the `auth.createPublicToken` function:
```ts
import { auth } from "@trigger.dev/sdk/v3";
// Somewhere in your backend code
const publicToken = await auth.createPublicToken({
scopes: {
read: {
tags: ["user:1234"],
},
},
});
```
### Response
The AsyncIterator yields an object with the following properties:
<ParamField path="id" type="string" required>
The run ID.
</ParamField>
<ParamField path="taskIdentifier" type="string" required>
The task identifier.
</ParamField>
<ParamField path="payload" type="object" required>
The input payload for the run.
</ParamField>
<ParamField path="output" type="object">
The output result of the run.
</ParamField>
<ParamField path="createdAt" type="Date" required>
Timestamp when the run was created.
</ParamField>
<ParamField path="updatedAt" type="Date" required>
Timestamp when the run was last updated.
</ParamField>
<ParamField path="number" type="number" required>
Sequential number assigned to the run.
</ParamField>
<ParamField path="status" type="RunStatus" required>
Current status of the run.
<Accordion title="RunStatus enum">
| Status               | Description                                                                                               |
| -------------------- | --------------------------------------------------------------------------------------------------------- |
| `WAITING_FOR_DEPLOY` | Task hasn't been deployed yet but is waiting to be executed                                               |
| `QUEUED`             | Run is waiting to be executed by a worker                                                                 |
| `EXECUTING`          | Run is currently being executed by a worker                                                               |
| `REATTEMPTING`       | Run has failed and is waiting to be retried                                                               |
| `FROZEN`             | Run has been paused by the system, and will be resumed by the system                                      |
| `COMPLETED`          | Run has been completed successfully                                                                       |
| `CANCELED`           | Run has been canceled by the user                                                                         |
| `FAILED`             | Run has been completed with errors                                                                        |
| `CRASHED`            | Run has crashed and won't be retried, most likely the worker ran out of resources, e.g. memory or storage |
| `INTERRUPTED`        | Run was interrupted during execution, mostly this happens in development environments                     |
| `SYSTEM_FAILURE`     | Run has failed to complete, due to an error in the system                                                 |
| `DELAYED`            | Run has been scheduled to run at a specific time                                                          |
| `EXPIRED`            | Run has expired and won't be executed                                                                     |
| `TIMED_OUT`          | Run has reached it's maxDuration and has been stopped                                                     |
</Accordion>
</ParamField>
<ParamField path="durationMs" type="number" required>
Duration of the run in milliseconds.
</ParamField>
<ParamField path="costInCents" type="number" required>
Total cost of the run in cents.
</ParamField>
<ParamField path="baseCostInCents" type="number" required>
Base cost of the run in cents before any additional charges.
</ParamField>
<ParamField path="tags" type="string[]" required>
Array of tags associated with the run.
</ParamField>
<ParamField path="idempotencyKey" type="string">
Key used to ensure idempotent execution.
</ParamField>
<ParamField path="expiredAt" type="Date">
Timestamp when the run expired.
</ParamField>
<ParamField path="ttl" type="string">
Time-to-live duration for the run.
</ParamField>
<ParamField path="finishedAt" type="Date">
Timestamp when the run finished.
</ParamField>
<ParamField path="startedAt" type="Date">
Timestamp when the run started.
</ParamField>
<ParamField path="delayedUntil" type="Date">
Timestamp until which the run is delayed.
</ParamField>
<ParamField path="queuedAt" type="Date">
Timestamp when the run was queued.
</ParamField>
<ParamField path="metadata" type="Record<string, DeserializedJson>">
Additional metadata associated with the run.
</ParamField>
<ParamField path="error" type="SerializedError">
Error information if the run failed.
</ParamField>
<ParamField path="isTest" type="boolean" required>
Indicates whether this is a test run.
</ParamField>
# Replaying
Source: https://trigger.dev/docs/replaying
A replay is a copy of a run with the same payload but against the latest version in that environment. This is useful if something went wrong and you want to try again with the latest version of your code.
### Replaying from the UI
<Tabs>
<Tab title="From a run">
<Steps>
<Step title="Click the Replay button in the top right">
![Select a task, then in the bottom right
click "Replay"](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/replay-run-action.png)
</Step>
<Step title="Confirm replay settings">
You can edit the payload <Icon icon="circle-1" iconType="solid" size={20} color="F43F47" /> (if available) and choose the environment <Icon icon="circle-2" iconType="solid" size={20} color="F43F47" /> to replay the run in.
![Select a task, then in the bottom right
click "Replay"](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/replay-run-modal.png)
</Step>
</Steps>
</Tab>
<Tab title="Runs list">
<Steps>
<Step title="Click the action button on a run">
![On the runs page, press the triple dot button](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/replay-runs-list.png)
</Step>
<Step title="Click replay">![Click replay](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/replay-runs-list-popover.png)</Step>
</Steps>
</Tab>
</Tabs>
### Replaying using the SDK
You can replay a run using the SDK:
```ts
const replayedRun = await runs.replay(run.id);
```
When you call `trigger()` or `batchTrigger()` on a task you receive back a run handle which has an `id` property. You can use that `id` to replay the run.
You can also access the run id from inside a run. You could write this to your database and then replay it later.
```ts
export const simpleChildTask = task({
id: "simple-child-task",
run: async (payload, { ctx }) => {
// the run ID (and other useful info) is in ctx
const runId = ctx.run.id;
},
});
```
### Bulk replaying
See [Bulk actions](/bulk-actions) for more information.
# Request a feature
Source: https://trigger.dev/docs/request-feature
If you have a feature request or idea for Trigger, we'd love to hear it! You can submit your ideas on our [public roadmap](https://feedback.trigger.dev/). We're always looking for feedback on what to build next, so feel free to submit your ideas or vote on existing ones.
# Roadmap
Source: https://trigger.dev/docs/roadmap
See what's coming up next on our [public roadmap](https://feedback.trigger.dev/roadmap). We're always looking for feedback on what to build next, so feel free to submit your ideas or vote on existing ones.
# Run tests
Source: https://trigger.dev/docs/run-tests
You can use the dashboard to run a test of your tasks.
From the "Test" page in the sidebar of the dashboard you can run a test for any of your tasks, that includes for any environment.
<Steps>
<Step title="Select an environment">
![Select an environment](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-select-environment.png)
</Step>
<Step title="Select the task to test">
![Select the task to test](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-select-task.png)
</Step>
<Step title="Enter a payload">
Select a recent payload as a starting point or enter from scratch. Payloads must be valid JSON â€“ you will see helpful errors if it is not. Press the "Run test" button or use the keyboard shortcut to run the test.
![Enter a payload](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/test-set-payload.png)
</Step>
<Step title="View the run live">![View the run live](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-in-progress.png)</Step>
</Steps>
# Usage
Source: https://trigger.dev/docs/run-usage
Get compute duration and cost from inside a run, or for a specific block of code.
## Getting the run cost and duration
You can get the cost and duration of the current including retries of the same run.
```ts
export const heavyTask = task({
id: "heavy-task",
machine: {
preset: "medium-2x",
},
run: async (payload, { ctx }) => {
// Do some compute
const result = await convertVideo(payload.videoUrl);
// Get the current cost and duration up until this line of code
// This includes the compute time of the previous lines
let currentUsage = usage.getCurrent();
/* currentUsage = {
compute: {
attempt: {
costInCents: 0.01700,
durationMs: 1000,
},
total: {
costInCents: 0.0255,
durationMs: 1500,
},
},
baseCostInCents: 0.0025,
totalCostInCents: 0.028,
}
*/
// In the cloud product we do not count waits towards the compute cost or duration.
// We also don't include time between attempts or before the run starts executing your code.
// So this line does not affect the cost or duration.
await wait.for({ seconds: 5 });
// This will give the same result as before the wait.
currentUsage = usage.getCurrent();
// Do more compute
const result = await convertVideo(payload.videoUrl);
// This would give a different value
currentUsage = usage.getCurrent();
},
});
```
<Note>
In Trigger.dev cloud we do not include time between attempts, before your code executes, or waits
towards the compute cost or duration.
</Note>
## Getting the run cost and duration from your backend
You can use [runs.retrieve()](/management/runs/retrieve) to get a single run or [runs.list()](/management/runs/list) to get a list of runs. The response will include `costInCents` `baseCostInCents` and `durationMs` fields.
```ts single run
import { runs } from "@trigger.dev/sdk/v3";
const run = await runs.retrieve("run-id");
console.log(run.costInCents, run.baseCostInCents, run.durationMs);
const totalCost = run.costInCents + run.baseCostInCents;
```
```ts multiple runs
import { runs } from "@trigger.dev/sdk/v3";
let totalCost = 0;
for await (const run of runs.list({ tag: "user_123456" })) {
totalCost += run.costInCents + run.baseCostInCents;
console.log(run.costInCents, run.baseCostInCents, run.durationMs);
}
console.log("Total cost", totalCost);
```
## Getting the cost and duration of a block of code
You can also wrap code with `usage.measure` to get the cost and duration of that block of code:
```ts
// Inside a task run function, or inside a function that's called from there.
const { result, compute } = await usage.measure(async () => {
//...Do something for 1 second
return {
foo: "bar",
};
});
logger.info("Result", { result, compute });
/* result = {
foo: "bar"
}
compute = {
costInCents: 0.01700,
durationMs: 1000,
}
*/
```
This will work from inside the `run` function, our lifecycle hooks (like `onStart`, `onFailure`, `onSuccess`, etc.), or any function you're calling from the `run` function. It won't work for code that's not executed using Trigger.dev.
# Runs
Source: https://trigger.dev/docs/runs
Understanding the lifecycle of task run execution in Trigger.dev
In Trigger.dev, the concepts of runs and attempts are fundamental to understanding how tasks are executed and managed. This article explains these concepts in detail and provides insights into the various states a run can go through during its lifecycle.
## What are runs?
A run is created when you trigger a task (e.g. calling `yourTask.trigger({ foo: "bar" })`). It represents a single instance of a task being executed and contains the following key information:
* A unique run ID
* The current status of the run
* The payload (input data) for the task
* Lots of other metadata
## The run lifecycle
A run can go through **various** states during its lifecycle. The following diagram illustrates a typical state transition where a single run is triggered and completes successfully:
![Run Lifecycle](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-lifecycle.png)
Runs can also find themselves in lots of other states depending on what's happening at any given time. The following sections describe all the possible states in more detail.
### Initial States
<Icon icon="rectangle-history" iconType="solid" color="#FBBF24" size={17} /> **Waiting for deploy**:
If a task is triggered before it has been deployed, the run enters this state and waits for the task
to be deployed.
<Icon icon="clock" iconType="solid" color="#878C99" size={17} /> **Delayed**: When a run is triggered
with a delay, it enters this state until the specified delay period has passed.
<Icon icon="rectangle-history" iconType="solid" color="#878C99" size={17} /> **Queued**: The run is ready
to be executed and is waiting in the queue.
### Execution States
<Icon icon="spinner-third" iconType="duotone" color="#3B82F6" size={17} /> **Executing**: The task is
currently running.
<Icon icon="arrows-rotate" iconType="solid" color="#3B82F6" size={17} /> **Reattempting**: The task has
failed and is being retried.
<Icon icon="hourglass" iconType="solid" color="#878C99" size={17} /> **Waiting**: You have used a
[triggerAndWait()](/triggering#yourtask-triggerandwait), [batchTriggerAndWait()](/triggering#yourtask-batchtriggerandwait) or a [wait function](/wait). When the wait is complete, the task will resume execution.
### Final States
<Icon icon="circle-check" iconType="solid" color="#28BF5C" size={17} /> **Completed**: The task has successfully
finished execution.
<Icon icon="ban" iconType="solid" color="#878C99" size={17} /> **Canceled**: The run was manually canceled
by the user.
<Icon icon="circle-xmark" iconType="solid" color="#E11D48" size={17} /> **Failed**: The task has failed
to complete successfully.
<Icon icon="alarm-exclamation" iconType="solid" color="#E11D48" size={17} /> **Timed out**: Task has
failed because it exceeded its `maxDuration`.
<Icon icon="fire" iconType="solid" color="#E11D48" size={17} /> **Crashed**: The worker process crashed
during execution (likely due to an Out of Memory error).
<Icon icon="bolt-slash" iconType="solid" color="#E11D48" size={17} /> **Interrupted**: In development
mode, when the CLI is disconnected.
<Icon icon="bug" iconType="solid" color="#E11D48" size={17} /> **System failure**: An unrecoverable system
error has occurred.
<Icon icon="trash-can" iconType="solid" color="#878C99" size={17} /> **Expired**: The run's Time-to-Live
(TTL) has passed before it could start executing.
## Attempts
An attempt represents a single execution of a task within a run. A run can have one or more attempts, depending on the task's retry settings and whether it fails. Each attempt has:
* A unique attempt ID
* A status
* An output (if successful) or an error (if failed)
When a task fails, it will be retried according to its retry settings, creating new attempts until it either succeeds or reaches the retry limit.
![Run with retries](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-with-retries.png)
## Run completion
A run is considered finished when:
1. The last attempt succeeds, or
2. The task has reached its retry limit and all attempts have failed
At this point, the run will have either an output (if successful) or an error (if failed).
## Advanced run features
### Idempotency Keys
When triggering a task, you can provide an idempotency key to ensure the task is executed only once, even if triggered multiple times. This is useful for preventing duplicate executions in distributed systems.
```ts
await yourTask.trigger({ foo: "bar" }, { idempotencyKey: "unique-key" });
```
* If a run with the same idempotency key is already in progress, the new trigger will be ignored.
* If the run has already finished, the previous output or error will be returned.
See our [Idempotency docs](/idempotency) for more information.
### Canceling runs
You can cancel an in-progress run using the API or the dashboard:
```ts
await runs.cancel(runId);
```
When a run is canceled:
â€“ The task execution is stopped
â€“ The run is marked as canceled
â€“ The task will not be retried
â€“ Any in-progress child runs are also canceled
### Time-to-live (TTL)
TTL is a time-to-live setting that defines the maximum duration a run can remain in a queued state before being automatically expired. You can set a TTL when triggering a run:
```ts
await yourTask.trigger({ foo: "bar" }, { ttl: "10m" });
```
If the run hasn't started within the specified TTL, it will automatically expire, returning the status `Expired`. This is useful for time-sensitive tasks where immediate execution is important. For example, when you queue many runs simultaneously and exceed your concurrency limits, some runs might be delayed - using TTL ensures they only execute if they can start within your specified timeframe.
Note that dev runs automatically have a 10-minute TTL. In Staging and Production environments, no TTL is set by default.
![Run with TTL](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-with-ttl.png)
### Delayed runs
You can schedule a run to start after a specified delay:
```ts
await yourTask.trigger({ foo: "bar" }, { delay: "1h" });
```
This is useful for tasks that need to be executed at a specific time in the future.
![Run with delay](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-with-delay.png)
### Replaying runs
You can create a new run with the same payload as a previous run:
```ts
await runs.replay(runId);
```
This is useful for re-running a task with the same input, especially for debugging or recovering from failures. The new run will use the latest version of the task.
You can also replay runs from the dashboard using the same or different payload. Learn how to do this [here](/replaying).
### Waiting for runs
#### triggerAndWait()
The `triggerAndWait()` function triggers a task and then lets you wait for the result before continuing. [Learn more about triggerAndWait()](/triggering#yourtask-triggerandwait).
![Run with triggerAndWait](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-with-triggerAndWait\(\).png)
#### batchTriggerAndWait()
Similar to `triggerAndWait()`, the `batchTriggerAndWait()` function lets you batch trigger a task and wait for all the results [Learn more about batchTriggerAndWait()](/triggering#yourtask-batchtriggerandwait).
![Run with batchTriggerAndWait](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-with-batchTriggerAndWait\(\).png)
### Runs API
#### runs.list()
List runs in a specific environment. You can filter the runs by status, created at, task identifier, version, and more:
```ts
import { runs } from "@trigger.dev/sdk/v3";
// Get the first page of runs, returning up to 20 runs
let page = await runs.list({ limit: 20 });
for (const run of page.data) {
console.log(run);
}
// Keep getting the next page until there are no more runs
while (page.hasNextPage()) {
page = await page.getNextPage();
// Do something with the next page of runs
}
```
You can also use an Async Iterator to get all runs:
```ts
import { runs } from "@trigger.dev/sdk/v3";
for await (const run of runs.list({ limit: 20 })) {
console.log(run);
}
```
You can provide multiple filters to the `list()` function to narrow down the results:
```ts
import { runs } from "@trigger.dev/sdk/v3";
const response = await runs.list({
status: ["QUEUED", "EXECUTING"], // Filter by status
taskIdentifier: ["my-task", "my-other-task"], // Filter by task identifier
from: new Date("2024-04-01T00:00:00Z"), // Filter by created at
to: new Date(),
version: "20241127.2", // Filter by deployment version,
tag: ["tag1", "tag2"], // Filter by tags
batch: "batch_1234", // Filter by batch ID
schedule: "sched_1234", // Filter by schedule ID
});
```
#### runs.retrieve()
Fetch a single run by it's ID:
```ts
import { runs } from "@trigger.dev/sdk/v3";
const run = await runs.retrieve(runId);
```
You can provide the type of the task to correctly type the `run.payload` and `run.output`:
```ts
import { runs } from "@trigger.dev/sdk/v3";
import type { myTask } from "./trigger/myTask";
const run = await runs.retrieve<typeof myTask>(runId);
console.log(run.payload.foo); // string
console.log(run.output.bar); // string
```
If you have just triggered a run, you can pass the entire response object to `retrieve()` and the response will already be typed:
```ts
import { runs, tasks } from "@trigger.dev/sdk/v3";
import type { myTask } from "./trigger/myTask";
const response = await tasks.trigger<typeof myTask>({ foo: "bar" });
const run = await runs.retrieve(response);
console.log(run.payload.foo); // string
console.log(run.output.bar); // string
```
#### runs.cancel()
Cancel a run:
```ts
import { runs } from "@trigger.dev/sdk/v3";
await runs.cancel(runId);
```
#### runs.replay()
Replay a run:
```ts
import { runs } from "@trigger.dev/sdk/v3";
await runs.replay(runId);
```
#### runs.reschedule()
Updates a delayed run with a new delay. Only valid when the run is in the DELAYED state.
```ts
import { runs } from "@trigger.dev/sdk/v3";
await runs.reschedule(runId, { delay: "1h" });
```
### Real-time updates
Subscribe to changes to a specific run in real-time:
```ts
import { runs } from "@trigger.dev/sdk/v3";
for await (const run of runs.subscribeToRun(runId)) {
console.log(run);
}
```
Similar to `runs.retrieve()`, you can provide the type of the task to correctly type the `run.payload` and `run.output`:
```ts
import { runs } from "@trigger.dev/sdk/v3";
import type { myTask } from "./trigger/myTask";
for await (const run of runs.subscribeToRun<typeof myTask>(runId)) {
console.log(run.payload.foo); // string
console.log(run.output?.bar); // string | undefined
}
```
For more on real-time updates, see the [Realtime](/realtime) documentation.
### Triggering runs for undeployed tasks
It's possible to trigger a run for a task that hasn't been deployed yet. The run will enter the "Waiting for deploy" state until the task is deployed. Once deployed, the run will be queued and executed normally.
This feature is particularly useful in CI/CD pipelines where you want to trigger tasks before the deployment is complete.
# Max duration
Source: https://trigger.dev/docs/runs/max-duration
Set a maximum duration for a task to run.
The `maxDuration` parameter sets a maximum compute time limit for tasks. When a task exceeds this duration, it will be automatically stopped. This helps prevent runaway tasks and manage compute resources effectively.
You must set a default maxDuration in your `trigger.config.ts` file, which will apply to all tasks unless overridden:
```ts /config/trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "proj_gtcwttqhhtlasxgfuhxs",
maxDuration: 60, // 60 seconds or 1 minute
});
```
<Note>
The minimum maxDuration is 5 seconds. If you want to avoid timeouts, set this value to a very large number of seconds.
</Note>
You can set the `maxDuration` for a run in the following ways:
* Across all your tasks in the [config](/config/config-file#max-duration)
* On a specific task
* On a specific run when you [trigger a task](/triggering#maxduration)
## How it works
The `maxDuration` is set in seconds, and is compared to the CPU time elapsed since the start of a single execution (which we call [attempts](/runs#attempts)) of the task. The CPU time is the time that the task has been actively running on the CPU, and does not include time spent waiting during the following:
* `wait.for` calls
* `triggerAndWait` calls
* `batchTriggerAndWait` calls
You can inspect the CPU time of a task inside the run function with our `usage` utility:
```ts /trigger/max-duration.ts
import { task, usage } from "@trigger.dev/sdk/v3";
export const maxDurationTask = task({
id: "max-duration-task",
maxDuration: 300, // 300 seconds or 5 minutes
run: async (payload: any, { ctx }) => {
let currentUsage = usage.getCurrent();
currentUsage.attempt.durationMs; // The CPU time in milliseconds since the start of the run
},
});
```
The above value will be compared to the `maxDuration` you set. If the task exceeds the `maxDuration`, it will be stopped with the following error:
![Max duration error](https://mintlify.s3.us-west-1.amazonaws.com/trigger/runs/max-duration-error.png)
## Configuring for a task
You can set a `maxDuration` on a specific task:
```ts /trigger/max-duration-task.ts
import { task } from "@trigger.dev/sdk/v3";
export const maxDurationTask = task({
id: "max-duration-task",
maxDuration: 300, // 300 seconds or 5 minutes
run: async (payload: any, { ctx }) => {
//...
},
});
```
This will override the default `maxDuration` set in the config file. If you have a config file with a default `maxDuration` of 60 seconds, and you set a `maxDuration` of 300 seconds on a task, the task will run for 300 seconds.
You can "turn off" the Max duration set in your config file for a specific task like so:
```ts /trigger/max-duration-task.ts
import { task, timeout } from "@trigger.dev/sdk/v3";
export const maxDurationTask = task({
id: "max-duration-task",
maxDuration: timeout.None, // No max duration
run: async (payload: any, { ctx }) => {
//...
},
});
```
## Configuring for a run
You can set a `maxDuration` on a specific run when you trigger a task:
```ts /trigger/max-duration.ts
import { maxDurationTask } from "./trigger/max-duration-task";
// Trigger the task with a maxDuration of 300 seconds
const run = await maxDurationTask.trigger(
{ foo: "bar" },
{
maxDuration: 300, // 300 seconds or 5 minutes
}
);
```
You can also set the `maxDuration` to `timeout.None` to turn off the max duration for a specific run:
```ts /trigger/max-duration.ts
import { maxDurationTask } from "./trigger/max-duration-task";
import { timeout } from "@trigger.dev/sdk/v3";
// Trigger the task with no maxDuration
const run = await maxDurationTask.trigger(
{ foo: "bar" },
{
maxDuration: timeout.None, // No max duration
}
);
```
## maxDuration in run context
You can access the `maxDuration` set for a run in the run context:
```ts /trigger/max-duration-task.ts
import { task } from "@trigger.dev/sdk/v3";
export const maxDurationTask = task({
id: "max-duration-task",
maxDuration: 300, // 300 seconds or 5 minutes
run: async (payload: any, { ctx }) => {
console.log(ctx.run.maxDuration); // 300
},
});
```
## maxDuration and lifecycle functions
When a task run exceeds the `maxDuration`, the lifecycle functions `cleanup`, `onSuccess`, and `onFailure` will not be called.
# Run metadata
Source: https://trigger.dev/docs/runs/metadata
Attach a small amount of data to a run and update it as the run progresses.
You can attach up to 256KB of metadata to a run, which you can then access from inside the run function, via the API, Realtime, and in the dashboard. You can use metadata to store additional, structured information on a run. For example, you could store your userâ€™s full name and corresponding unique identifier from your system on every task that is associated with that user. Or you could store the progress of a long-running task, or intermediate results that you want to access later.
## Usage
Add metadata to a run when triggering by passing it as an object to the `trigger` function:
```ts
const handle = await myTask.trigger(
{ message: "hello world" },
{ metadata: { user: { name: "Eric", id: "user_1234" } } }
);
```
You can get the current metadata at any time by calling `metadata.get()` or `metadata.current()` (only inside a run):
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Get the whole metadata object
const currentMetadata = metadata.current();
console.log(currentMetadata);
// Get a specific key
const user = metadata.get("user");
console.log(user.name); // "Eric"
},
});
```
Any of these methods can be called anywhere "inside" the run function, or a function called from the run function:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
doSomeWork();
},
});
async function doSomeWork() {
// Set the value of a specific key
metadata.set("progress", 0.5);
}
```
If you call any of the metadata methods outside of the run function, they will have no effect:
```ts
import { metadata } from "@trigger.dev/sdk/v3";
// Somewhere outside of the run function
function doSomeWork() {
metadata.set("progress", 0.5); // This will do nothing
}
```
This means it's safe to call these methods anywhere in your code, and they will only have an effect when called inside the run function.
<Note>
Calling `metadata.current()` or `metadata.get()` outside of the run function will always return
undefined.
</Note>
These methods also work inside any task lifecycle hook, either attached to the specific task or the global hooks defined in your `trigger.config.ts` file.
<CodeGroup>
```ts myTasks.ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Your run function work here
},
onStart: async () => {
metadata.set("progress", 0.5);
},
onSuccess: async () => {
metadata.set("progress", 1.0);
},
});
```
```ts trigger.config.ts
import { defineConfig, metadata } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "proj_1234",
onStart: async () => {
metadata.set("progress", 0.5);
},
});
```
</CodeGroup>
## Updates API
One of the more powerful features of metadata is the ability to update it as the run progresses. This is useful for tracking the progress of a run, storing intermediate results, or storing any other information that changes over time. (Combining metadata with [Realtime](/realtime) can give you a live view of the progress of your runs.)
All metadata update methods (accept for `flush` and `stream`) are synchronous and will not block the run function. We periodically flush metadata to the database in the background, so you can safely update the metadata inside a run as often as you need to, without worrying about impacting the run's performance.
### set
Set the value of a key in the metadata object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Do some more work
metadata.set("progress", 0.5);
// Do even more work
metadata.set("progress", 1.0);
},
});
```
### del
Delete a key from the metadata object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Do some more work
metadata.set("progress", 0.5);
// Remove the progress key
metadata.del("progress");
},
});
```
### replace
Replace the entire metadata object with a new object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Replace the metadata object
metadata.replace({ user: { name: "Eric", id: "user_1234" } });
},
});
```
### append
Append a value to an array in the metadata object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Append a value to an array
metadata.append("logs", "Step 1 complete");
console.log(metadata.get("logs")); // ["Step 1 complete"]
},
});
```
### remove
Remove a value from an array in the metadata object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Append a value to an array
metadata.append("logs", "Step 1 complete");
// Remove a value from the array
metadata.remove("logs", "Step 1 complete");
console.log(metadata.get("logs")); // []
},
});
```
### increment
Increment a numeric value in the metadata object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Increment a value
metadata.increment("progress", 0.4);
console.log(metadata.get("progress")); // 0.5
},
});
```
### decrement
Decrement a numeric value in the metadata object:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.5);
// Decrement a value
metadata.decrement("progress", 0.4);
console.log(metadata.get("progress")); // 0.1
},
});
```
### stream
Capture a stream of values and make the stream available when using Realtime. See our [Realtime streams](/realtime/streams) documentation for more information.
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
const readableStream = new ReadableStream({
start(controller) {
controller.enqueue("Step 1 complete");
controller.enqueue("Step 2 complete");
controller.enqueue("Step 3 complete");
controller.close();
},
});
// IMPORTANT: you must await the stream method
const stream = await metadata.stream("logs", readableStream);
// You can read from the returned stream locally
for await (const value of stream) {
console.log(value);
}
},
});
```
`metadata.stream` accepts any `AsyncIterable` or `ReadableStream` object. The stream will be captured and made available in the Realtime API. So for example, you could pass the body of a fetch response to `metadata.stream` to capture the response body and make it available in Realtime:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { url: string }) => {
logger.info("Streaming response", { url });
const response = await fetch(url);
if (!response.body) {
throw new Error("Response body is not readable");
}
const stream = await metadata.stream(
"fetch",
response.body.pipeThrough(new TextDecoderStream())
);
let text = "";
for await (const chunk of stream) {
logger.log("Received chunk", { chunk });
text += chunk;
}
return { text };
},
});
```
Or the results of a streaming call to the OpenAI SDK:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
import OpenAI from "openai";
const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});
export const myTask = task({
id: "my-task",
run: async (payload: { prompt: string }) => {
const completion = await openai.chat.completions.create({
messages: [{ role: "user", content: payload.prompt }],
model: "gpt-3.5-turbo",
stream: true,
});
const stream = await metadata.stream("openai", completion);
let text = "";
for await (const chunk of stream) {
logger.log("Received chunk", { chunk });
text += chunk.choices.map((choice) => choice.delta?.content).join("");
}
return { text };
},
});
```
### flush
Flush the metadata to the database. The SDK will automatically flush the metadata periodically, so you don't need to call this method unless you need to ensure that the metadata is persisted immediately.
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Flush the metadata to the database
await metadata.flush();
},
});
```
## Fluent API
All of the update methods can be chained together in a fluent API:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
metadata
.set("progress", 0.1)
.append("logs", "Step 1 complete")
.increment("progress", 0.4)
.decrement("otherProgress", 0.1);
},
});
```
## Parent & root updates
Tasks that have been triggered by a parent task (a.k.a. a "child task") can update the metadata of the parent task. This is useful for propagating progress information up the task hierarchy. You can also update the metadata of the root task (root = the initial task that was triggered externally, like from your backend).
To update the parent task's metadata, use the `metadata.parent` accessor:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myParentTask = task({
id: "my-parent-task",
run: async (payload: { message: string }) => {
// Do some work
metadata.set("progress", 0.1);
// Trigger a child task
await childTask.triggerAndWait({ message: "hello world" });
},
});
export const childTask = task({
id: "child-task",
run: async (payload: { message: string }) => {
// This will update the parent task's metadata
metadata.parent.set("progress", 0.5);
},
});
```
All of the update methods are available on `metadata.parent` and `metadata.root`:
```ts
metadata.parent.set("progress", 0.5);
metadata.parent.append("logs", "Step 1 complete");
metadata.parent.remove("logs", "Step 1 complete");
metadata.parent.increment("progress", 0.4);
metadata.parent.decrement("otherProgress", 0.1);
metadata.parent.stream("llm", readableStream);
metadata.root.set("progress", 0.5);
metadata.root.append("logs", "Step 1 complete");
metadata.root.remove("logs", "Step 1 complete");
metadata.root.increment("progress", 0.4);
metadata.root.decrement("otherProgress", 0.1);
metadata.root.stream("llm", readableStream);
```
You can also chain the update methods together:
```ts
metadata.parent
.set("progress", 0.1)
.append("logs", "Step 1 complete")
.increment("progress", 0.4)
.decrement("otherProgress", 0.1);
```
### Example
An example of where you might use parent and root updates is in a task that triggers multiple child tasks in parallel. You could use the parent metadata to track the progress of the child tasks and update the parent task's progress as each child task completes:
```ts
import { CSVRow, UploadedFileData, parseCSVFromUrl } from "@/utils";
import { batch, logger, metadata, schemaTask } from "@trigger.dev/sdk/v3";
export const handleCSVRow = schemaTask({
id: "handle-csv-row",
schema: CSVRow,
run: async (row, { ctx }) => {
// Do some work with the row
// Update the parent task's metadata with the progress of this row
metadata.parent.increment("processedRows", 1).append("rowRuns", ctx.run.id);
return row;
},
});
export const handleCSVUpload = schemaTask({
id: "handle-csv-upload",
schema: UploadedFileData,
run: async (file, { ctx }) => {
metadata.set("status", "fetching");
const rows = await parseCSVFromUrl(file.url);
metadata.set("status", "processing").set("totalRows", rows.length);
const results = await batch.triggerAndWait<typeof handleCSVRow>(
rows.map((row) => ({ id: "handle-csv-row", payload: row }))
);
metadata.set("status", "complete");
return {
file,
rows,
results,
};
},
});
```
Combined with [Realtime](/realtime), you could use this to show a live progress bar of the CSV processing in your frontend, like this:
<video src="https://content.trigger.dev/csv-upload-realtime.mp4" preload="auto" controls={true} loop muted autoPlay={true} width="100%" height="100%" />
## Metadata propagation
Metadata is NOT propagated to child tasks. If you want to pass metadata to a child task, you must do so explicitly:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
await metadata.set("progress", 0.5);
await childTask.trigger(payload, { metadata: metadata.current() });
},
});
```
## Type-safe metadata
The metadata APIs are currently loosely typed, accepting any object that is JSON-serializable:
```ts
// âŒ You can't pass a top-level array
const handle = await myTask.trigger(
{ message: "hello world" },
{ metadata: [{ user: { name: "Eric", id: "user_1234" } }] }
);
// âŒ You can't pass a string as the entire metadata:
const handle = await myTask.trigger(
{ message: "hello world" },
{ metadata: "this is the metadata" }
);
// âŒ You can't pass in a function or a class instance
const handle = await myTask.trigger(
{ message: "hello world" },
{ metadata: { user: () => "Eric", classInstance: new HelloWorld() } }
);
// âœ… You can pass in dates and other JSON-serializable objects
const handle = await myTask.trigger(
{ message: "hello world" },
{ metadata: { user: { name: "Eric", id: "user_1234" }, date: new Date() } }
);
```
<Note>
If you pass in an object like a Date, it will be serialized to a string when stored in the
metadata. That also means that when you retrieve it using `metadata.get()` or
`metadata.current()`, you will get a string back. You will need to deserialize it back to a Date
object if you need to use it as a Date.
</Note>
We recommend wrapping the metadata API in a [Zod](https://zod.dev) schema (or your validator library of choice) to provide type safety:
```ts
import { task, metadata } from "@trigger.dev/sdk/v3";
import { z } from "zod";
const Metadata = z.object({
user: z.object({
name: z.string(),
id: z.string(),
}),
date: z.coerce.date(), // Coerce the date string back to a Date object
});
type Metadata = z.infer<typeof Metadata>;
// Helper function to get the metadata object in a type-safe way
// Note: you would probably want to use .safeParse instead of .parse in a real-world scenario
function getMetadata() {
return Metadata.parse(metadata.current());
}
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }) => {
const metadata = getMetadata();
console.log(metadata.user.name); // "Eric"
console.log(metadata.user.id); // "user_1234"
console.log(metadata.date); // Date object
},
});
```
## Inspecting metadata
### Dashboard
You can view the metadata for a run in the Trigger.dev dashboard. The metadata will be displayed in the run details view:
![View run metadata dashboard](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/run-metadata.png)
### API
You can use the `runs.retrieve()` SDK function to get the metadata for a run:
```ts
import { runs } from "@trigger.dev/sdk/v3";
const run = await runs.retrieve("run_1234");
console.log(run.metadata);
```
See the [API reference](/management/runs/retrieve) for more information.
## Size limit
The maximum size of the metadata object is 256KB. If you exceed this limit, the SDK will throw an error. If you are self-hosting Trigger.dev, you can increase this limit by setting the `TASK_RUN_METADATA_MAXIMUM_SIZE` environment variable. For example, to increase the limit to 16KB, you would set `TASK_RUN_METADATA_MAXIMUM_SIZE=16384`.
# Tags
Source: https://trigger.dev/docs/tags
Tags allow you to easily filter runs in the dashboard and when using the SDK.
## What are tags?
We support up to 5 tags per run. Each one must be a string between 1 and 64 characters long.
We recommend prefixing your tags with their type and then an underscore or colon. For example, `user_123456` or `video:123`.
<Info>
Many great APIs, like Stripe, already prefix their IDs with the type and an underscore. Like
`cus_123456` for a customer.
</Info>
We don't enforce prefixes but if you use them you'll find it easier to filter and it will be clearer what the tag represents.
## How to add tags
There are two ways to add tags to a run:
1. When triggering the run.
2. Inside the `run` function, using `tags.add()`.
### 1. Adding tags when triggering the run
You can add tags when triggering a run using the `tags` option. All the different [trigger](/triggering) methods support this.
<CodeGroup>
```ts trigger
const handle = await myTask.trigger(
{ message: "hello world" },
{ tags: ["user_123456", "org_abcdefg"] }
);
```
```ts batchTrigger
const batch = await myTask.batchTrigger([
{
payload: { message: "foo" },
options: { tags: "product_123456" },
},
{
payload: { message: "bar" },
options: { tags: ["user_123456", "product_3456789"] },
},
]);
```
</CodeGroup>
This will create a run with the tags `user_123456` and `org_abcdefg`. They look like this in the runs table:
![How tags appear in the dashboard](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/tags-org-user.png)
### 2. Adding tags inside the `run` function
Use the `tags.add()` function to add tags to a run from inside the `run` function. This will add the tag `product_1234567` to the run:
```ts
import { task, tags } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { message: string }, { ctx }) => {
// Get the tags from when the run was triggered using the context
// This is not updated if you add tags during the run
logger.log("Tags from the run context", { tags: ctx.run.tags });
// Add tags during the run (a single string or array of strings)
await tags.add("product_1234567");
},
});
```
Reminder: you can only have up to 5 tags per run. If you call `tags.add()` and the total number of tags will be more than 5 we log an error and ignore the new tags. That includes tags from triggering and from inside the run function.
### Propagating tags to child runs
Tags do not propagate to child runs automatically. By default runs have no tags and you have to set them explicitly.
It's easy to propagate tags if you want:
```ts
export const myTask = task({
id: "my-task",
run: async (payload: Payload, { ctx }) => {
// Pass the tags from ctx into the child run
const { id } = await otherTask.trigger(
{ message: "triggered from myTask" },
{ tags: ctx.run.tags }
);
},
});
```
## Filtering runs by tags
You can filter runs by tags in the dashboard and in the SDK.
### In the dashboard
On the Runs page open the filter menu, choose "Tags" and then start typing in the name of the tag you want to filter by. You can select it and it will restrict the results to only runs with that tag. You can add multiple tags to filter by more than one.
![Filter by tags](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/tags-filtering.png)
### Using `runs.list()`
You can provide filters to the `runs.list` SDK function, including an array of tags.
```ts
import { runs } from "@trigger.dev/sdk/v3";
// Loop through all runs with the tag "user_123456" that have completed
for await (const run of runs.list({ tag: "user_123456", status: ["COMPLETED"] })) {
console.log(run.id, run.taskIdentifier, run.finishedAt, run.tags);
}
```
# Tasks: Overview
Source: https://trigger.dev/docs/tasks/overview
Tasks are functions that can run for a long time and provide strong resilience to failure.
There are different types of tasks including regular tasks and [scheduled tasks](/tasks/scheduled).
## Hello world task and how to trigger it
Here's an incredibly simple task:
```ts /trigger/hello-world.ts
import { task } from "@trigger.dev/sdk/v3";
//1. You need to export each task, even if it's a subtask
export const helloWorld = task({
//2. Use a unique id for each task
id: "hello-world",
//3. The run function is the main function of the task
run: async (payload: { message: string }) => {
//4. You can write code that runs for a long time here, there are no timeouts
console.log(payload.message);
},
});
```
<Note>
You must `export` each task, even subtasks inside the same file. When exported they are accessible so their configuration can be registered with the platform.
</Note>
You can trigger this in two ways:
1. From the dashboard [using the "Test" feature](/run-tests).
2. Trigger it from your backend code. See the [full triggering guide here](/triggering).
Here's how to trigger a single run from elsewhere in your code:
```ts Your backend code
import { helloWorld } from "./trigger/hello-world";
async function triggerHelloWorld() {
//This triggers the task and returns a handle
const handle = await helloWorld.trigger({ message: "Hello world!" });
//You can use the handle to check the status of the task, cancel and retry it.
console.log("Task is running with handle", handle.id);
}
```
You can also [trigger a task from another task](/triggering), and wait for the result.
## Defining a `task`
The task function takes an object with the following fields.
### The `id` field
This is used to identify your task so it can be triggered, managed, and you can view runs in the dashboard. This must be unique in your project â€“ we recommend making it descriptive and unique.
### The `run` function
Your custom code inside `run()` will be executed when your task is triggered. Itâ€™s an async function that has two arguments:
1. The run payload - the data that you pass to the task when you trigger it.
2. An object with `ctx` about the run (Context), and any output from the optional `init` function that runs before every run attempt.
Anything you return from the `run` function will be the result of the task. Data you return must be JSON serializable: strings, numbers, booleans, arrays, objects, and null.
### `retry` options
A task is retried if an error is thrown, by default we retry 3 times.
You can set the number of retries and the delay between retries in the `retry` field:
```ts /trigger/retry.ts
export const taskWithRetries = task({
id: "task-with-retries",
retry: {
maxAttempts: 10,
factor: 1.8,
minTimeoutInMs: 500,
maxTimeoutInMs: 30_000,
randomize: false,
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
For more information read [the retrying guide](/errors-retrying).
It's also worth mentioning that you can [retry a block of code](/errors-retrying) inside your tasks as well.
### `queue` options
Queues allow you to control the concurrency of your tasks. This allows you to have one-at-a-time execution and parallel executions. There are also more advanced techniques like having different concurrencies for different sets of your users. For more information read [the concurrency & queues guide](/queue-concurrency).
```ts /trigger/one-at-a-time.ts
export const oneAtATime = task({
id: "one-at-a-time",
queue: {
concurrencyLimit: 1,
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
### `machine` options
Some tasks require more vCPUs or GBs of RAM. You can specify these requirements in the `machine` field. For more information read [the machines guide](/machines).
```ts /trigger/heavy-task.ts
export const heavyTask = task({
id: "heavy-task",
machine: {
preset: "large-1x", // 4 vCPU, 8 GB RAM
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
### `maxDuration` option
By default tasks can execute indefinitely, which can be great! But you also might want to set a `maxDuration` to prevent a task from running too long. You can set the `maxDuration` on a task, and all runs of that task will be stopped if they exceed the duration.
```ts /trigger/long-task.ts
export const longTask = task({
id: "long-task",
maxDuration: 300, // 300 seconds or 5 minutes
run: async (payload: any, { ctx }) => {
//...
},
});
```
See our [maxDuration guide](/runs/max-duration) for more information.
## Lifecycle functions
![Lifecycle functions](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/lifecycle-functions.png)
### `init` function
This function is called before a run attempt:
```ts /trigger/init.ts
export const taskWithInit = task({
id: "task-with-init",
init: async (payload, { ctx }) => {
//...
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
You can also return data from the `init` function that will be available in the params of the `run`, `cleanup`, `onSuccess`, and `onFailure` functions.
```ts /trigger/init-return.ts
export const taskWithInitReturn = task({
id: "task-with-init-return",
init: async (payload, { ctx }) => {
return { someData: "someValue" };
},
run: async (payload: any, { ctx, init }) => {
console.log(init.someData); // "someValue"
},
});
```
<Info>Errors thrown in the `init` function are ignored.</Info>
### `cleanup` function
This function is called after the `run` function is executed, regardless of whether the run was successful or not. It's useful for cleaning up resources, logging, or other side effects.
```ts /trigger/cleanup.ts
export const taskWithCleanup = task({
id: "task-with-cleanup",
cleanup: async (payload, { ctx }) => {
//...
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
<Info>Errors thrown in the `cleanup` function will fail the attempt.</Info>
### `middleware` function
This function is called before the `run` function, it allows you to wrap the run function with custom code.
<Info>
An error thrown in `middleware` is just like an uncaught error in the run function: it will
propagate through to `handleError()` and then will fail the attempt (causing a retry).
</Info>
### `onStart` function
When a task run starts, the `onStart` function is called. It's useful for sending notifications, logging, and other side effects. This function will only be called one per run (not per retry). If you want to run code before each retry, use the `init` function.
```ts /trigger/on-start.ts
export const taskWithOnStart = task({
id: "task-with-on-start",
onStart: async (payload, { ctx }) => {
//...
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
You can also define an `onStart` function in your `trigger.config.ts` file to get notified when any task starts.
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "proj_1234",
onStart: async (payload, { ctx }) => {
console.log("Task started", ctx.task.id);
},
});
```
<Info>Errors thrown in the `onStart` function are ignored.</Info>
### `onSuccess` function
When a task run succeeds, the `onSuccess` function is called. It's useful for sending notifications, logging, syncing state to your database, or other side effects.
```ts /trigger/on-success.ts
export const taskWithOnSuccess = task({
id: "task-with-on-success",
onSuccess: async (payload, output, { ctx }) => {
//...
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
You can also define an `onSuccess` function in your `trigger.config.ts` file to get notified when any task succeeds.
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "proj_1234",
onSuccess: async (payload, output, { ctx }) => {
console.log("Task succeeded", ctx.task.id);
},
});
```
<Info>Errors thrown in the `onSuccess` function are ignored.</Info>
### `onFailure` function
When a task run fails, the `onFailure` function is called. It's useful for sending notifications, logging, or other side effects. It will only be executed once the task run has exhausted all its retries.
```ts /trigger/on-failure.ts
export const taskWithOnFailure = task({
id: "task-with-on-failure",
onFailure: async (payload, error, { ctx }) => {
//...
},
run: async (payload: any, { ctx }) => {
//...
},
});
```
You can also define an `onFailure` function in your `trigger.config.ts` file to get notified when any task fails.
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "proj_1234",
onFailure: async (payload, error, { ctx }) => {
console.log("Task failed", ctx.task.id);
},
});
```
<Info>Errors thrown in the `onFailure` function are ignored.</Info>
<Note>
`onFailure` doesnâ€™t fire for some of the run statuses like `Crashed`, `System failures`, and `Canceled`.
</Note>
### `handleError` functions
You can define a function that will be called when an error is thrown in the `run` function, that allows you to control how the error is handled and whether the task should be retried.
Read more about `handleError` in our [Errors and Retrying guide](/errors-retrying).
<Info>Uncaught errors will throw a special internal error of the type `HANDLE_ERROR_ERROR`.</Info>
## Next steps
<CardGroup>
<Card title="Triggering" icon="bolt" href="/triggering">
Learn how to trigger your tasks from your code.
</Card>
<Card title="Writing tasks" icon="wand-magic-sparkles" href="/writing-tasks-introduction">
Tasks are the core of Trigger.dev. Learn how to write them.
</Card>
</CardGroup>
# Scheduled tasks (cron)
Source: https://trigger.dev/docs/tasks/scheduled
A task that is triggered on a recurring schedule using cron syntax.
<Note>Scheduled tasks are only for recurring tasks. If you want to trigger a one-off task at a future time, you should [use the delay option](/triggering#delay).</Note>
## Defining a scheduled task
This task will run when any of the attached schedules trigger. They have a predefined payload with some useful properties:
```ts
import { schedules } from "@trigger.dev/sdk/v3";
export const firstScheduledTask = schedules.task({
id: "first-scheduled-task",
run: async (payload) => {
//when the task was scheduled to run
//note this will be slightly different from new Date() because it takes a few ms to run the task
console.log(payload.timestamp); //is a Date object
//when the task was last run
//this can be undefined if it's never been run
console.log(payload.lastTimestamp); //is a Date object or undefined
//the timezone the schedule was registered with, defaults to "UTC"
//this is in IANA format, e.g. "America/New_York"
//See the full list here: https://cloud.trigger.dev/timezones
console.log(payload.timezone); //is a string
//If you want to output the time in the user's timezone do this:
const formatted = payload.timestamp.toLocaleString("en-US", {
timeZone: payload.timezone,
});
//the schedule id (you can have many schedules for the same task)
//using this you can remove the schedule, update it, etc
console.log(payload.scheduleId); //is a string
//you can optionally provide an external id when creating the schedule
//usually you would set this to a userId or some other unique identifier
//this can be undefined if you didn't provide one
console.log(payload.externalId); //is a string or undefined
//the next 5 dates this task is scheduled to run
console.log(payload.upcoming); //is an array of Date objects
},
});
```
You can see from the comments that the payload has several useful properties:
* `timestamp` - the time the task was scheduled to run, as a UTC date.
* `lastTimestamp` - the time the task was last run, as a UTC date.
* `timezone` - the timezone the schedule was registered with, defaults to "UTC". In IANA format, e.g. "America/New\_York".
* `scheduleId` - the id of the schedule that triggered the task
* `externalId` - the external id you (optionally) provided when creating the schedule
* `upcoming` - the next 5 times the task is scheduled to run
<Note>
This task will NOT get triggered on a schedule until you attach a schedule to it. Read on for how
to do that.
</Note>
Like all tasks they don't have timeouts, they should be placed inside a [/trigger folder](/config/config-file), and you [can configure them](/tasks/overview#defining-a-task).
## How to attach a schedule
Now that we've defined a scheduled task, we need to define when it will actually run. To do this we need to attach one or more schedules.
There are two ways of doing this:
* **Declarative:** defined on your `schedules.task`. They sync when you run the dev command or deploy.
* **Imperative:** created from the dashboard or by using the imperative SDK functions like `schedules.create()`.
<Info>
A scheduled task can have multiple schedules attached to it, including a declarative schedule
and/or many imperative schedules.
</Info>
### Declarative schedules
These sync when you run the [dev](/cli-dev) or [deploy](/cli-deploy) commands.
To create them you add the `cron` property to your `schedules.task()`. This property is optional and is only used if you want to add a declarative schedule to your task:
```ts
export const firstScheduledTask = schedules.task({
id: "first-scheduled-task",
//every two hours (UTC timezone)
cron: "0 */2 * * *",
run: async (payload, { ctx }) => {
//do something
},
});
```
If you use a string it will be in UTC. Alternatively, you can specify a timezone like this:
```ts
export const secondScheduledTask = schedules.task({
id: "second-scheduled-task",
cron: {
//5am every day Tokyo time
pattern: "0 5 * * *",
timezone: "Asia/Tokyo",
},
run: async (payload) => {},
});
```
When you run the [dev](/cli-dev) or [deploy](/cli-deploy) commands, declarative schedules will be synced. If you add, delete or edit the `cron` property it will be updated when you run these commands. You can view your schedules on the Schedules page in the dashboard.
### Imperative schedules
Alternatively you can explicitly attach schedules to a `schedules.task`. You can do this in the Schedules page in the dashboard by just pressing the "New schedule" button, or you can use the SDK to create schedules.
The advantage of imperative schedules is that they can be created dynamically, for example, you could create a schedule for each user in your database. They can also be activated, disabled, edited, and deleted without deploying new code by using the SDK or dashboard.
To use imperative schedules you need to do two things:
1. Define a task in your code using `schedules.task()`.
2. Attach 1+ schedules to the task either using the dashboard or the SDK.
## Supported cron syntax
```
*    *    *    *    *
â”¬    â”¬    â”¬    â”¬    â”¬
â”‚    â”‚    â”‚    â”‚    |
â”‚    â”‚    â”‚    â”‚    â”” day of week (0 - 7, 1L - 7L) (0 or 7 is Sun)
â”‚    â”‚    â”‚    â””â”€â”€â”€â”€â”€ month (1 - 12)
â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of month (1 - 31, L)
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)
```
"L" means the last. In the "day of week" field, 1L means the last Monday of the month. In the "day of month" field, L means the last day of the month.
We do not support seconds in the cron syntax.
## When schedules won't trigger
There are two situations when a scheduled task won't trigger:
* For Dev environments scheduled tasks will only trigger if you're running the dev CLI.
* For Staging/Production environments scheduled tasks will only trigger if the task is in the current deployment (latest version). We won't trigger tasks from previous deployments.
## Attaching schedules in the dashboard
You need to attach a schedule to a task before it will run on a schedule. You can attach static schedules in the dashboard:
<Steps>
<Step title="Go to the Schedules page">
In the sidebar select the "Schedules" page, then press the "New schedule" button. Or you can
follow the onboarding and press the create in dashboard button. ![Blank schedules
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/schedules-blank.png)
</Step>
<Step title="Create your schedule">
Fill in the form and press "Create schedule" when you're done. ![Environment variables
page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/schedules-create.png)
These are the options when creating a schedule:
| Name              | Description                                                                                   |
| ----------------- | --------------------------------------------------------------------------------------------- |
| Task              | The id of the task you want to attach to.                                                     |
| Cron pattern      | The schedule in cron format.                                                                  |
| Timezone          | The timezone the schedule will run in. Defaults to "UTC"                                      |
| External id       | An optional external id, usually you'd use a userId.                                          |
| Deduplication key | An optional deduplication key. If you pass the same value, it will update rather than create. |
| Environments      | The environments this schedule will run in.                                                   |
</Step>
</Steps>
## Attaching schedules with the SDK
You call `schedules.create()` to create a schedule from your code. Here's the simplest possible example:
```ts
const createdSchedule = await schedules.create({
//The id of the scheduled task you want to attach to.
task: firstScheduledTask.id,
//The schedule in cron format.
cron: "0 0 * * *",
//this is required, it prevents you from creating duplicate schedules. It will update the schedule if it already exists.
deduplicationKey: "my-deduplication-key",
});
```
<Note>The `task` id must be a task that you defined using `schedules.task()`.</Note>
You can create many schedules with the same `task`, `cron`, and `externalId` but only one with the same `deduplicationKey`.
This means you can have thousands of schedules attached to a single task, but only one schedule per `deduplicationKey`. Here's an example with all the options:
```ts
const createdSchedule = await schedules.create({
//The id of the scheduled task you want to attach to.
task: firstScheduledTask.id,
//The schedule in cron format.
cron: "0 0 * * *",
// Optional, it defaults to "UTC". In IANA format, e.g. "America/New_York".
// In this case, the task will run at midnight every day in New York time.
// If you specify a timezone it will automatically work with daylight saving time.
timezone: "America/New_York",
//Optionally, you can specify your own IDs (like a user ID) and then use it inside the run function of your task.
//This allows you to have per-user cron tasks.
externalId: "user_123456",
//You can only create one schedule with this key.
//If you use it twice, the second call will update the schedule.
//This is useful because you don't want to create duplicate schedules for a user.
deduplicationKey: "user_123456-todo_reminder",
});
```
See [the SDK reference](/management/schedules/create) for full details.
### Dynamic schedules (or multi-tenant schedules)
By using the `externalId` you can have schedules for your users. This is useful for things like reminders, where you want to have a schedule for each user.
A reminder task:
```ts /trigger/reminder.ts
import { schedules } from "@trigger.dev/sdk/v3";
//this task will run when any of the attached schedules trigger
export const reminderTask = schedules.task({
id: "todo-reminder",
run: async (payload) => {
if (!payload.externalId) {
throw new Error("externalId is required");
}
//get user using the externalId you used when creating the schedule
const user = await db.getUser(payload.externalId);
//send a reminder email
await sendReminderEmail(user);
},
});
```
Then in your backend code, you can create a schedule for each user:
```ts Next.js API route
import { reminderTask } from "~/trigger/reminder";
//app/reminders/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
//create a schedule for the user
const createdSchedule = await schedules.create({
task: reminderTask.id,
//8am every day
cron: "0 8 * * *",
//the user's timezone
timezone: data.timezone,
//the user id
externalId: data.userId,
//this makes it impossible to have two reminder schedules for the same user
deduplicationKey: `${data.userId}-reminder`,
});
//return a success response with the schedule
return Response.json(createdSchedule);
}
```
You can also retrieve, list, delete, deactivate and re-activate schedules using the SDK. More on that later.
## Testing schedules
You can test a scheduled task in the dashboard. Note that the `scheduleId` will always come through as `sched_1234` to the run.
<Steps>
<Step title="Go to the Test page">
In the sidebar select the "Test" page, then select a scheduled task from the list (they have a
clock icon on them) ![Test page](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/schedules-test.png)
</Step>
<Step title="Create your schedule">
Fill in the form \[1]. You can select from a recent run \[2] to pre-populate the fields. Press "Run
test" when you're ready ![Schedule test form](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/schedules-test-form.png)
</Step>
</Steps>
## Managing schedules with the SDK
### Retrieving an existing schedule
```ts
const retrievedSchedule = await schedules.retrieve(scheduleId);
```
See [the SDK reference](/management/schedules/retrieve) for full details.
### Listing schedules
```ts
const allSchedules = await schedules.list();
```
See [the SDK reference](/management/schedules/list) for full details.
### Updating a schedule
```ts
const updatedSchedule = await schedules.update(scheduleId, {
task: firstScheduledTask.id,
cron: "0 0 1 * *",
externalId: "ext_1234444",
deduplicationKey: "my-deduplication-key",
});
```
See [the SDK reference](/management/schedules/update) for full details.
### Deactivating a schedule
```ts
const deactivatedSchedule = await schedules.deactivate(scheduleId);
```
See [the SDK reference](/management/schedules/deactivate) for full details.
### Activating a schedule
```ts
const activatedSchedule = await schedules.activate(scheduleId);
```
See [the SDK reference](/management/schedules/activate) for full details.
### Deleting a schedule
```ts
const deletedSchedule = await schedules.del(scheduleId);
```
See [the SDK reference](/management/schedules/delete) for full details.
### Getting possible timezones
You might want to show a dropdown menu in your UI so your users can select their timezone. You can get a list of all possible timezones using the SDK:
```ts
const timezones = await schedules.timezones();
```
See [the SDK reference](/management/schedules/timezones) for full details.
# schemaTask
Source: https://trigger.dev/docs/tasks/schemaTask
Define tasks with a runtime payload schema and validate the payload before running the task.
The `schemaTask` function allows you to define a task with a runtime payload schema. This schema is used to validate the payload before running the task or when triggering a task directly. If the payload does not match the schema, the task will not execute.
## Usage
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { z } from "zod";
const myTask = schemaTask({
id: "my-task",
schema: z.object({
name: z.string(),
age: z.number(),
}),
run: async (payload) => {
console.log(payload.name, payload.age);
},
});
```
`schemaTask` takes all the same options as [task](/tasks/overview), with the addition of a `schema` field. The `schema` field is a schema parser function from a schema library or or a custom parser function.
<Note>
We will probably eventually combine `task` and `schemaTask` into a single function, but because
that would be a breaking change, we are keeping them separate for now.
</Note>
When you trigger the task directly, the payload will be validated against the schema before the [run](/runs) is created:
```ts
import { tasks } from "@trigger.dev/sdk/v3";
import { myTask } from "./trigger/myTasks";
// This will call the schema parser function and validate the payload
await myTask.trigger({ name: "Alice", age: "oops" }); // this will throw an error
// This will NOT call the schema parser function
await tasks.trigger<typeof myTask>("my-task", { name: "Alice", age: "oops" }); // this will not throw an error
```
The error thrown when the payload does not match the schema will be the same as the error thrown by the schema parser function. For example, if you are using Zod, the error will be a `ZodError`.
We will also validate the payload every time before the task is run, so you can be sure that the payload is always valid. In the example above, the task would fail with a `TaskPayloadParsedError` error and skip retrying if the payload does not match the schema.
## Input/output schemas
Certain schema libraries, like Zod, split their type inference into "schema in" and "schema out". This means that you can define a single schema that will produce different types when triggering the task and when running the task. For example, you can define a schema that has a default value for a field, or a string coerced into a date:
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { z } from "zod";
const myTask = schemaTask({
id: "my-task",
schema: z.object({
name: z.string().default("John"),
age: z.number(),
dob: z.coerce.date(),
}),
run: async (payload) => {
console.log(payload.name, payload.age);
},
});
```
In this case, the trigger payload type is `{ name?: string, age: number; dob: string }`, but the run payload type is `{ name: string, age: number; dob: Date }`. So you can trigger the task with a payload like this:
```ts
await myTask.trigger({ age: 30, dob: "2020-01-01" }); // this is valid
await myTask.trigger({ name: "Alice", age: 30, dob: "2020-01-01" }); // this is also valid
```
## Supported schema types
### Zod
You can use the [Zod](https://zod.dev) schema library to define your schema. The schema will be validated using Zod's `parse` function.
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { z } from "zod";
export const zodTask = schemaTask({
id: "types/zod",
schema: z.object({
bar: z.string(),
baz: z.string().default("foo"),
}),
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### Yup
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import * as yup from "yup";
export const yupTask = schemaTask({
id: "types/yup",
schema: yup.object({
bar: yup.string().required(),
baz: yup.string().default("foo"),
}),
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### Superstruct
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { object, string } from "superstruct";
export const superstructTask = schemaTask({
id: "types/superstruct",
schema: object({
bar: string(),
baz: string(),
}),
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### ArkType
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { type } from "arktype";
export const arktypeTask = schemaTask({
id: "types/arktype",
schema: type({
bar: "string",
baz: "string",
}).assert,
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### @effect/schema
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import * as Schema from "@effect/schema/Schema";
// For some funny typescript reason, you cannot pass the Schema.decodeUnknownSync directly to schemaTask
const effectSchemaParser = Schema.decodeUnknownSync(
Schema.Struct({ bar: Schema.String, baz: Schema.String })
);
export const effectTask = schemaTask({
id: "types/effect",
schema: effectSchemaParser,
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### runtypes
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import * as T from "runtypes";
export const runtypesTask = schemaTask({
id: "types/runtypes",
schema: T.Record({
bar: T.String,
baz: T.String,
}),
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### valibot
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import * as v from "valibot";
// For some funny typescript reason, you cannot pass the v.parser directly to schemaTask
const valibotParser = v.parser(
v.object({
bar: v.string(),
baz: v.string(),
})
);
export const valibotTask = schemaTask({
id: "types/valibot",
schema: valibotParser,
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### typebox
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
import { Type } from "@sinclair/typebox";
import { wrap } from "@typeschema/typebox";
export const typeboxTask = schemaTask({
id: "types/typebox",
schema: wrap(
Type.Object({
bar: Type.String(),
baz: Type.String(),
})
),
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
### Custom parser function
You can also define a custom parser function that will be called with the payload before the task is run. The parser function should return the parsed payload or throw an error if the payload is invalid.
```ts
import { schemaTask } from "@trigger.dev/sdk/v3";
export const customParserTask = schemaTask({
id: "types/custom-parser",
schema: (data: unknown) => {
// This is a custom parser, and should do actual parsing (not just casting)
if (typeof data !== "object") {
throw new Error("Invalid data");
}
const { bar, baz } = data as { bar: string; baz: string };
return { bar, baz };
},
run: async (payload) => {
console.log(payload.bar, payload.baz);
},
});
```
# Triggering
Source: https://trigger.dev/docs/triggering
Tasks need to be triggered in order to run.
## Trigger functions
Trigger tasks **from your backend**:
| Function                 | What it does                                                                                     |                               |
| :----------------------- | :----------------------------------------------------------------------------------------------- | ----------------------------- |
| `tasks.trigger()`        | Triggers a task and returns a handle you can use to fetch and manage the run.                    | [Docs](#tasks-trigger)        |
| `tasks.batchTrigger()`   | Triggers a single task in a batch and returns a handle you can use to fetch and manage the runs. | [Docs](#tasks-batchtrigger)   |
| `tasks.triggerAndPoll()` | Triggers a task and then polls the run until itâ€™s complete.                                      | [Docs](#tasks-triggerandpoll) |
| `batch.trigger()`        | Similar to `tasks.batchTrigger` but allows running multiple different tasks                      | [Docs](#batch-trigger)        |
Trigger tasks **from inside a another task**:
| Function                         | What it does                                                                                                                       |                                       |
| :------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
| `yourTask.trigger()`             | Triggers a task and gets a handle you can use to monitor and manage the run. It does not wait for the result.                      | [Docs](#yourtask-trigger)             |
| `yourTask.batchTrigger()`        | Triggers a task multiple times and gets a handle you can use to monitor and manage the runs. It does not wait for the results.     | [Docs](#yourtask-batchtrigger)        |
| `yourTask.triggerAndWait()`      | Triggers a task and then waits until it's complete. You get the result data to continue with.                                      | [Docs](#yourtask-triggerandwait)      |
| `yourTask.batchTriggerAndWait()` | Triggers a task multiple times in parallel and then waits until they're all complete. You get the resulting data to continue with. | [Docs](#yourtask-batchtriggerandwait) |
| `batch.triggerAndWait()`         | Similar to `batch.trigger` but will wait on the triggered tasks to finish and return the results.                                  | [Docs](#batch-triggerandwait)         |
| `batch.triggerByTask()`          | Similar to `batch.trigger` but allows passing in task instances instead of task IDs.                                               | [Docs](#batch-triggerbytask)          |
| `batch.triggerByTaskAndWait()`   | Similar to `batch.triggerbyTask` but will wait on the triggered tasks to finish and return the results.                            | [Docs](#batch-triggerbytaskandwait)   |
## Triggering from your backend
When you trigger a task from your backend code, you need to set the `TRIGGER_SECRET_KEY` environment variable. You can find the value on the API keys page in the Trigger.dev dashboard. [More info on API keys](/apikeys).
<Note>
If you are using Next.js Server Actions [you'll need to be careful with
bundling](/guides/frameworks/nextjs#triggering-your-task-in-next-js).
</Note>
### tasks.trigger()
Triggers a single run of a task with the payload you pass in, and any options you specify, without needing to import the task.
<Note>
By using `tasks.trigger()`, you can pass in the task type as a generic argument, giving you full
type checking. Make sure you use a `type` import so that your task code is not imported into your
application.
</Note>
```ts Your backend
import { tasks } from "@trigger.dev/sdk/v3";
import type { emailSequence } from "~/trigger/emails";
//     ðŸ‘† **type-only** import
//app/email/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass the task type to `trigger()` as a generic argument, giving you full type checking
const handle = await tasks.trigger<typeof emailSequence>("email-sequence", {
to: data.email,
name: data.name,
});
//return a success response with the handle
return Response.json(handle);
}
```
You can pass in options to the task using the second argument:
```ts Your backend
import { tasks } from "@trigger.dev/sdk/v3";
import type { emailSequence } from "~/trigger/emails";
//app/email/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass the task type to `trigger()` as a generic argument, giving you full type checking
const handle = await tasks.trigger<typeof emailSequence>(
"email-sequence",
{
to: data.email,
name: data.name,
},
{ delay: "1h" } // ðŸ‘ˆ Pass in the options here
);
//return a success response with the handle
return Response.json(handle);
}
```
### tasks.batchTrigger()
Triggers multiple runs of a single task with the payloads you pass in, and any options you specify, without needing to import the task.
```ts Your backend
import { tasks } from "@trigger.dev/sdk/v3";
import type { emailSequence } from "~/trigger/emails";
//     ðŸ‘† **type-only** import
//app/email/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass the task type to `batchTrigger()` as a generic argument, giving you full type checking
const batchHandle = await tasks.batchTrigger<typeof emailSequence>(
"email-sequence",
data.users.map((u) => ({ payload: { to: u.email, name: u.name } }))
);
//return a success response with the handle
return Response.json(batchHandle);
}
```
You can pass in options to the `batchTrigger` function using the second argument:
```ts Your backend
import { tasks } from "@trigger.dev/sdk/v3";
import type { emailSequence } from "~/trigger/emails";
//app/email/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass the task type to `batchTrigger()` as a generic argument, giving you full type checking
const batchHandle = await tasks.batchTrigger<typeof emailSequence>(
"email-sequence",
data.users.map((u) => ({ payload: { to: u.email, name: u.name } })),
{ idempotencyKey: "my-idempotency-key" } // ðŸ‘ˆ Pass in the options here
);
//return a success response with the handle
return Response.json(batchHandle);
}
```
You can also pass in options for each run in the batch:
```ts Your backend
import { tasks } from "@trigger.dev/sdk/v3";
import type { emailSequence } from "~/trigger/emails";
//app/email/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass the task type to `batchTrigger()` as a generic argument, giving you full type checking
const batchHandle = await tasks.batchTrigger<typeof emailSequence>(
"email-sequence",
data.users.map((u) => ({ payload: { to: u.email, name: u.name }, options: { delay: "1h" } })) // ðŸ‘ˆ Pass in options to each item like so
);
//return a success response with the handle
return Response.json(batchHandle);
}
```
### tasks.triggerAndPoll()
Triggers a single run of a task with the payload you pass in, and any options you specify, and then polls the run until it's complete.
<Warning>
We don't recommend using `triggerAndPoll()`, especially inside a web request, as it will block the
request until the run is complete. Please see our [Realtime docs](/realtime) for a better way to
handle this.
</Warning>
```ts Your backend
import { tasks } from "@trigger.dev/sdk/v3";
import type { emailSequence } from "~/trigger/emails";
//app/email/route.ts
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass the task type to `triggerAndPoll()` as a generic argument, giving you full type checking
const result = await tasks.triggerAndPoll<typeof emailSequence>(
"email-sequence",
{
to: data.email,
name: data.name,
},
{ pollIntervalMs: 5000 }
);
//return a success response with the result
return Response.json(result);
}
```
### batch.trigger()
Triggers multiple runs of different tasks with the payloads you pass in, and any options you specify. This is useful when you need to trigger multiple tasks at once.
```ts Your backend
import { batch } from "@trigger.dev/sdk/v3";
import type { myTask1, myTask2 } from "~/trigger/myTasks";
export async function POST(request: Request) {
//get the JSON from the request
const data = await request.json();
// Pass a union of the tasks to `trigger()` as a generic argument, giving you full type checking
const result = await batch.trigger<typeof myTask1 | typeof myTask2>([
// Because we're using a union, we can pass in multiple tasks by ID
{ id: "my-task-1", payload: { some: data.some } },
{ id: "my-task-2", payload: { other: data.other } },
]);
//return a success response with the result
return Response.json(result);
}
```
## Triggering from inside another task
The following functions should only be used when running inside a task, for one of the following reasons:
* You need to **wait** for the result of the triggered task.
* You need to import the task instance. Importing a task instance from your backend code is not recommended, as it can pull in a lot of unnecessary code and dependencies.
### yourTask.trigger()
Triggers a single run of a task with the payload you pass in, and any options you specify.
<Note>
If you need to call `trigger()` on a task in a loop, use
[`batchTrigger()`](#yourTask-batchtrigger) instead which will trigger up to 500 runs in a single
call.
</Note>
```ts ./trigger/my-task.ts
import { myOtherTask, runs } from "~/trigger/my-other-task";
export const myTask = task({
id: "my-task",
run: async (payload: string) => {
const handle = await myOtherTask.trigger({ foo: "some data" });
const run = await runs.retrieve(handle);
// Do something with the run
},
});
```
To pass options to the triggered task, you can use the second argument:
```ts ./trigger/my-task.ts
import { myOtherTask, runs } from "~/trigger/my-other-task";
export const myTask = task({
id: "my-task",
run: async (payload: string) => {
const handle = await myOtherTask.trigger({ foo: "some data" }, { delay: "1h" });
const run = await runs.retrieve(handle);
// Do something with the run
},
});
```
### yourTask.batchTrigger()
Triggers multiple runs of a single task with the payloads you pass in, and any options you specify.
```ts /trigger/my-task.ts
import { myOtherTask, batch } from "~/trigger/my-other-task";
export const myTask = task({
id: "my-task",
run: async (payload: string) => {
const batchHandle = await myOtherTask.batchTrigger([{ payload: "some data" }]);
//...do other stuff
const batch = await batch.retrieve(batchHandle.id);
},
});
```
If you need to pass options to `batchTrigger`, you can use the second argument:
```ts /trigger/my-task.ts
import { myOtherTask, batch } from "~/trigger/my-other-task";
export const myTask = task({
id: "my-task",
run: async (payload: string) => {
const batchHandle = await myOtherTask.batchTrigger([{ payload: "some data" }], {
idempotencyKey: "my-task-key",
});
//...do other stuff
const batch = await batch.retrieve(batchHandle.id);
},
});
```
You can also pass in options for each run in the batch:
```ts /trigger/my-task.ts
import { myOtherTask, batch } from "~/trigger/my-other-task";
export const myTask = task({
id: "my-task",
run: async (payload: string) => {
const batchHandle = await myOtherTask.batchTrigger([
{ payload: "some data", options: { delay: "1h" } },
]);
//...do other stuff
const batch = await batch.retrieve(batchHandle.id);
},
});
```
### yourTask.triggerAndWait()
This is where it gets interesting. You can trigger a task and then wait for the result. This is useful when you need to call a different task and then use the result to continue with your task.
<Accordion title="Don't use this in parallel, e.g. with `Promise.all()`">
Instead, use `batchTriggerAndWait()` if you can, or a for loop if you can't.
To control concurrency using batch triggers, you can set `queue.concurrencyLimit` on the child task.
<CodeGroup>
```ts /trigger/batch.ts
export const batchTask = task({
id: "batch-task",
run: async (payload: string) => {
const results = await childTask.batchTriggerAndWait([
{ payload: "item1" },
{ payload: "item2" },
]);
console.log("Results", results);
//...do stuff with the results
},
});
```
```ts /trigger/loop.ts
export const loopTask = task({
id: "loop-task",
run: async (payload: string) => {
//this will be slower than the batch version
//as we have to resume the parent after each iteration
for (let i = 0; i < 2; i++) {
const result = await childTask.triggerAndWait(`item${i}`);
console.log("Result", result);
//...do stuff with the result
}
},
});
```
</CodeGroup>
</Accordion>
```ts /trigger/parent.ts
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
const result = await childTask.triggerAndWait("some-data");
console.log("Result", result);
//...do stuff with the result
},
});
```
The `result` object is a "Result" type that needs to be checked to see if the child task run was successful:
```ts /trigger/parent.ts
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
const result = await childTask.triggerAndWait("some-data");
if (result.ok) {
console.log("Result", result.output); // result.output is the typed return value of the child task
} else {
console.error("Error", result.error); // result.error is the error that caused the run to fail
}
},
});
```
If instead you just want to get the output of the child task, and throw an error if the child task failed, you can use the `unwrap` method:
```ts /trigger/parent.ts
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
const output = await childTask.triggerAndWait("some-data").unwrap();
console.log("Output", output);
},
});
```
You can also catch the error if the child task fails and get more information about the error:
```ts /trigger/parent.ts
import { task, SubtaskUnwrapError } from "@trigger.dev/sdk/v3";
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
try {
const output = await childTask.triggerAndWait("some-data").unwrap();
console.log("Output", output);
} catch (error) {
if (error instanceof SubtaskUnwrapError) {
console.error("Error in fetch-post-task", {
runId: error.runId,
taskId: error.taskId,
cause: error.cause,
});
}
}
},
});
```
<Warning>
This method should only be used inside a task. If you use it outside a task, it will throw an
error.
</Warning>
### yourTask.batchTriggerAndWait()
You can batch trigger a task and wait for all the results. This is useful for the fan-out pattern, where you need to call a task multiple times and then wait for all the results to continue with your task.
<Accordion title="Don't use this in parallel, e.g. with `Promise.all()`">
Instead, pass in all items at once and set an appropriate `maxConcurrency`. Alternatively, use sequentially with a for loop.
To control concurrency, you can set `queue.concurrencyLimit` on the child task.
<CodeGroup>
```ts /trigger/batch.ts
export const batchTask = task({
id: "batch-task",
run: async (payload: string) => {
const results = await childTask.batchTriggerAndWait([
{ payload: "item1" },
{ payload: "item2" },
]);
console.log("Results", results);
//...do stuff with the results
},
});
```
```ts /trigger/loop.ts
export const loopTask = task({
id: "loop-task",
run: async (payload: string) => {
//this will be slower than a single batchTriggerAndWait()
//as we have to resume the parent after each iteration
for (let i = 0; i < 2; i++) {
const result = await childTask.batchTriggerAndWait([
{ payload: `itemA${i}` },
{ payload: `itemB${i}` },
]);
console.log("Result", result);
//...do stuff with the result
}
},
});
```
</CodeGroup>
</Accordion>
<Accordion title="How to handle run failures">
When using `batchTriggerAndWait`, you have full control over how to handle failures within the batch. The method returns an array of run results, allowing you to inspect each run's outcome individually and implement custom error handling.
Here's how you can manage run failures:
1. **Inspect individual run results**: Each run in the returned array has an `ok` property indicating success or failure.
2. **Access error information**: For failed runs, you can examine the `error` property to get details about the failure.
3. **Choose your failure strategy**: You have two main options:
* **Fail the entire batch**: Throw an error if any run fails, causing the parent task to reattempt.
* **Continue despite failures**: Process the results without throwing an error, allowing the parent task to continue.
4. **Implement custom logic**: You can create sophisticated handling based on the number of failures, types of errors, or other criteria.
Here's an example of how you might handle run failures:
<CodeGroup>
```ts /trigger/batchTriggerAndWait.ts
const result = await batchChildTask.batchTriggerAndWait([
{ payload: "item1" },
{ payload: "item2" },
{ payload: "item3" },
]);
// Result will contain the finished runs.
// They're only finished if they have succeeded or failed.
// "Failed" means all attempts failed
for (const run of result.runs) {
// Check if the run succeeded
if (run.ok) {
logger.info("Batch task run succeeded", { output: run.output });
} else {
logger.error("Batch task run error", { error: run.error });
//You can choose if you want to throw an error and fail the entire run
throw new Error(`Fail the entire run because ${run.id} failed`);
}
}
```
</CodeGroup>
</Accordion>
```ts /trigger/nested.ts
export const batchParentTask = task({
id: "parent-task",
run: async (payload: string) => {
const results = await childTask.batchTriggerAndWait([
{ payload: "item4" },
{ payload: "item5" },
{ payload: "item6" },
]);
console.log("Results", results);
//...do stuff with the result
},
});
```
<Warning>
This method should only be used inside a task. If you use it outside a task, it will throw an
error.
</Warning>
### batch.triggerAndWait()
You can batch trigger multiple different tasks and wait for all the results:
```ts /trigger/batch.ts
import { batch, task } from "@trigger.dev/sdk/v3";
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
//                                         ðŸ‘‡ Pass a union of all the tasks you want to trigger
const results = await batch.triggerAndWait<typeof childTask1 | typeof childTask2>([
{ id: "child-task-1", payload: { foo: "World" } }, // ðŸ‘ˆ The payload is typed correctly based on the task `id`
{ id: "child-task-2", payload: { bar: 42 } }, // ðŸ‘ˆ The payload is typed correctly based on the task `id`
]);
for (const result of results) {
if (result.ok) {
// ðŸ‘‡ Narrow the type of the result based on the taskIdentifier
switch (result.taskIdentifier) {
case "child-task-1":
console.log("Child task 1 output", result.output); // ðŸ‘ˆ result.output is typed as a string
break;
case "child-task-2":
console.log("Child task 2 output", result.output); // ðŸ‘ˆ result.output is typed as a number
break;
}
} else {
console.error("Error", result.error); // ðŸ‘ˆ result.error is the error that caused the run to fail
}
}
},
});
export const childTask1 = task({
id: "child-task-1",
run: async (payload: { foo: string }) => {
return `Hello ${payload}`;
},
});
export const childTask2 = task({
id: "child-task-2",
run: async (payload: { bar: number }) => {
return bar + 1;
},
});
```
### batch.triggerByTask()
You can batch trigger multiple different tasks by passing in the task instances. This function is especially useful when you have a static set of tasks you want to trigger:
```ts /trigger/batch.ts
import { batch, task, runs } from "@trigger.dev/sdk/v3";
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
const results = await batch.triggerByTask([
{ task: childTask1, payload: { foo: "World" } }, // ðŸ‘ˆ The payload is typed correctly based on the task instance
{ task: childTask2, payload: { bar: 42 } }, // ðŸ‘ˆ The payload is typed correctly based on the task instance
]);
// ðŸ‘‡ results.runs is a tuple, allowing you to get type safety without needing to narrow
const run1 = await runs.retrieve(results.runs[0]); // ðŸ‘ˆ run1 is typed as the output of childTask1
const run2 = await runs.retrieve(results.runs[1]); // ðŸ‘ˆ run2 is typed as the output of childTask2
},
});
export const childTask1 = task({
id: "child-task-1",
run: async (payload: { foo: string }) => {
return `Hello ${payload}`;
},
});
export const childTask2 = task({
id: "child-task-2",
run: async (payload: { bar: number }) => {
return bar + 1;
},
});
```
### batch.triggerByTaskAndWait()
You can batch trigger multiple different tasks by passing in the task instances, and wait for all the results. This function is especially useful when you have a static set of tasks you want to trigger:
```ts /trigger/batch.ts
import { batch, task, runs } from "@trigger.dev/sdk/v3";
export const parentTask = task({
id: "parent-task",
run: async (payload: string) => {
const { runs } = await batch.triggerByTaskAndWait([
{ task: childTask1, payload: { foo: "World" } }, // ðŸ‘ˆ The payload is typed correctly based on the task instance
{ task: childTask2, payload: { bar: 42 } }, // ðŸ‘ˆ The payload is typed correctly based on the task instance
]);
if (runs[0].ok) {
console.log("Child task 1 output", runs[0].output); // ðŸ‘ˆ runs[0].output is typed as the output of childTask1
}
if (runs[1].ok) {
console.log("Child task 2 output", runs[1].output); // ðŸ‘ˆ runs[1].output is typed as the output of childTask2
}
// ðŸ’­ A nice alternative syntax is to destructure the runs array:
const {
runs: [run1, run2],
} = await batch.triggerByTaskAndWait([
{ task: childTask1, payload: { foo: "World" } }, // ðŸ‘ˆ The payload is typed correctly based on the task instance
{ task: childTask2, payload: { bar: 42 } }, // ðŸ‘ˆ The payload is typed correctly based on the task instance
]);
if (run1.ok) {
console.log("Child task 1 output", run1.output); // ðŸ‘ˆ run1.output is typed as the output of childTask1
}
if (run2.ok) {
console.log("Child task 2 output", run2.output); // ðŸ‘ˆ run2.output is typed as the output of childTask2
}
},
});
export const childTask1 = task({
id: "child-task-1",
run: async (payload: { foo: string }) => {
return `Hello ${payload}`;
},
});
export const childTask2 = task({
id: "child-task-2",
run: async (payload: { bar: number }) => {
return bar + 1;
},
});
```
## Triggering from your frontend
If you want to trigger a task directly from a frontend application, you can use our [React
hooks](/frontend/react-hooks/triggering).
## Options
All of the above functions accept an options object:
```ts
await myTask.trigger({ some: "data" }, { delay: "1h", ttl: "1h" });
await myTask.batchTrigger([{ payload: { some: "data" }, options: { delay: "1h" } }]);
```
The following options are available:
### `delay`
When you want to trigger a task now, but have it run at a later time, you can use the `delay` option:
```ts
// Delay the task run by 1 hour
await myTask.trigger({ some: "data" }, { delay: "1h" });
// Delay the task run by 88 seconds
await myTask.trigger({ some: "data" }, { delay: "88s" });
// Delay the task run by 1 hour and 52 minutes and 18 seconds
await myTask.trigger({ some: "data" }, { delay: "1h52m18s" });
// Delay until a specific time
await myTask.trigger({ some: "data" }, { delay: "2024-12-01T00:00:00" });
// Delay using a Date object
await myTask.trigger({ some: "data" }, { delay: new Date(Date.now() + 1000 * 60 * 60) });
// Delay using a timezone
await myTask.trigger({ some: "data" }, { delay: new Date("2024-07-23T11:50:00+02:00") });
```
Runs that are delayed and have not been enqueued yet will display in the dashboard with a "Delayed" status:
![Delayed run in the dashboard](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/delayed-runs.png)
<Note>
Delayed runs will be enqueued at the time specified, and will run as soon as possible after that
time, just as a normally triggered run would.
</Note>
You can cancel a delayed run using the `runs.cancel` SDK function:
```ts
import { runs } from "@trigger.dev/sdk/v3";
await runs.cancel("run_1234");
```
You can also reschedule a delayed run using the `runs.reschedule` SDK function:
```ts
import { runs } from "@trigger.dev/sdk/v3";
// The delay option here takes the same format as the trigger delay option
await runs.reschedule("run_1234", { delay: "1h" });
```
The `delay` option is also available when using `batchTrigger`:
```ts
await myTask.batchTrigger([{ payload: { some: "data" }, options: { delay: "1h" } }]);
```
### `ttl`
You can set a TTL (time to live) when triggering a task, which will automatically expire the run if it hasn't started within the specified time. This is useful for ensuring that a run doesn't get stuck in the queue for too long.
<Note>
All runs in development have a default `ttl` of 10 minutes. You can disable this by setting the
`ttl` option.
</Note>
```ts
import { myTask } from "./trigger/myTasks";
// Expire the run if it hasn't started within 1 hour
await myTask.trigger({ some: "data" }, { ttl: "1h" });
// If you specify a number, it will be treated as seconds
await myTask.trigger({ some: "data" }, { ttl: 3600 }); // 1 hour
```
When a run is expired, it will be marked as "Expired" in the dashboard:
![Expired runs in the dashboard](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/expired-runs.png)
When you use both `delay` and `ttl`, the TTL will start counting down from the time the run is enqueued, not from the time the run is triggered.
So for example, when using the following code:
```ts
await myTask.trigger({ some: "data" }, { delay: "10m", ttl: "1h" });
```
The timeline would look like this:
1. The run is created at 12:00:00
2. The run is enqueued at 12:10:00
3. The TTL starts counting down from 12:10:00
4. If the run hasn't started by 13:10:00, it will be expired
For this reason, the `ttl` option only accepts durations and not absolute timestamps.
### `idempotencyKey`
You can provide an `idempotencyKey` to ensure that a task is only triggered once with the same key. This is useful if you are triggering a task within another task that might be retried:
```typescript
import { idempotencyKeys, task } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
retry: {
maxAttempts: 4,
},
run: async (payload: any) => {
// By default, idempotency keys generated are unique to the run, to prevent retries from duplicating child tasks
const idempotencyKey = await idempotencyKeys.create("my-task-key");
// childTask will only be triggered once with the same idempotency key
await childTask.trigger(payload, { idempotencyKey });
// Do something else, that may throw an error and cause the task to be retried
},
});
```
For more information, see our [Idempotency](/idempotency) documentation.
<Warning>
In version 3.3.0 and later, the `idempotencyKey` option is not available when using
`triggerAndWait` or `batchTriggerAndWait`, due to a bug that would sometimes cause the parent task
to become stuck. We are working on a fix for this issue.
</Warning>
### `idempotencyKeyTTL`
Idempotency keys automatically expire after 30 days, but you can set a custom TTL for an idempotency key when triggering a task:
```typescript
import { idempotencyKeys, task } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
retry: {
maxAttempts: 4,
},
run: async (payload: any) => {
// By default, idempotency keys generated are unique to the run, to prevent retries from duplicating child tasks
const idempotencyKey = await idempotencyKeys.create("my-task-key");
// childTask will only be triggered once with the same idempotency key
await childTask.trigger(payload, { idempotencyKey, idempotencyKeyTTL: "1h" });
// Do something else, that may throw an error and cause the task to be retried
},
});
```
For more information, see our [Idempotency](/idempotency) documentation.
### `queue`
When you trigger a task you can override the concurrency limit. This is really useful if you sometimes have high priority runs.
The task:
```ts /trigger/override-concurrency.ts
const generatePullRequest = task({
id: "generate-pull-request",
queue: {
//normally when triggering this task it will be limited to 1 run at a time
concurrencyLimit: 1,
},
run: async (payload) => {
//todo generate a PR using OpenAI
},
});
```
Triggering from your backend and overriding the concurrency:
```ts app/api/push/route.ts
import { generatePullRequest } from "~/trigger/override-concurrency";
export async function POST(request: Request) {
const data = await request.json();
if (data.branch === "main") {
//trigger the task, with a different queue
const handle = await generatePullRequest.trigger(data, {
queue: {
//the "main-branch" queue will have a concurrency limit of 10
//this triggered run will use that queue
name: "main-branch",
concurrencyLimit: 10,
},
});
return Response.json(handle);
} else {
//triggered with the default (concurrency of 1)
const handle = await generatePullRequest.trigger(data);
return Response.json(handle);
}
}
```
### `concurrencyKey`
If you're building an application where you want to run tasks for your users, you might want a separate queue for each of your users. (It doesn't have to be users, it can be any entity you want to separately limit the concurrency for.)
You can do this by using `concurrencyKey`. It creates a separate queue for each value of the key.
Your backend code:
```ts app/api/pr/route.ts
import { generatePullRequest } from "~/trigger/override-concurrency";
export async function POST(request: Request) {
const data = await request.json();
if (data.isFreeUser) {
//free users can only have 1 PR generated at a time
const handle = await generatePullRequest.trigger(data, {
queue: {
//every free user gets a queue with a concurrency limit of 1
name: "free-users",
concurrencyLimit: 1,
},
concurrencyKey: data.userId,
});
//return a success response with the handle
return Response.json(handle);
} else {
//trigger the task, with a different queue
const handle = await generatePullRequest.trigger(data, {
queue: {
//every paid user gets a queue with a concurrency limit of 10
name: "paid-users",
concurrencyLimit: 10,
},
concurrencyKey: data.userId,
});
//return a success response with the handle
return Response.json(handle);
}
}
```
### `maxAttempts`
You can set the maximum number of attempts for a task run. If the run fails, it will be retried up to the number of attempts you specify.
```ts
await myTask.trigger({ some: "data" }, { maxAttempts: 3 });
await myTask.trigger({ some: "data" }, { maxAttempts: 1 }); // no retries
```
This will override the `retry.maxAttempts` value set in the task definition.
### `tags`
View our [tags doc](/tags) for more information.
### `metadata`
View our [metadata doc](/runs/metadata) for more information.
### `maxDuration`
View our [maxDuration doc](/runs/max-duration) for more information.
## Large Payloads
We recommend keeping your task payloads as small as possible. We currently have a hard limit on task payloads above 10MB.
If your payload size is larger than 512KB, instead of saving the payload to the database, we will upload it to an S3-compatible object store and store the URL in the database.
When your task runs, we automatically download the payload from the object store and pass it to your task function. We also will return to you a `payloadPresignedUrl` from the `runs.retrieve` SDK function so you can download the payload if needed:
```ts
import { runs } from "@trigger.dev/sdk/v3";
const run = await runs.retrieve(handle);
if (run.payloadPresignedUrl) {
const response = await fetch(run.payloadPresignedUrl);
const payload = await response.json();
console.log("Payload", payload);
}
```
<Note>
We also use this same system for dealing with large task outputs, and subsequently will return a
corresponding `outputPresignedUrl`. Task outputs are limited to 100MB.
</Note>
If you need to pass larger payloads, you'll need to upload the payload to your own storage and pass a URL to the file in the payload instead. For example, uploading to S3 and then sending a presigned URL that expires in URL:
<CodeGroup>
```ts /yourServer.ts
import { myTask } from "./trigger/myTasks";
import { s3Client, getSignedUrl, PutObjectCommand, GetObjectCommand } from "./s3";
import { createReadStream } from "node:fs";
// Upload file to S3
await s3Client.send(
new PutObjectCommand({
Bucket: "my-bucket",
Key: "myfile.json",
Body: createReadStream("large-payload.json"),
})
);
// Create presigned URL
const presignedUrl = await getSignedUrl(
s3Client,
new GetObjectCommand({
Bucket: "my-bucket",
Key: "my-file.json",
}),
{
expiresIn: 3600, // expires in 1 hour
}
);
// Now send the URL to the task
const handle = await myTask.trigger({
url: presignedUrl,
});
```
```ts /trigger/myTasks.ts
import { task } from "@trigger.dev/sdk/v3";
export const myTask = task({
id: "my-task",
run: async (payload: { url: string }) => {
// Download the file from the URL
const response = await fetch(payload.url);
const data = await response.json();
// Do something with the data
},
});
```
</CodeGroup>
### Batch Triggering
When using triggering a batch, the total size of all payloads cannot exceed 1MB. This means if you are doing a batch of 100 runs, each payload should be less than 100KB. The max batch size is 500 runs.
# Common problems
Source: https://trigger.dev/docs/troubleshooting
Some common problems you might experience and their solutions
## Development
### `EACCES: permission denied`
If you see this error:
```
6090 verbose stack Error: EACCES: permission denied, rename '/Users/user/.npm/_cacache/tmp/f1bfea11' -> '/Users/user/.npm/_cacache/content-v2/sha512/31/d8/e094a47a0105d06fd246892ed1736c02eae323726ec6a3f34734eeb71308895dfba4f4f82a88ffe7e480c90b388c91fc3d9f851ba7b96db4dc33fbc65528'
```
First, clear the npm cache:
```sh
npm cache clean --force
```
Then change the permissions of the npm folder (if 1 doesn't work):
```sh
sudo chown -R $(whoami) ~/.npm
```
### Clear the build cache
Ensure you have stopped your local dev server then locate the hidden `.trigger` folder in your project and delete it. You can then restart your local dev server.
## Deployment
Running the \[trigger.dev deploy] command builds and deploys your code. Sometimes there can be issues building your code.
You can run the deploy command with `--log-level debug` at the end. This will spit out a lot of information about the deploy. If you can't figure out the problem from the information below please join [our Discord](https://trigger.dev/discord) and create a help forum post. Do NOT share the extended debug logs publicly as they might reveal private information about your project.
You can also review the build by supplying the `--dry-run` flag. This will build your project but not deploy it. You can then inspect the build output on your machine.
Here are some common problems and their solutions:
### `Failed to build project image: Error building image`
There should be a link below the error message to the full build logs on your machine. Take a look at these to see what went wrong. Join [our Discord](https://trigger.dev/discord) and you share it privately with us if you can't figure out what's going wrong. Do NOT share these publicly as the verbose logs might reveal private information about your project.
### `Deployment encountered an error`
Usually there will be some useful guidance below this message. If you can't figure out what's going wrong then join [our Discord](https://trigger.dev/discord) and create a Help forum post with a link to your deployment.
### `No loader is configured for ".node" files`
This happens because `.node` files are native code and can't be bundled like other packages. To fix this, add your package to [`build.external`](/config/config-file#external) in the `trigger.config.ts` file like this:
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
// Your other config settings...
build: {
external: ["your-node-package"],
},
});
```
### `Cannot find module '/app/lib/worker.js"` when using pino
If you see this error, add pino (and any other associated packages) to your `external` build settings in your `trigger.config.ts` file. Learn more about the `external` setting in the [config docs](/config/config-file#external).
### `Cannot find matching keyid`
This error occurs when using Node.js v22 with corepack, as it's not yet compatible with the latest package manager signatures. To fix this, either:
1. Downgrade to Node.js v20 (LTS), or
2. Install corepack globally: `npm i -g corepack@latest`
The corepack bug and workaround are detailed in [this issue](https://github.com/npm/cli/issues/8075).
## Project setup issues
### `The requested module 'node:events' does not provide an export named 'addAbortListener'`
If you see this error it means you're not a supported version of Node:
```
SyntaxError: The requested module 'node:events' does not provide an export named 'addAbortListener'
at ModuleJob._instantiate (node:internal/modules/esm/module_job:123:21)
at async ModuleJob.run (node:internal/modules/esm/module_job:189:5)
Node.js v19.9.0
```
You need to be on at least these minor versions:
| Version | Minimum |
| ------- | ------- |
| 18      | 18.20+  |
| 20      | 20.5+   |
| 21      | 21.0+   |
| 22      | 22.0+   |
## Runtime issues
### `Environment variable not found:`
Your code is deployed separately from the rest of your app(s) so you need to make sure that you set any environment variables you use in your tasks in the Trigger.dev dashboard. [Read the guide](/deploy-environment-variables).
### `Error: @prisma/client did not initialize yet.`
Prisma uses code generation to create the client from your schema file. This means you need to add a bit of config so we can generate this file before your tasks run: [Read the guide](/config/extensions/prismaExtension).
### `Parallel waits are not supported`
In the current version, you can't perform more that one "wait" in parallel.
Waits include:
* `wait.for()`
* `wait.until()`
* `task.triggerAndWait()`
* `task.batchTriggerAndWait()`
* And any of our functions with `wait` in the name.
This restriction exists because we suspend the task server after a wait, and resume it when the wait is done. At the moment, if you do more than one wait, the run will never continue when deployed, so we throw this error instead.
The most common situation this happens is if you're using `Promise.all` around some of our wait functions. Instead of doing this use our built-in functions for [triggering tasks](/triggering#triggering-from-inside-another-task). We have functions that allow you to trigger different tasks in parallel.
### When triggering subtasks the parent task finishes too soon
Make sure that you always use `await` when you call `trigger`, `triggerAndWait`, `batchTrigger`, and `batchTriggerAndWait`. If you don't then it's likely the task(s) won't be triggered because the calling function process can be terminated before the networks calls are sent.
### Rate limit exceeded
The most common cause of hitting the API rate limit is if youâ€™re calling `trigger()` on a task in a loop, instead of doing this use `batchTrigger()` which will trigger multiple tasks in a single API call. You can have up to 100 tasks in a single batch trigger call.
View the [rate limits](/limits) page for more information.
### `Crypto is not defined`
This can happen in different situations, for example when using plain strings as idempotency keys. Support for `Crypto` without a special flag was added in Node `v19.0.0`. You will have to upgrade Node - we recommend even-numbered major releases, e.g. `v20` or `v22`. Alternatively, you can switch from plain strings to the `idempotencyKeys.create` SDK function. [Read the guide](/idempotency).
## Framework specific issues
### NestJS swallows all errors/exceptions
If you're using NestJS and you add code like this into your tasks you will prevent any errors from being surfaced:
```ts
export const simplestTask = task({
id: "nestjs-example",
run: async (payload) => {
//by doing this you're swallowing any errors
const app = await NestFactory.createApplicationContext(AppModule);
await app.init();
//etc...
},
});
```
NestJS has a global exception filter that catches all errors and swallows them, so we can't receive them. Our current recommendation is to not use NestJS inside your tasks. If you're a NestJS user you can still use Trigger.dev but just don't use NestJS inside your tasks like this.
### React is not defined
If you see this error:
```
Worker failed to start ReferenceError: React is not defined
```
Either add this to your file:
```ts
import React from "react";
```
Or change the tsconfig jsx setting:
```json
{
"compilerOptions": {
//...
"jsx": "react-jsx"
}
}
```
### Next.js build failing due to missing API key in GitHub CI
This issue occurs during the Next.js app build process on GitHub CI where the Trigger.dev SDK is expecting the TRIGGER\_SECRET\_KEY environment variable to be set at build time. Next.js attempts to compile routes and creates static pages, which can cause issues with SDKs that require runtime environment variables. The solution is to mark the relevant pages as dynamic to prevent Next.js from trying to make them static. You can do this by adding the following line to the route file:
```ts
export const dynamic = "force-dynamic";
```
### Correctly passing event handlers to React components
An issue can sometimes arise when you try to pass a function directly to the `onClick` prop. This is because the function may require specific arguments or context that are not available when the event occurs. By wrapping the function call in an arrow function, you ensure that the handler is called with the correct context and any necessary arguments. For example:
This works:
```tsx
<Button onClick={() => myTask()}>Trigger my task</Button>
```
Whereas this does not work:
```tsx
<Button onClick={myTask}>Trigger my task</Button>
```
# Alerts
Source: https://trigger.dev/docs/troubleshooting-alerts
Get alerted when runs or deployments fail, or when deployments succeed.
We support receiving alerts for the following events:
* Run fails
* Deployment fails
* Deployment succeeds
## How to setup alerts
<Steps>
<Step title="Create a new alert">
Click on "Alerts" in the left hand side menu, then click on "New alert" to open the new alert modal.
![Email alerts](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/troubleshooting-alerts-blank.png)
</Step>
<Step title="Choose your alert method">
Choose to be notified by email, Slack notification or webhook whenever:
* a run fails
* a deployment fails
* a deployment succeeds
![Email alerts](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/troubleshooting-alerts-modal.png)
</Step>
<Step title="Delete or disable alerts">
Click on the triple dot menu on the right side of the table row and select "Disable" or "Delete".
![Disable and delete alerts](https://mintlify.s3.us-west-1.amazonaws.com/trigger/images/troubleshooting-alerts-disable-delete.png)
</Step>
</Steps>
## Alert webhooks
For the alert webhooks you can use the SDK to parse them. Here is an example of how to parse the webhook payload in Remix:
```ts
import { ActionFunctionArgs, json } from "@remix-run/server-runtime";
import { webhooks, WebhookError } from "@trigger.dev/sdk/v3";
export async function action({ request }: ActionFunctionArgs) {
// Make sure this is a POST request
if (request.method !== "POST") {
return json({ error: "Method not allowed" }, { status: 405 });
}
try {
// Construct and verify the webhook event
// This secret can be found on your Alerts page when you create a webhook alert
const event = await webhooks.constructEvent(request, process.env.ALERT_WEBHOOK_SECRET!);
// Process the event based on its type
switch (event.type) {
case "alert.run.failed": {
console.log("[Webhook Internal Test] Run failed alert webhook received", { event });
break;
}
case "alert.deployment.success": {
console.log("[Webhook Internal Test] Deployment success alert webhook received", { event });
break;
}
case "alert.deployment.failed": {
console.log("[Webhook Internal Test] Deployment failed alert webhook received", { event });
break;
}
default: {
console.log("[Webhook Internal Test] Unhandled webhook type", { event });
}
}
// Return a success response
return json({ received: true }, { status: 200 });
} catch (err) {
// Handle webhook errors
if (err instanceof WebhookError) {
console.error("Webhook error:", { message: err.message });
return json({ error: err.message }, { status: 400 });
}
if (err instanceof Error) {
console.error("Error processing webhook:", { message: err.message });
return json({ error: err.message }, { status: 400 });
}
// Handle other errors
console.error("Error processing webhook:", { err });
return json({ error: "Internal server error" }, { status: 500 });
}
}
```
### Common properties
When you create a webhook alert, you'll receive different payloads depending on the type of alert. All webhooks share some common properties:
<ParamField path="id" type="string">
A unique identifier for this webhook event
</ParamField>
<ParamField path="created" type="datetime">
When this webhook event was created
</ParamField>
<ParamField path="webhookVersion" type="string">
The version of the webhook payload format
</ParamField>
<ParamField path="type" type="string">
The type of alert webhook. One of: `alert.run.failed`, `alert.deployment.success`, or `alert.deployment.failed`
</ParamField>
### Run Failed Alert
This webhook is sent when a run fails. The payload is available on the `object` property:
<ParamField path="object.task.id" type="string">
Unique identifier for the task
</ParamField>
<ParamField path="object.task.filePath" type="string">
File path where the task is defined
</ParamField>
<ParamField path="object.task.exportName" type="string">
Name of the exported task function
</ParamField>
<ParamField path="object.task.version" type="string">
Version of the task
</ParamField>
<ParamField path="object.task.sdkVersion" type="string">
Version of the SDK used
</ParamField>
<ParamField path="object.task.cliVersion" type="string">
Version of the CLI used
</ParamField>
<ParamField path="object.run.id" type="string">
Unique identifier for the run
</ParamField>
<ParamField path="object.run.number" type="number">
Run number
</ParamField>
<ParamField path="object.run.status" type="string">
Current status of the run
</ParamField>
<ParamField path="object.run.createdAt" type="datetime">
When the run was created
</ParamField>
<ParamField path="object.run.startedAt" type="datetime">
When the run started executing
</ParamField>
<ParamField path="object.run.completedAt" type="datetime">
When the run finished executing
</ParamField>
<ParamField path="object.run.isTest" type="boolean">
Whether this is a test run
</ParamField>
<ParamField path="object.run.idempotencyKey" type="string">
Idempotency key for the run
</ParamField>
<ParamField path="object.run.tags" type="string[]">
Associated tags
</ParamField>
<ParamField path="object.run.error" type="object">
Error information
</ParamField>
<ParamField path="object.run.isOutOfMemoryError" type="boolean">
Whether the run was an out-of-memory error
</ParamField>
<ParamField path="object.run.machine" type="string">
Machine preset used for the run
</ParamField>
<ParamField path="object.run.dashboardUrl" type="string">
URL to view the run in the dashboard
</ParamField>
<ParamField path="object.environment.id" type="string">
Environment ID
</ParamField>
<ParamField path="object.environment.type" type="string">
Environment type (STAGING or PRODUCTION)
</ParamField>
<ParamField path="object.environment.slug" type="string">
Environment slug
</ParamField>
<ParamField path="object.organization.id" type="string">
Organization ID
</ParamField>
<ParamField path="object.organization.slug" type="string">
Organization slug
</ParamField>
<ParamField path="object.organization.name" type="string">
Organization name
</ParamField>
<ParamField path="object.project.id" type="string">
Project ID
</ParamField>
<ParamField path="object.project.ref" type="string">
Project reference
</ParamField>
<ParamField path="object.project.slug" type="string">
Project slug
</ParamField>
<ParamField path="object.project.name" type="string">
Project name
</ParamField>
### Deployment Success Alert
This webhook is sent when a deployment succeeds. The payload is available on the `object` property:
<ParamField path="object.deployment.id" type="string">
Deployment ID
</ParamField>
<ParamField path="object.deployment.status" type="string">
Deployment status
</ParamField>
<ParamField path="object.deployment.version" type="string">
Deployment version
</ParamField>
<ParamField path="object.deployment.shortCode" type="string">
Short code identifier
</ParamField>
<ParamField path="object.deployment.deployedAt" type="datetime">
When the deployment completed
</ParamField>
<ParamField path="object.tasks" type="array">
Array of deployed tasks with properties: id, filePath, exportName, and triggerSource
</ParamField>
<ParamField path="object.environment.id" type="string">
Environment ID
</ParamField>
<ParamField path="object.environment.type" type="string">
Environment type (STAGING or PRODUCTION)
</ParamField>
<ParamField path="object.environment.slug" type="string">
Environment slug
</ParamField>
<ParamField path="object.organization.id" type="string">
Organization ID
</ParamField>
<ParamField path="object.organization.slug" type="string">
Organization slug
</ParamField>
<ParamField path="object.organization.name" type="string">
Organization name
</ParamField>
<ParamField path="object.project.id" type="string">
Project ID
</ParamField>
<ParamField path="object.project.ref" type="string">
Project reference
</ParamField>
<ParamField path="object.project.slug" type="string">
Project slug
</ParamField>
<ParamField path="object.project.name" type="string">
Project name
</ParamField>
### Deployment Failed Alert
This webhook is sent when a deployment fails. The payload is available on the `object` property:
<ParamField path="object.deployment.id" type="string">
Deployment ID
</ParamField>
<ParamField path="object.deployment.status" type="string">
Deployment status
</ParamField>
<ParamField path="object.deployment.version" type="string">
Deployment version
</ParamField>
<ParamField path="object.deployment.shortCode" type="string">
Short code identifier
</ParamField>
<ParamField path="object.deployment.failedAt" type="datetime">
When the deployment failed
</ParamField>
<ParamField path="object.error.name" type="string">
Error name
</ParamField>
<ParamField path="object.error.message" type="string">
Error message
</ParamField>
<ParamField path="object.error.stack" type="string">
Error stack trace (optional)
</ParamField>
<ParamField path="object.error.stderr" type="string">
Standard error output (optional)
</ParamField>
<ParamField path="object.environment.id" type="string">
Environment ID
</ParamField>
<ParamField path="object.environment.type" type="string">
Environment type (STAGING or PRODUCTION)
</ParamField>
<ParamField path="object.environment.slug" type="string">
Environment slug
</ParamField>
<ParamField path="object.organization.id" type="string">
Organization ID
</ParamField>
<ParamField path="object.organization.slug" type="string">
Organization slug
</ParamField>
<ParamField path="object.organization.name" type="string">
Organization name
</ParamField>
<ParamField path="object.project.id" type="string">
Project ID
</ParamField>
<ParamField path="object.project.ref" type="string">
Project reference
</ParamField>
<ParamField path="object.project.slug" type="string">
Project slug
</ParamField>
<ParamField path="object.project.name" type="string">
Project name
</ParamField>
# Debugging in VS Code
Source: https://trigger.dev/docs/troubleshooting-debugging-in-vscode
Debugging your task code in `dev` is supported via VS Code, without having to pass in any additional flags. Create a launch configuration in `.vscode/launch.json`:
```json launch.json
{
"version": "0.2.0",
"configurations": [
{
"name": "Trigger.dev: Dev",
"type": "node",
"request": "launch",
"cwd": "${workspaceFolder}",
"runtimeExecutable": "npx",
"runtimeArgs": ["trigger.dev@latest", "dev"],
"skipFiles": ["<node_internals>/**"],
"sourceMaps": true
}
]
}
```
Then you can start debugging your tasks code by selecting the `Trigger.dev: Dev` configuration in the debug panel, and set breakpoints in your tasks code.
# GitHub Issues
Source: https://trigger.dev/docs/troubleshooting-github-issues
Please [join our community on Discord](https://github.com/triggerdotdev/trigger.dev/issues) to ask questions, share your projects, and get help from other developers.
# Uptime Status
Source: https://trigger.dev/docs/troubleshooting-uptime-status
Get email notifications when Trigger.dev creates, updates or resolves a platform incident.
[Subscribe](https://status.trigger.dev/)
# Upgrade to new build system
Source: https://trigger.dev/docs/upgrading-beta
How to update to 3.0.0 from the beta
The Trigger.dev packages are now at version `3.0.x` in the `latest` tag. This is our first official release of v3 under the latest tag, and we recommend anyone still using packages in the `beta` tag to upgrade to the latest version. This guide will help you upgrade your project to the latest version of Trigger.dev.
The major changes in this release are a new build system, which is more flexible and powerful than the previous build system. We've also made some changes to the `trigger.dev` CLI to improve the developer experience.
The main features of the new build sytem are:
* **Bundling by default**: All dependencies are bundled by default, so you no longer need to specify which dependencies to bundle. This solves a whole bunch of issues related to monorepos.
* **Build extensions**: A new way to extend the build process with custom logic. This is a more flexible and powerful way to extend the build process compared to the old system. (including custom esbuild plugin support)
* **Improved configuration**: We've migrated to using [c12](https://github.com/unjs/c12) to power our configuration system.
* **Improved error handling**: We now do a much better job of reporting of any errors that happen during the indexing process by loading your trigger task files dynamically.
* **Improved cold start times**: Previously, we would load all your trigger task files at once, which could lead to long cold start times. Now we load your trigger task files dynamically, which should improve cold start times.
## Update packages
To use the new build system, you have to update to use our latest packages. Update the `@trigger.dev/sdk` package in your package.json:
```json
"@trigger.dev/sdk": "^3.0.0",
```
You will also need to update your usage of the `trigger.dev` CLI to use the latest release. If you run the CLI via `npx` you can update to the latest release like so:
```sh
# old way
npx trigger.dev@3.0.0-beta.56 dev
# using the latest release
npx trigger.dev@latest dev
```
If you've added the `trigger.dev` CLI to your `devDependencies`, then you should update the version to point to the latest release:
```json
"trigger.dev": "^3.0.0",
```
Once you do that make sure you re-install your dependencies using `npm i` or the equivalent with your preferred package manager.
<Note>If you deploy using GitHub actions, make sure you update the version there too.</Note>
## Update your `trigger.config.ts`
The new build system does not effect your trigger task files at all, so those can remain unchanged. However, you may need to make changes to your `trigger.config.ts` file.
### `defineConfig`
You should now import the `defineConfig` function from `@trigger.dev/sdk/v3` and export the config as the default export:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
});
```
### Deprecated: `dependenciesToBundle`
The new build system will bundle all dependencies by default, so `dependenciesToBundle` no longer makes any sense and can be removed.
#### Externals
Now that all dependencies are bundled, there are some situations where bundling a dependency doesn't work, and needs to be made external (e.g. when a dependency includes a native module). You can now specify these dependencies as build externals in the `defineConfig` function:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
export default defineConfig({
project: "<project ref>",
build: {
external: ["native-module"],
},
});
```
`external` is an array of strings, where each string is the name of a dependency that should be made external. Glob expressions are also supported and use the [minimatch](https://github.com/isaacs/minimatch) matcher.
### additionalFiles
The `additionalFiles` option has been moved to our new build extension system.
To use build extensions, you'll need to add the `@trigger.dev/build` package to your `devDependencies`:
```sh
npm add @trigger.dev/build@latest -D
```
Now you can import the `additionalFiles` build extension and use it in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { additionalFiles } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
additionalFiles({ files: ["wrangler/wrangler.toml", "./assets/**", "./fonts/**"] }),
],
},
});
```
### additionalPackages
The `additionalPackages` option has been moved to our new build extension system.
To use build extensions, you'll need to add the `@trigger.dev/build` package to your `devDependencies`:
```sh
npm add @trigger.dev/build@latest -D
```
Now you can import the `additionalPackages` build extension and use it in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { additionalPackages } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [additionalPackages({ packages: ["wrangler"] })],
},
});
```
### resolveEnvVars
The `resolveEnvVars` export has been moved to our new build extension system.
To use build extensions, you'll need to add the `@trigger.dev/build` package to your `devDependencies`:
```sh
npm add @trigger.dev/build@latest -D
```
Now you can import the `syncEnvVars` build extension and use it in your `trigger.config.ts` file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { syncEnvVars } from "@trigger.dev/build/extensions/core";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
syncEnvVars(async (params) => {
return {
MY_ENV_VAR: "my-value",
};
}),
],
},
});
```
The `syncEnvVars` callback function works very similarly to the deprecated `resolveEnvVars` handler, but now instead of returning an object with a `variables` key that contains the environment variables, you return an object with the environment variables directly (see the example above).
One other difference is now `params.env` only contains the environment variables that are set in the Trigger.dev environment variables, and not the environment variables from the process. If you want to access the environment variables from the process, you can use `process.env`.
See the [syncEnvVars](/deploy-environment-variables#sync-env-vars-from-another-service) documentation for more information.
### emitDecoratorMetadata
If you make use of decorators in your code, and have enabled the `emitDecoratorMetadata` tsconfig compiler option, you'll need to enable this in the new build sytem using the `emitDecoratorMetadata` build extension:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { emitDecoratorMetadata } from "@trigger.dev/build/extensions/typescript";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [emitDecoratorMetadata()],
},
});
```
### Prisma
We've created a build extension to support using Prisma in your Trigger.dev tasks. To use this extension, you'll need to add the `@trigger.dev/build` package to your `devDependencies`:
```sh
npm add @trigger.dev/build@latest -D
```
Then you can import the `prismaExtension` build extension and use it in your `trigger.config.ts` file, passing in the path to your Prisma schema file:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { prismaExtension } from "@trigger.dev/build/extensions/prisma";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
prismaExtension({
schema: "prisma/schema.prisma",
}),
],
},
});
```
This will make sure that your prisma client is generated during the build process when deploying to Trigger.dev.
<Note>
This does not have any effect when running the `dev` command, so you'll need to make sure you
generate your client locally first.
</Note>
If you want to also run migrations during the build process, you can pass in the `migrate` option:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { prismaExtension } from "@trigger.dev/build/extensions/prisma";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
prismaExtension({
schema: "prisma/schema.prisma",
migrate: true,
directUrlEnvVarName: "DATABASE_URL_UNPOOLED", // optional - the name of the environment variable that contains the direct database URL if you are using a direct database URL
}),
],
},
});
```
If you have multiple `generator` statements defined in your schema file, you can pass in the `clientGenerator` option to specify the `prisma-client-js` generator, which will prevent other generators from being generated:
<CodeGroup>
```prisma schema.prisma
datasource db {
provider  = "postgresql"
url       = env("DATABASE_URL")
directUrl = env("DATABASE_URL_UNPOOLED")
}
// We only want to generate the prisma-client-js generator
generator client {
provider        = "prisma-client-js"
}
generator kysely {
provider     = "prisma-kysely"
output       = "../../src/kysely"
enumFileName = "enums.ts"
fileName     = "types.ts"
}
```
```ts trigger.config.ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { prismaExtension } from "@trigger.dev/build/extensions/prisma";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
prismaExtension({
schema: "prisma/schema.prisma",
clientGenerator: "client",
}),
],
},
});
```
</CodeGroup>
### audioWaveform
Previously, we installed [Audio Waveform](https://github.com/bbc/audiowaveform) in the build image. That's been moved to a build extension:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { audioWaveform } from "@trigger.dev/build/extensions/audioWaveform";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [audioWaveform()], // uses verson 1.1.0 of audiowaveform by default
},
});
```
### esbuild plugins
You can now add esbuild plugins to customize the build process using the `esbuildPlugin` build extension. The example below shows how to automatically upload sourcemaps to Sentry using their esbuild plugin:
```ts
import { defineConfig } from "@trigger.dev/sdk/v3";
import { esbuildPlugin } from "@trigger.dev/build/extensions";
import { sentryEsbuildPlugin } from "@sentry/esbuild-plugin";
export default defineConfig({
project: "<project ref>",
build: {
extensions: [
esbuildPlugin(
sentryEsbuildPlugin({
org: process.env.SENTRY_ORG,
project: process.env.SENTRY_PROJECT,
authToken: process.env.SENTRY_AUTH_TOKEN,
}),
// optional - only runs during the deploy command, and adds the plugin to the end of the list of plugins
{ placement: "last", target: "deploy" }
),
],
},
});
```
## Changes to the `trigger.dev` CLI
### No more typechecking during deploy
We no longer run typechecking during the deploy command. This was causing issues with some projects, and we found that it wasn't necessary to run typechecking during the deploy command. If you want to run typechecking before deploying to Trigger.dev, you can run the `tsc` command before running the `deploy` command.
```sh
tsc && npx trigger.dev@latest deploy
```
Or if you are using GitHub actions, you can add an additional step to run the `tsc` command before deploying to Trigger.dev.
```yaml
- name: Install dependencies
run: npm install
- name: Typecheck
run: npx tsc
- name: ðŸš€ Deploy Trigger.dev
env:
TRIGGER_ACCESS_TOKEN: ${{ secrets.TRIGGER_ACCESS_TOKEN }}
run: |
npx trigger.dev@latest deploy
```
### deploy `--dry-run`
You can now inspect the build output of your project without actually deploying it to Trigger.dev by using the `--dry-run` flag:
```sh
npx trigger.dev@latest deploy --dry-run
```
This will save the build output and print the path to the build output directory. If you face any issues with deploying, please include the build output in your issue report.
### `--env-file`
You can now pass the path to your local `.env` file using the `--env-file` flag during `dev` and `deploy` commands:
```sh
npx trigger.dev@latest dev --env-file ../../.env
npx trigger.dev@latest deploy --env-file ../../.env
```
The `.env` file works slightly differently in `dev` vs `deploy`:
* In `dev`, the `.env` file is loaded into the CLI's `process.env` and also into the environment variables of the Trigger.dev environment.
* In `deploy`, the `.env` file is loaded into the CLI's `process.env` but not into the environment variables of the Trigger.dev environment. If you want to sync the environment variables from the `.env` file to the Trigger.dev environment variables, you can use the `syncEnvVars` build extension.
### dev debugging in VS Code
Debugging your task code in `dev` is supported via VS Code, without having to pass in any additional flags. Create a launch configuration in `.vscode/launch.json`:
```json launch.json
{
"version": "0.2.0",
"configurations": [
{
"name": "Trigger.dev: Dev",
"type": "node",
"request": "launch",
"cwd": "${workspaceFolder}",
"runtimeExecutable": "npx",
"runtimeArgs": ["trigger.dev@latest", "dev"],
"skipFiles": ["<node_internals>/**"],
"sourceMaps": true
}
]
}
```
Then you can start debugging your tasks code by selecting the `Trigger.dev: Dev` configuration in the debug panel, and set breakpoints in your tasks code.
### TRIGGER\_ACCESS\_TOKEN in dev
You can now authenticate the `dev` command using the `TRIGGER_ACCESS_TOKEN` environment variable. Previously this was only supported in the `deploy` command.
```sh
TRIGGER_ACCESS_TOKEN=<your access token> npx trigger.dev@latest dev
```
### Better deploy support for self-hosters
You can now specify a custom registry and namespace when deploying via a self-hosted instance of Trigger.dev:
```sh
npx trigger.dev@latest deploy \
--self-hosted \
--load-image \
--registry docker.io \
--namespace mydockerhubusername
```
All you have to do is create a repository in dockerhub that matches the project ref of your Trigger.dev project (e.g. `proj_rrkpdguyagvsoktglnod`)
<Note>
Docker Hub will automatically create a repository the first time you push, which is public by
default. If you want to keep these images private, make sure you create the repository before you
first run the `deploy` command
</Note>
## Known issues
* Path aliases are not yet support in your `trigger.config.ts` file. To workaround this issue you'll need to rewrite path aliases to their relative paths. (See [this](https://github.com/unjs/jiti/issues/166) and [this](https://knip.dev/reference/known-issues#path-aliases-in-config-files)) for more info.
* `*.test.ts` and `.spec.ts` files inside the trigger dirs will be bundled and could cause issues. You'll need to move these files outside of the trigger dirs to avoid this issue.
# How to upgrade the Trigger.dev packages
Source: https://trigger.dev/docs/upgrading-packages
When we release fixes and new features we recommend you upgrade your Trigger.dev packages.
## Update command
Run this command in your project:
```sh
npx trigger.dev@latest update
```
This will update all of the Trigger.dev packages in your project to the latest version.
## Running the CLI locally
When you run the CLI locally use the latest version for the `dev` and `deploy` commands:
```sh
npx trigger.dev@latest dev
```
```sh
npx trigger.dev@latest deploy
```
These commands will also give you the option to upgrade if you are behind on versions.
## Deploying with GitHub Actions
You can deploy using [GitHub Actions](/github-actions). We recommend that you lock your version in the workflow file so make sure to upgrade.
<Warning>
The deploy step will fail if version mismatches are detected. It's important that you update the
version using the steps below.
</Warning>
<Steps>
<Step title="Find your workflow file">
In your `.githubs/workflows` folder you can find your workflow yml files. You may have a prod
and staging one.
</Step>
<Step title="Update the version for the run command">
In the steps you'll see a `run` command. It will run the trigger.dev deploy CLI command. Make
sure to update this version to the latest version (e.g. `npx trigger.dev@3.0.0 deploy`).
</Step>
</Steps>
## package.json dev dependency
Instead of using `npx`, `pnpm dlx` or `yarn dlx` you can add the Trigger.dev CLI as a dev dependency to your package.json file.
For example:
```json
{
"devDependencies": {
"trigger.dev": "3.0.0"
}
}
```
If you've done this make sure to update the version to match the `@trigger.dev/sdk` package.
Once you have added the `trigger.dev` package to your `devDependencies`, you can use `npm exec trigger.dev`, `pnpm exec trigger.dev`, or `yarn exec trigger.dev` to run the CLI.
But we recommend adding your dev and deploy commands to the `scripts` section of your `package.json` file:
```json
{
"scripts": {
"dev:trigger": "trigger dev",
"deploy:trigger": "trigger deploy"
}
}
```
Then you can run `npm run dev:trigger` and `npm run deploy:trigger` to run the CLI.
# Vercel integration
Source: https://trigger.dev/docs/vercel-integration
When you deploy to Vercel, automatically deploy your associated tasks.
<Note>This feature is in review. Stay up to date with progress and vote on its priority on our [Roadmap](https://feedback.trigger.dev/roadmap).</Note>
# Versioning
Source: https://trigger.dev/docs/versioning
We use atomic versioning to ensure that started tasks are not affected by changes to the task code.
A version is a bundle of tasks at a certain point in time.
## Version identifiers
Version identifiers look like this:
* `20240313.1` - March 13th, 2024, version 1
* `20240313.2` - March 13th, 2024, version 2
* `20240314.1` - March 14th, 2024, version 1
You can see there are two parts to the version identifier:
* The date (in reverse format)
* The version number
Versions numbers are incremented each time a new version is created for that date and environment. So it's possible to have `20240313.1` in both the `dev` and `prod` environments.
## Version locking
When a task run starts it is locked to the latest version of the code (for that environment). Once locked it won't change versions, even if you deploy new versions. This is to ensure that a task run is not affected by changes to the code.
### Child tasks and version locking
Trigger and wait functions version lock child task runs to the parent task run version. This ensures the results from child runs match what the parent task is expecting. If you don't wait then version locking doesn't apply.
| Trigger function        | Parent task version | Child task version | isLocked |
| ----------------------- | ------------------- | ------------------ | -------- |
| `trigger()`             | `20240313.2`        | Latest             | No       |
| `batchTrigger()`        | `20240313.2`        | Latest             | No       |
| `triggerAndWait()`      | `20240313.2`        | `20240313.2`       | Yes      |
| `batchTriggerAndWait()` | `20240313.2`        | `20240313.2`       | Yes      |
## Local development
When running the local server (using `npx trigger.dev dev`), every relevant code change automatically creates a new version of all tasks.
So a task run will continue running on the version it was locked to. We do this by spawning a new process for each task run. This ensures that the task run is not affected by changes to the code.
## Deployment
Every deployment creates a new version of all tasks for that environment.
## Retries and reattempts
When a task has an uncaught error it will [retry](/errors-retrying), assuming you have not set `maxAttempts` to 0. Retries are locked to the original version of the run.
## Replays
A "replay" is a new run of a task that uses the same inputs but will use the latest version of the code. This is useful when you fix a bug and want to re-run a task with the same inputs. See [replaying](/replaying) for more information.
# Video walkthrough
Source: https://trigger.dev/docs/video-walkthrough
Go from zero to a working task in your Next.js app in 10 minutes.
<iframe width="100%" height="315" src="https://www.youtube.com/embed/YH_4c0K7fGM?si=5JzZmZseuqI5aciM" title="Trigger.dev walkthrough" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen />
### In this video we cover the following topics:
* [0:00](https://youtu.be/YH_4c0K7fGM?si=J8svVzotZtyTXDap\&t=0) â€“ [Install Trigger.dev](/quick-start) in an existing Next.js project
* [1:44](https://youtu.be/YH_4c0K7fGM?si=J8svVzotZtyTXDap\&t=104) â€“ [Run and test](/run-tests) the "Hello, world!" example project
* [2:09](https://youtu.be/YH_4c0K7fGM?si=FMTP8ep_cDBCU0_x\&t=128) â€“ Create and run an AI image generation task that uses [Fal.ai](https://fal.ai) â€“ ([View the code](/guides/examples/fal-ai-image-to-cartoon))
* [6:25](https://youtu.be/YH_4c0K7fGM?si=pPc8iLI2Y9FGD3yo\&t=385) â€“ Create and run a [Realtime](/realtime/overview) example using [React hooks](/frontend/react-hooks) â€“ ([View the code](/guides/examples/fal-ai-realtime))
* [11:10](https://youtu.be/YH_4c0K7fGM?si=Mjd0EvvNsNlVouvY\&t=670) â€“ [Deploy your task](/cli-deploy) to the Trigger.dev Cloud
# Wait: Overview
Source: https://trigger.dev/docs/wait
During your run you can wait for a period of time or for something to happen.
Waiting allows you to write complex tasks as a set of async code, without having to scheduled another task or poll for changes.
<Note>
In the Trigger.dev Cloud we automatically pause execution of tasks when they are waiting for
longer than a few seconds. You are not charged when execution is paused.
</Note>
| Function                               | What it does                                                                              |
| -------------------------------------- | ----------------------------------------------------------------------------------------- |
| [wait.for()](/wait-for)                | Waits for a specific period of time, e.g. 1 day.                                          |
| [wait.until()](/wait-until)            | Waits until the provided `Date`.                                                          |
| [wait.forRequest()](/wait-for-request) | Waits until a matching HTTP request is received, and gives you the data to continue with. |
| [waitForEvent()](/wait-for-event)      | Waits for a matching event, like in the example above.                                    |
# Wait for
Source: https://trigger.dev/docs/wait-for
Wait for a period of time, then continue execution.
Inside your tasks you can wait for a period of time before you want execution to continue.
```ts /trigger/long-task.ts
export const veryLongTask = task({
id: "very-long-task",
run: async (payload) => {
await wait.for({ seconds: 5 });
await wait.for({ minutes: 10 });
await wait.for({ hours: 1 });
await wait.for({ days: 1 });
await wait.for({ weeks: 1 });
await wait.for({ months: 1 });
await wait.for({ years: 1 });
},
});
```
This allows you to write linear code without having to worry about the complexity of scheduling or managing cron jobs.
<Note>
In the Trigger.dev Cloud we automatically pause execution of tasks when they are waiting for
longer than a few seconds. You are not charged when execution is paused.
</Note>
# Wait for event
Source: https://trigger.dev/docs/wait-for-event
Wait until an event has been received, then continue execution.
<Note>This feature is in review. Stay up to date with progress and vote on its priority on our [Roadmap](https://feedback.trigger.dev/roadmap).</Note>
# Wait for request
Source: https://trigger.dev/docs/wait-for-request
Wait until a `Request` has been received at the provided URL, then continue execution.
<Note>This feature is in review. Stay up to date with progress and vote on its priority on our [Roadmap](https://feedback.trigger.dev/roadmap).</Note>
# Wait until
Source: https://trigger.dev/docs/wait-until
Wait until a date, then continue execution.
This example sends a reminder email to a user at the specified datetime.
```ts /trigger/reminder-email.ts
export const sendReminderEmail = task({
id: "send-reminder-email",
run: async (payload: { to: string; name: string; date: string }) => {
//wait until the date
await wait.until({ date: new Date(payload.date) });
//todo send email
const { data, error } = await resend.emails.send({
from: "hello@trigger.dev",
to: payload.to,
subject: "Don't forgetâ€¦",
html: `<p>Hello ${payload.name},</p><p>...</p>`,
});
},
});
```
This allows you to write linear code without having to worry about the complexity of scheduling or managing cron jobs.
<Note>
In the Trigger.dev Cloud we automatically pause execution of tasks when they are waiting for
longer than a few seconds. You are not charged when execution is paused.
</Note>
## `throwIfInThePast`
You can optionally throw an error if the date is already in the past when the function is called:
```ts
await wait.until({ date: new Date(date), throwIfInThePast: true });
```
You can of course use try/catch if you want to do something special in this case.
# Writing tasks: Overview
Source: https://trigger.dev/docs/writing-tasks-introduction
Tasks are the core of Trigger.dev. They are long-running processes that are triggered by events.
Before digging deeper into the details of writing tasks, you should read the [fundamentals of tasks](/tasks/overview) to understand what tasks are and how they work.
## Writing tasks
| Topic                                        | Description                                                                                         |
| :------------------------------------------- | :-------------------------------------------------------------------------------------------------- |
| [Logging](/logging)                          | View and send logs and traces from your tasks.                                                      |
| [Errors & retrying](/errors-retrying)        | How to deal with errors and write reliable tasks.                                                   |
| [Wait](/wait)                                | Wait for periods of time or for external events to occur before continuing.                         |
| [Concurrency & Queues](/queue-concurrency)   | Configure what you want to happen when there is more than one run at a time.                        |
| [Realtime notifications](/realtime/overview) | Send realtime notifications from your task that you can subscribe to from your backend or frontend. |
| [Versioning](/versioning)                    | How versioning works.                                                                               |
| [Machines](/machines)                        | Configure the CPU and RAM of the machine your task runs on                                          |
| [Idempotency](/idempotency)                  | Protect against mutations happening twice.                                                          |
| [Replaying](/replaying)                      | You can replay a single task or many at once with a new version of your code.                       |
| [Max duration](/runs/max-duration)           | Set a maximum duration for your task to run.                                                        |
| [Tags](/tags)                                | Tags allow you to easily filter runs in the dashboard and when using the SDK.                       |
| [Metadata](/runs/metadata)                   | Attach a small amount of data to a run and update it as the run progresses.                         |
| [Usage](/run-usage)                          | Get compute duration and cost from inside a run, or for a specific block of code.                   |
| [Context](/context)                          | Access the context of the task run.                                                                 |
| [Bulk actions](/bulk-actions)                | Run actions on many task runs at once.                                                              |
## Our library of examples, guides and projects
<CardGroup cols={2}>
<Card title="Walkthrough guides" icon="book" href="/guides/introduction">
Detailed guides for setting up Trigger.dev with popular frameworks and services, including
Next.js, Remix, Supabase, Stripe and more.
</Card>
<Card title="Example tasks" icon="code" href="/guides/introduction#example-tasks">
Task code you can copy and paste to use in your own projects, including OpenAI, Vercel AI SDK,
Deepgram, FFmpeg, Puppeteer, Stripe, Supabase and more.
</Card>
<Card title="Webhook guides" icon="code" href="/guides/frameworks/webhooks-guides-overview">
Learn how to trigger tasks from webhooks, including Next.js, Remix, Supabase and Stripe and
more.
</Card>
<Card title="Example projects" icon="GitHub" href="/guides/introduction#example-projects">
Full-stack projects demonstrating how to use Trigger.dev. Fork them in GitHub as a starting
point for your own projects.
</Card>
</CardGroup>